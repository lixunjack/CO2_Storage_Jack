{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e006060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "h5py._errors.unsilence_errors()\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "# SUGGESTION: create all folders for storing results\n",
    "if not os.path.exists('./vis'):\n",
    "    os.mkdir('./vis')\n",
    "\n",
    "if not os.path.exists('./vis_results'):\n",
    "    os.mkdir('./vis_results')\n",
    "\n",
    "if not os.path.exists('./model256_weights'):\n",
    "    os.mkdir('./model256_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(im):\n",
    "    '''returns log(image) scaled to the interval [0,1]'''\n",
    "    try:\n",
    "        (min, max) = 1e-16, 1 #(im[im > 0].min(), im.max())\n",
    "        # print(im.min(), im.max())\n",
    "        if (max > min) and (max > 0):\n",
    "            return (np.log(im.clip(min, max)) - np.log(min)) / (np.log(max) - np.log(min))\n",
    "    except:\n",
    "        pass\n",
    "    return im\n",
    "\n",
    "def matshow(np_data, figsize = (12, 12), title=None, vmin=None, vmax=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    if vmin is None and vmax is None:\n",
    "        im = ax.matshow(np_data)\n",
    "    else:\n",
    "        im = ax.matshow(np_data, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ax.set_title(f'{title}')\n",
    "      \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def matshow2(C, eps, figsize = (12, 12), title=None, save_filename=None):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "    im = ax[0].matshow(C, vmin = 0, vmax = 1)\n",
    "    ax[0].set_title(f'C at {title}')\n",
    "    im.set_clim(0, 1)\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[1].matshow(eps)\n",
    "    ax[1].set_title(f'eps at {title}')\n",
    "    im.set_clim(0, 1)\n",
    "\n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_filename:\n",
    "        plt.savefig(f'vis/{save_filename}.pdf', dpi=800)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def matshow3(C, eps, dissolution, figsize = (12, 12), title=None, save_filename=None):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    im = ax[0].matshow(C, vmin = 0, vmax = 1)\n",
    "    ax[0].set_title(f'C at {title}')\n",
    "    im.set_clim(0, 1)\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[1].matshow(eps)\n",
    "    ax[1].set_title(f'eps at {title}')\n",
    "    im.set_clim(0, 1)\n",
    "\n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[2].matshow(dissolution)\n",
    "    ax[2].set_title(f'dissolution (log(difference)) at {title}')\n",
    "    im.set_clim(0, 1)\n",
    "\n",
    "    divider = make_axes_locatable(ax[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_filename:\n",
    "        plt.savefig(f'vis/{save_filename}.pdf', dpi=800)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def matshow4(C, eps, Ux, Uy, figsize = (12, 12), title=None):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=figsize)\n",
    "    im = ax[0, 0].matshow(C, vmin = 0, vmax = 1)\n",
    "    ax[0, 0].set_title(f'{title} for C')\n",
    "      \n",
    "    divider = make_axes_locatable(ax[0, 0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[0, 1].matshow(eps)\n",
    "    ax[0, 1].set_title(f'{title} for eps')\n",
    "      \n",
    "    divider = make_axes_locatable(ax[0, 1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[1, 0].matshow(Ux)\n",
    "    ax[1, 0].set_title(f'{title} for Ux')\n",
    "      \n",
    "    divider = make_axes_locatable(ax[1, 0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im = ax[1, 1].matshow(Uy)\n",
    "    ax[1, 1].set_title(f'{title} for Uy')\n",
    "      \n",
    "    divider = make_axes_locatable(ax[1, 1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_simulation_hdf(file_name):\n",
    "    print(f'loading the file: {file_name}')\n",
    "    data_dict = {}\n",
    "\n",
    "    with h5py.File(file_name, \"r\") as file_handle:\n",
    "        # List all groups\n",
    "        print(f\"Keys: {file_handle.keys()}\")\n",
    "        scaling_factor = 1\n",
    "        for key_ in file_handle.keys():\n",
    "            if 'key_' == 'C':\n",
    "                scaling_factor = 1 # 100\n",
    "            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n",
    "                scaling_factor = 1 # 1000\n",
    "            \n",
    "            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n",
    "            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n",
    "\n",
    "        print(f'Done with {file_name} == closing file now')\n",
    "\n",
    "    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n",
    "\n",
    "\n",
    "def load_datafiles(data_filenames):\n",
    "    # snapshot_indices will split the data in time into train and validation\n",
    "    data_dict = {\n",
    "        'C': [], # list of np arrays\n",
    "        'eps': [],\n",
    "        'Ux': [],\n",
    "        'Uy': [],\n",
    "    }\n",
    "\n",
    "    for filename in data_filenames:\n",
    "        C, eps, Ux, Uy = read_simulation_hdf(filename)\n",
    "        data_dict['C'].append(C[2:-2, 2:-2, :])\n",
    "        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n",
    "        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n",
    "        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n",
    "    return data_dict\n",
    "\n",
    "def get_filelist():\n",
    "    from itertools import permutations, product\n",
    "    # permutations = list(permutations(range(4), 2))\n",
    "    permutations = list(product(range(4), repeat=2))\n",
    "\n",
    "    file_list = []\n",
    "    properties_list = []\n",
    "    for idx1, idx2 in permutations:\n",
    "        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n",
    "        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n",
    "        file_name = f'256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n",
    "        file_list.append(file_name)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = get_filelist()\n",
    "print(file_list)\n",
    "\n",
    "data_list = load_datafiles(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a00143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate sample mean and std -- this should be done better\n",
    "\n",
    "n_training_samples = 12\n",
    "\n",
    "\n",
    "C = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\n",
    "eps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\n",
    "Ux = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\n",
    "Uy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n",
    "\n",
    "Ux_mean, Ux_std = Ux.mean(), Ux.std()\n",
    "Uy_mean, Uy_std = Uy.mean(), Uy.std()\n",
    "eps_mean, eps_std = eps.mean(), eps.std()\n",
    "\n",
    "\n",
    "print(Ux_mean, Ux_std)\n",
    "print(Uy_mean, Uy_std)\n",
    "print(eps_mean, eps_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_idx = 0\n",
    "for idx_ in range(2): # range(C.shape[-1]):\n",
    "    title = f'time step: {idx_}, eps'\n",
    "    matshow(eps[file_idx, :, :, idx_], figsize=(10, 10), title=title)\n",
    "    title = f'time step: {idx_}, C'\n",
    "    matshow(C[file_idx, :, :, idx_], figsize=(10, 10), title=title)\n",
    "    title = f'time step: {idx_}, U_x'\n",
    "    matshow(Ux[file_idx, :, :, idx_], figsize=(10, 10), title=title)\n",
    "    title = f'time step: {idx_}, U_y'\n",
    "    matshow(Uy[file_idx, :, :, idx_], figsize=(10, 10), title=title)\n",
    "    title = f'time step: {idx_}, dissolution'\n",
    "    matshow((eps[file_idx, :, :, idx_+1] - eps[file_idx, :, :, idx_]), figsize=(10, 10), title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 0\n",
    "for idx in range(0, 100, 10):\n",
    "    title = f'time step: {idx_}'\n",
    "    save_filename = f'dissolution_{idx_}'\n",
    "\n",
    "    matshow3(\n",
    "        100*C[file_idx, :, :, idx_],\n",
    "        eps[file_idx, :, :, idx_+1]-eps[file_idx, :, :, idx_],\n",
    "        log_transform(eps[file_idx, :, :, idx_+1]-eps[file_idx, :, :, idx_]), \n",
    "        figsize=(15, 5),\n",
    "        title=title,\n",
    "        save_filename=save_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cab32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "def preprocess_data_cube(data_dict, scaling_dict):\n",
    "    print(f'preprocess_data_cube')\n",
    "\n",
    "    masks = []\n",
    "    images = []\n",
    "    for file_idx in range(len(data_dict['C'])):\n",
    "        C = data_dict['C'][file_idx][:, :, :-1]\n",
    "        eps = data_dict['eps'][file_idx][:, :, :-1]\n",
    "        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n",
    "        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n",
    "        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n",
    "\n",
    "        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n",
    "        mask = eps_t - eps\n",
    "\n",
    "        # these should be moved to preprocessing\n",
    "        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n",
    "        C = C*scaling_dict['C_scaling'] - 0.5\n",
    "        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n",
    "        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n",
    "        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n",
    "\n",
    "\n",
    "        image = np.stack([C, eps, Ux, Uy], axis=-1)\n",
    "        image = np.swapaxes(image, 3, 2)\n",
    "\n",
    "        masks.append(mask)\n",
    "        images.append(image)\n",
    "    \n",
    "    masks = np.concatenate(masks, axis=-1)\n",
    "    images = np.concatenate(images, axis=-1)\n",
    "    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n",
    "    return images, masks\n",
    "\n",
    "class DissolutionDataset(Dataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): path to data folder\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(\n",
    "            self,\n",
    "            data_filenames,\n",
    "            scaling_dict,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "\n",
    "        # self.scaling_dict = scaling_dict\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        data_dict = load_datafiles(data_filenames)\n",
    "        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n",
    "        print(self.image.shape, self.mask.shape)\n",
    "        self.data_len = self.image.shape[-1]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image[:, :, :, idx]\n",
    "        mask = self.mask[:, :, idx]\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # assume one file for now\n",
    "        return self.data_len # last element we cann't predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc77ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_scaling = 100\n",
    "data_scalingdict = {\n",
    "    'C_scaling': C_scaling,\n",
    "    'Ux_mean': Ux_mean,\n",
    "    'Ux_std': Ux_std,\n",
    "    'Uy_mean': Uy_mean,\n",
    "    'Uy_std': Uy_std,\n",
    "    'eps_mean': eps_mean,\n",
    "    'eps_std': eps_std,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filenames = get_filelist()\n",
    "dataset = DissolutionDataset(data_filenames[:3], data_scalingdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "\n",
    "\"\"\"\n",
    "WARNING: This function has been defined before\n",
    "\"\"\"\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(32, 4))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        im = plt.imshow(image)\n",
    "        plt.colorbar(im)\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(f'Dataset of size {len(dataset)}')\n",
    "\n",
    "for idx_ in range(5):\n",
    "    current_timestep = 20*idx_\n",
    "    print(f'plotting for time step: {current_timestep}')\n",
    "    image, mask = dataset[current_timestep] # get some sample\n",
    "    print(image.shape, mask.shape)\n",
    "    visualize(\n",
    "        concentration=image[:,:, 0],\n",
    "        eps=image[:,:, 1],\n",
    "        Ux=image[:,:, 2],\n",
    "        Uy=image[:,:, 3],\n",
    "        dissolution=mask.squeeze(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.Resize(256, 256),  # not needed\n",
    "        # albu.HorizontalFlip(p=0.5),\n",
    "        # albu.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Resize to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(256, 256),  \n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor_img(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def to_tensor_mask(x, **kwargs):\n",
    "    return x.astype('float32')\n",
    "\n",
    "def get_preprocessing():\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25491efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filenames = get_filelist()\n",
    "\n",
    "#### Visualize resulted augmented images and masks\n",
    "dataset_train = DissolutionDataset(\n",
    "    data_filenames[:n_training_samples],\n",
    "    scaling_dict=data_scalingdict,\n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb36f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same image with different random transforms\n",
    "for idx_ in range(5):\n",
    "    current_timestep = 10*idx_\n",
    "    print(f'plotting for time step: {current_timestep}')\n",
    "    image, mask = dataset_train[current_timestep] # get some sample\n",
    "    visualize(\n",
    "        concentration=image[:,:, 0],\n",
    "        eps=image[:,:, 1],\n",
    "        Ux=image[:,:, 2],\n",
    "        Uy=image[:,:, 3],\n",
    "        dissolution=mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca98aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = DissolutionDataset(\n",
    "    data_filenames[n_training_samples:],\n",
    "    scaling_dict=data_scalingdict,\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_ in range(4):\n",
    "    current_timestep = 10*idx_\n",
    "    print(f'plotting for time step: {current_timestep}')\n",
    "    image, mask = dataset_valid[current_timestep] # get some sample\n",
    "    visualize(\n",
    "        concentration=image[:,:, 0],\n",
    "        eps=image[:,:, 1],\n",
    "        Ux=image[:,:, 2],\n",
    "        Uy=image[:,:, 3],\n",
    "        dissolution=mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ab830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset_train = DissolutionDataset(\n",
    "    data_filenames[:n_training_samples],\n",
    "    scaling_dict=data_scalingdict,\n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(),\n",
    ")\n",
    "\n",
    "dataset_valid = DissolutionDataset(\n",
    "    data_filenames[n_training_samples:],\n",
    "    scaling_dict=data_scalingdict,\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\n",
    "train_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\n",
    "train_loader = DataLoader(dataset_train, batch_size=64, num_workers=16, sampler=train_sampler, drop_last=True)                        \n",
    "valid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=4, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "encoder_name = 'resnet34'\n",
    "model = smp.Unet(\n",
    "    encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=None,           # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=4,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "    activation='sigmoid', #'sigmoid',  # 'clamp', # between 0 and 1\n",
    ")\n",
    "\n",
    "l2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\n",
    "l1_loss = torch.nn.L1Loss() # solution is sparse\n",
    "\n",
    "metrics = [\n",
    "    torch.nn.L1Loss\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=5e-4),\n",
    "])\n",
    "\n",
    "def save_checkpoint(state_dict, model_filename):\n",
    "    torch.save(state_dict, model_filename)\n",
    "    print(f'saved model to {model_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_device = torch.device('cpu')\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "for epoch_counter in range(max_epochs):\n",
    "    # print(f'length of train/val dataloaders are: {len(train_loader)}, {len(valid_loader)}')\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f'{time_str}: epoch_counter: {epoch_counter}, start of training')\n",
    "\n",
    "    # training steps\n",
    "    train_loss = 0\n",
    "    for imgs, masks in tqdm(train_loader):\n",
    "        imgs, masks = imgs.to(device, non_blocking=True), masks.to(device, non_blocking=True)\n",
    "        # c_field = imgs[:, 0, :, :].squeeze() + 0.5\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs).squeeze()\n",
    "\n",
    "        # l2_loss_values = l2_loss(masks, preds)\n",
    "        # l1_loss_values = l1_loss(masks, preds)\n",
    "        # loss_values = l2_loss_values + l1_loss_values\n",
    "        loss_values = l2_loss(masks, preds) # weighted_mse_loss(masks, preds, (c_field+1)**2)\n",
    "        loss_values.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_values\n",
    "        time_str = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f'{time_str}: epoch_counter {epoch_counter}, current loss {loss_values.item()}')\n",
    "    \n",
    "    # check point every 10 epochs\n",
    "    if epoch_counter % 10 == 0: \n",
    "        model = model.to(cpu_device)\n",
    "        state_dict = {\n",
    "            'epoch': epoch_counter,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }\n",
    "        model_filename = f\"model256_weights/resnet34_{epoch_counter}_l2loss.pth\"\n",
    "        save_checkpoint(state_dict, model_filename)\n",
    "        print('Saved a checkpoint {}'.format(model_filename))\n",
    "        model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19564e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # loop 1 on training data\n",
    "    train_loss = []\n",
    "    # train_loss_scaled = []\n",
    "    preds_list_train = []\n",
    "    masks_list_train = []\n",
    "\n",
    "    for imgs, masks in tqdm(train_loader):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True) # .squeeze()\n",
    "        preds = model(imgs).squeeze()\n",
    "\n",
    "        l2_loss_values = l2_loss(masks, preds)\n",
    "        train_loss.append(l2_loss_values.item())\n",
    "\n",
    "        # store the true values\n",
    "        # Changed to store in CPU\n",
    "        masks_list_train.append(masks.to(cpu_device).numpy())\n",
    "        preds_list_train.append(preds.to(cpu_device).numpy())\n",
    "\n",
    "    train_loss = np.array(train_loss)\n",
    "    print(f'train_loss: {train_loss.mean()}')\n",
    "    \n",
    "    val_loss = []\n",
    "    preds_list_val = []\n",
    "    masks_list_val = []\n",
    "\n",
    "    # loop 2 on validation data\n",
    "    for imgs, masks in tqdm(valid_loader):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)# .squeeze()\n",
    "        preds = model(imgs).squeeze()\n",
    "\n",
    "        l2_loss_values = l2_loss(masks, preds)\n",
    "        val_loss.append(l2_loss_values.item())\n",
    "\n",
    "        # store the true values\n",
    "        # Changed to store in CPU\n",
    "        masks_list_val.append(masks.to(cpu_device).numpy())\n",
    "        preds_list_val.append(preds.to(cpu_device).numpy())\n",
    "\n",
    "    val_loss = np.array(val_loss)\n",
    "    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d43dc-02e8-47d3-a143-b24dfef308f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list_train = np.concatenate(preds_list_train)\n",
    "masks_list_train = np.concatenate(masks_list_train)\n",
    "\n",
    "preds_list_val = np.concatenate(preds_list_val)\n",
    "masks_list_val = np.concatenate(masks_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    v_max = max(truth.max(), pred.max())\n",
    "    v_min = max(truth.min(), pred.min())\n",
    "\n",
    "    if scale:\n",
    "        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "    else:\n",
    "        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "    # im.set_clim(0.0, 0.3)\n",
    "    ax[0].set_title(f'{title} prediction')\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    if scale:\n",
    "        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "    else:\n",
    "        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "    # im.set_clim(0.0, 0.3)\n",
    "    ax[1].set_title(f'{title} reference')\n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    # error = np.abs(pred-truth)\n",
    "    error = pred-truth\n",
    "\n",
    "    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n",
    "    max_abs_error = np.max(np.abs(error))\n",
    "    # Set the color limits dynamically centered around zero\n",
    "    clim = (-max_abs_error, max_abs_error)\n",
    "    im.set_clim(clim)\n",
    "\n",
    "    ax[2].set_title(f'{title} error')\n",
    "    divider = make_axes_locatable(ax[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "for sample_idx in range(1): #12):\n",
    "    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n",
    "        preds = preds_list_train[sample_idx*100+time_step, :, :]\n",
    "        masks = masks_list_train[sample_idx*100+time_step, :, :]\n",
    "        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n",
    "        matshow_error(\n",
    "            preds,\n",
    "            masks, \n",
    "            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n",
    "            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n",
    "            figsize=(15, 7))\n",
    "        \n",
    "for sample_idx in range(1): #4):\n",
    "    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n",
    "        preds = preds_list_val[sample_idx*100+time_step, :, :]\n",
    "        masks = masks_list_val[sample_idx*100+time_step, :, :]\n",
    "        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n",
    "        matshow_error(\n",
    "            preds,\n",
    "            masks,\n",
    "            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n",
    "            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n",
    "            figsize=(15, 7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
