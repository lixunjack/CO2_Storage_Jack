{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7843411,"sourceType":"datasetVersion","datasetId":4598564}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:29:30.192751Z","iopub.execute_input":"2024-03-14T16:29:30.193174Z","iopub.status.idle":"2024-03-14T16:29:30.200733Z","shell.execute_reply.started":"2024-03-14T16:29:30.193137Z","shell.execute_reply":"2024-03-14T16:29:30.199508Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction3/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:29:30.205925Z","iopub.execute_input":"2024-03-14T16:29:30.206246Z","iopub.status.idle":"2024-03-14T16:29:30.217436Z","shell.execute_reply.started":"2024-03-14T16:29:30.206218Z","shell.execute_reply":"2024-03-14T16:29:30.216371Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/helpfunction3/dataset.py\n/kaggle/input/helpfunction3/pre_processing.py\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:29:30.227577Z","iopub.execute_input":"2024-03-14T16:29:30.227858Z","iopub.status.idle":"2024-03-14T16:30:24.349461Z","shell.execute_reply.started":"2024-03-14T16:29:30.227832Z","shell.execute_reply":"2024-03-14T16:30:24.348297Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:30:24.352318Z","iopub.execute_input":"2024-03-14T16:30:24.353084Z","iopub.status.idle":"2024-03-14T16:30:39.058148Z","shell.execute_reply.started":"2024-03-14T16:30:24.353033Z","shell.execute_reply":"2024-03-14T16:30:39.057262Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=1,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        loss = self.l1_loss(preds, masks)\n        \n        self.log('train_loss', loss, prog_bar=True)\n        self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n    \n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:30:39.059291Z","iopub.execute_input":"2024-03-14T16:30:39.059819Z","iopub.status.idle":"2024-03-14T16:30:39.071336Z","shell.execute_reply.started":"2024-03-14T16:30:39.059790Z","shell.execute_reply":"2024-03-14T16:30:39.070326Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        \n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        \n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n             \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=4, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4,persistent_workers=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:30:39.073668Z","iopub.execute_input":"2024-03-14T16:30:39.073967Z","iopub.status.idle":"2024-03-14T16:30:39.096402Z","shell.execute_reply.started":"2024-03-14T16:30:39.073921Z","shell.execute_reply":"2024-03-14T16:30:39.095549Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:30:39.097610Z","iopub.execute_input":"2024-03-14T16:30:39.097882Z","iopub.status.idle":"2024-03-14T16:30:39.103571Z","shell.execute_reply.started":"2024-03-14T16:30:39.097858Z","shell.execute_reply":"2024-03-14T16:30:39.102757Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=-1,  # Save all checkpoints\n    monitor=\"train_loss\",  # Disable monitoring\n    verbose=True\n)\n\ntrainer = L.Trainer(max_epochs=3,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=20),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.00001, patience=5, \\\n                                              verbose=False)])\n\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\ntrainer.fit(model, data_module)\n\n\n# plt.figure()\n# plt.plot(model.history[\"train_loss\"], label=\"Training Loss\")\n# plt.plot(model.history[\"val_loss\"], label=\"Validation Loss\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Loss\")\n# plt.legend()\n# plt.show()\n\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n# test the model \ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:30:39.104873Z","iopub.execute_input":"2024-03-14T16:30:39.105190Z","iopub.status.idle":"2024-03-14T16:35:43.582247Z","shell.execute_reply.started":"2024-03-14T16:30:39.105165Z","shell.execute_reply":"2024-03-14T16:35:43.579599Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n2024-03-14 16:30:42.452002: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 16:30:42.452132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 16:30:42.582696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type    | Params\n------------------------------------\n0 | model   | Unet    | 24.4 M\n1 | l2_loss | MSELoss | 0     \n2 | l1_loss | L1Loss  | 0     \n------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.758    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff599812ee74eaaa539bc00d4780b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 156: 'train_loss' reached 0.01155 (best 0.01155), saving model to '/kaggle/working/model-epoch=0000-v3.ckpt' as top 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 1, global step 312: 'train_loss' reached 0.00700 (best 0.00700), saving model to '/kaggle/working/model-epoch=0001-v3.ckpt' as top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 2, global step 468: 'train_loss' reached 0.00484 (best 0.00484), saving model to '/kaggle/working/model-epoch=0002-v3.ckpt' as top 3\nINFO: `Trainer.fit` stopped: `max_epochs=3` reached.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n\u001b[1;32m     53\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m---> 54\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(model\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'MyLightningModel' object has no attribute 'history'"],"ename":"AttributeError","evalue":"'MyLightningModel' object has no attribute 'history'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.583172Z","iopub.status.idle":"2024-03-14T16:35:43.583575Z","shell.execute_reply.started":"2024-03-14T16:35:43.583374Z","shell.execute_reply":"2024-03-14T16:35:43.583395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\nfor idx_ in range(4):\n    current_timestep = 10*idx_\n    print(f'plotting for time step: {current_timestep}')\n    image, mask = data_module.example_dataset[current_timestep] # get some sample\n    visualize(\n        concentration=image[0,:, :].squeeze(),\n        eps=image[1,:, :].squeeze(),\n        Ux=image[2,:, :].squeeze(),\n        Uy=image[3,:, :].squeeze(),\n        dissolution=mask.squeeze(),\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.585686Z","iopub.status.idle":"2024-03-14T16:35:43.586465Z","shell.execute_reply.started":"2024-03-14T16:35:43.586197Z","shell.execute_reply":"2024-03-14T16:35:43.586221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use original code to do plotting (see test results!)","metadata":{}},{"cell_type":"code","source":"def read_simulation_hdf(file_name):\n    print(f'loading the file: {file_name}')\n    data_dict = {}\n\n    with h5py.File(file_name, \"r\") as file_handle:\n        # List all groups\n        print(f\"Keys: {file_handle.keys()}\")\n        scaling_factor = 1\n        for key_ in file_handle.keys():\n            if 'key_' == 'C':\n                scaling_factor = 1 # 100\n            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n                scaling_factor = 1 # 1000\n            \n            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n\n        print(f'Done with {file_name} == closing file now')\n\n    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n\n\ndef load_datafiles(data_filenames):\n    # snapshot_indices will split the data in time into train and validation\n    data_dict = {\n        'C': [], # list of np arrays\n        'eps': [],\n        'Ux': [],\n        'Uy': [],\n    }\n\n    for filename in data_filenames:\n        C, eps, Ux, Uy = read_simulation_hdf(filename)\n        data_dict['C'].append(C[2:-2, 2:-2, :])\n        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n    return data_dict\n\ndef get_filelist():\n    from itertools import permutations, product\n    # permutations = list(permutations(range(4), 2))\n    permutations = list(product(range(4), repeat=2))\n\n    file_list = []\n    properties_list = []\n    for idx1, idx2 in permutations:\n        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n        file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n        file_list.append(file_name)\n    return file_list","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.587710Z","iopub.status.idle":"2024-03-14T16:35:43.588191Z","shell.execute_reply.started":"2024-03-14T16:35:43.587948Z","shell.execute_reply":"2024-03-14T16:35:43.587970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, RandomSampler\n\ndef preprocess_data_cube(data_dict, scaling_dict):\n    print(f'preprocess_data_cube')\n\n    masks = []\n    images = []\n    for file_idx in range(len(data_dict['C'])):\n        C = data_dict['C'][file_idx][:, :, :-1]\n        eps = data_dict['eps'][file_idx][:, :, :-1]\n        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n\n        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n        mask = eps_t #- eps\n\n        # these should be moved to preprocessing\n        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n        C = C*scaling_dict['C_scaling'] - 0.5\n        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n\n\n        image = np.stack([C, eps, Ux, Uy], axis=-1)\n        image = np.swapaxes(image, 3, 2)\n\n        masks.append(mask)\n        images.append(image)\n    \n    masks = np.concatenate(masks, axis=-1)\n    images = np.concatenate(images, axis=-1)\n    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n    return images, masks\n\nclass DissolutionDataset(Dataset):\n    \"\"\"Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        data_dir (str): path to data folder\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n  \n    def __init__(\n            self,\n            data_filenames,\n            scaling_dict,\n            augmentation=None, \n            preprocessing=None,\n    ):\n\n        # self.scaling_dict = scaling_dict\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        data_dict = load_datafiles(data_filenames)\n        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n        print(self.image.shape, self.mask.shape)\n        self.data_len = self.image.shape[-1]\n\n    \n    def __getitem__(self, idx):\n        \n        image = self.image[:, :, :, idx]\n        mask = self.mask[:, :, idx]\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # assume one file for now\n        return self.data_len # last element we cann't predict\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.589631Z","iopub.status.idle":"2024-03-14T16:35:43.590112Z","shell.execute_reply.started":"2024-03-14T16:35:43.589854Z","shell.execute_reply":"2024-03-14T16:35:43.589875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor_img(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef to_tensor_mask(x, **kwargs):\n    return x.astype('float32')\n\ndef get_preprocessing():\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.591679Z","iopub.status.idle":"2024-03-14T16:35:43.592050Z","shell.execute_reply.started":"2024-03-14T16:35:43.591854Z","shell.execute_reply":"2024-03-14T16:35:43.591869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.594055Z","iopub.status.idle":"2024-03-14T16:35:43.594512Z","shell.execute_reply.started":"2024-03-14T16:35:43.594273Z","shell.execute_reply":"2024-03-14T16:35:43.594293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = get_filelist()\nprint(file_list)\n\ndata_list = load_datafiles(file_list)\n# estimate sample mean and std -- this should be done better\n\nn_training_samples = 8\n\n\nC = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\neps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\nUx = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\nUy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n\nUx_mean, Ux_std = Ux.mean(), Ux.std()\nUy_mean, Uy_std = Uy.mean(), Uy.std()\neps_mean, eps_std = eps.mean(), eps.std()\n\n\nprint(Ux_mean, Ux_std)\nprint(Uy_mean, Uy_std)\nprint(eps_mean, eps_std)\n\nC_scaling = 100\ndata_scalingdict = {\n    'C_scaling': C_scaling,\n    'Ux_mean': Ux_mean,\n    'Ux_std': Ux_std,\n    'Uy_mean': Uy_mean,\n    'Uy_std': Uy_std,\n    'eps_mean': eps_mean,\n    'eps_std': eps_std,\n}\n\ndel Ux, Uy, eps, C","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.595993Z","iopub.status.idle":"2024-03-14T16:35:43.596453Z","shell.execute_reply.started":"2024-03-14T16:35:43.596214Z","shell.execute_reply":"2024-03-14T16:35:43.596234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\ndata_filenames = get_filelist()\n\ndataset_train = DissolutionDataset(\n    data_filenames[:8],\n    scaling_dict=data_scalingdict,\n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndataset_valid = DissolutionDataset(\n    data_filenames[12:],\n    scaling_dict=data_scalingdict,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndel data_scalingdict","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.597973Z","iopub.status.idle":"2024-03-14T16:35:43.598441Z","shell.execute_reply.started":"2024-03-14T16:35:43.598197Z","shell.execute_reply":"2024-03-14T16:35:43.598217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\ntrain_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\ntrain_loader = DataLoader(dataset_train, batch_size=64, num_workers=4, sampler=train_sampler, drop_last=True)                        \nvalid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=4, shuffle=False, drop_last=True)\n\ndel dataset_train, dataset_valid\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.599982Z","iopub.status.idle":"2024-03-14T16:35:43.600322Z","shell.execute_reply.started":"2024-03-14T16:35:43.600158Z","shell.execute_reply":"2024-03-14T16:35:43.600173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = torch.load('/kaggle/working/model-epoch=0001-v1.ckpt') \n\ntrain_loss = model.trainer.logged_metrics['train_loss']\nval_loss = model.trainer.logged_metrics['val_loss']\nepochs = range(1, len(train_loss) + 1)  # Assuming train_loss and val_loss have same length\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_loss, label='Train Loss', marker='o')\nplt.plot(epochs, val_loss, label='Validation Loss', marker='o')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.601670Z","iopub.status.idle":"2024-03-14T16:35:43.602014Z","shell.execute_reply.started":"2024-03-14T16:35:43.601829Z","shell.execute_reply":"2024-03-14T16:35:43.601842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.603233Z","iopub.status.idle":"2024-03-14T16:35:43.603582Z","shell.execute_reply.started":"2024-03-14T16:35:43.603408Z","shell.execute_reply":"2024-03-14T16:35:43.603423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyLightningModel.load_from_checkpoint('/kaggle/working/example.ckpt')\nmodel.eval()\n\ncpu_device = torch.device('cpu')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nl2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\nl1_loss = torch.nn.L1Loss() # solution is sparse\n\nwith torch.no_grad():\n    # loop 1 on training data\n    train_loss = []\n    # train_loss_scaled = []\n    preds_list_train = []\n    masks_list_train = []\n\n    for imgs, masks in tqdm(train_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True) # .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        train_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_train.append(masks.to(cpu_device).numpy())\n        preds_list_train.append(preds.to(cpu_device).numpy())\n\n    train_loss = np.array(train_loss)\n    print(f'train_loss: {train_loss.mean()}')\n    \n    val_loss = []\n    preds_list_val = []\n    masks_list_val = []\n\n    # loop 2 on validation data\n    for imgs, masks in tqdm(valid_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)# .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        val_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_val.append(masks.to(cpu_device).numpy())\n        preds_list_val.append(preds.to(cpu_device).numpy())\n\n    val_loss = np.array(val_loss)\n    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.605044Z","iopub.status.idle":"2024-03-14T16:35:43.605361Z","shell.execute_reply.started":"2024-03-14T16:35:43.605206Z","shell.execute_reply":"2024-03-14T16:35:43.605219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_list_train = np.concatenate(preds_list_train)\nmasks_list_train = np.concatenate(masks_list_train)\n\npreds_list_val = np.concatenate(preds_list_val)\nmasks_list_val = np.concatenate(masks_list_val)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.606865Z","iopub.status.idle":"2024-03-14T16:35:43.607220Z","shell.execute_reply.started":"2024-03-14T16:35:43.607057Z","shell.execute_reply":"2024-03-14T16:35:43.607071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    \n    v_max = max(truth.max(), pred.max())\n    v_min = max(truth.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[2].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\nfor sample_idx in range(1): #12):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_train[sample_idx*100+time_step, :, :]\n        masks = masks_list_train[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks, \n            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n\n        \nfor sample_idx in range(1): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_val[sample_idx*100+time_step, :, :]\n        masks = masks_list_val[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.609047Z","iopub.status.idle":"2024-03-14T16:35:43.609366Z","shell.execute_reply.started":"2024-03-14T16:35:43.609211Z","shell.execute_reply":"2024-03-14T16:35:43.609225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# for dirname, _, filenames in os.walk('/kaggle/working/tb_logs'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:35:43.610758Z","iopub.status.idle":"2024-03-14T16:35:43.611130Z","shell.execute_reply.started":"2024-03-14T16:35:43.610945Z","shell.execute_reply":"2024-03-14T16:35:43.610967Z"},"trusted":true},"execution_count":null,"outputs":[]}]}