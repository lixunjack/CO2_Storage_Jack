{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7840557,"sourceType":"datasetVersion","datasetId":4596400}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:17.159216Z","iopub.execute_input":"2024-03-14T09:16:17.159677Z","iopub.status.idle":"2024-03-14T09:16:17.168019Z","shell.execute_reply.started":"2024-03-14T09:16:17.159647Z","shell.execute_reply":"2024-03-14T09:16:17.166627Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction/')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:17.170102Z","iopub.execute_input":"2024-03-14T09:16:17.171304Z","iopub.status.idle":"2024-03-14T09:16:17.191413Z","shell.execute_reply.started":"2024-03-14T09:16:17.171272Z","shell.execute_reply":"2024-03-14T09:16:17.189941Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n/kaggle/input/helpfunction/dataset.py\n/kaggle/input/helpfunction/pre_processing.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip install lightning\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:17.193477Z","iopub.execute_input":"2024-03-14T09:16:17.194035Z","iopub.status.idle":"2024-03-14T09:16:49.740456Z","shell.execute_reply.started":"2024-03-14T09:16:17.194003Z","shell.execute_reply":"2024-03-14T09:16:49.738875Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2+cpu)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2+cpu)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2+cpu)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:49.744659Z","iopub.execute_input":"2024-03-14T09:16:49.745065Z","iopub.status.idle":"2024-03-14T09:16:49.754110Z","shell.execute_reply.started":"2024-03-14T09:16:49.745032Z","shell.execute_reply":"2024-03-14T09:16:49.752747Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=1,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        loss = self.l2_loss(preds, masks)\n        \n        self.log('train_loss', loss, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        val_loss = self.l2_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        test_loss = self.l2_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n    \n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:49.755673Z","iopub.execute_input":"2024-03-14T09:16:49.756062Z","iopub.status.idle":"2024-03-14T09:16:49.771034Z","shell.execute_reply.started":"2024-03-14T09:16:49.756034Z","shell.execute_reply":"2024-03-14T09:16:49.769864Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=32):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        #self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        \n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        \n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n             \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=2, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2,persistent_workers=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:49.772680Z","iopub.execute_input":"2024-03-14T09:16:49.773039Z","iopub.status.idle":"2024-03-14T09:16:49.789068Z","shell.execute_reply.started":"2024-03-14T09:16:49.773009Z","shell.execute_reply":"2024-03-14T09:16:49.787920Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\nIf youâ€™re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:49.791228Z","iopub.execute_input":"2024-03-14T09:16:49.791699Z","iopub.status.idle":"2024-03-14T09:16:49.806068Z","shell.execute_reply.started":"2024-03-14T09:16:49.791666Z","shell.execute_reply":"2024-03-14T09:16:49.804634Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n# \n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n\n#consider trying this https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n\n#fast_dev_run=True,\n#logger=logger\n\ntrainer = L.Trainer(max_epochs=20, profiler=\"advanced\",\\\n                     callbacks=[DeviceStatsMonitor(), ModelSummary(max_depth=-1), EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=3, verbose=False, mode=\"max\")])\n\n#VisualizationCallback(data_module)\n#TQDMProgressBar(refresh_rate=10)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\n\n# \n# trainer = Trainer(callbacks=[DeviceStatsMonitor()])\n\ntrainer.fit(model, data_module)\n\n\n# test the model \ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T09:16:49.807535Z","iopub.execute_input":"2024-03-14T09:16:49.807876Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"INFO: Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\nINFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: \n    | Name                                        | Type             | Params\n-----------------------------------------------------------------------------------\n0   | model                                       | Unet             | 24.4 M\n1   | model.encoder                               | ResNetEncoder    | 21.3 M\n2   | model.encoder.conv1                         | Conv2d           | 12.5 K\n3   | model.encoder.bn1                           | BatchNorm2d      | 128   \n4   | model.encoder.relu                          | ReLU             | 0     \n5   | model.encoder.maxpool                       | MaxPool2d        | 0     \n6   | model.encoder.layer1                        | Sequential       | 221 K \n7   | model.encoder.layer1.0                      | BasicBlock       | 74.0 K\n8   | model.encoder.layer1.0.conv1                | Conv2d           | 36.9 K\n9   | model.encoder.layer1.0.bn1                  | BatchNorm2d      | 128   \n10  | model.encoder.layer1.0.relu                 | ReLU             | 0     \n11  | model.encoder.layer1.0.conv2                | Conv2d           | 36.9 K\n12  | model.encoder.layer1.0.bn2                  | BatchNorm2d      | 128   \n13  | model.encoder.layer1.1                      | BasicBlock       | 74.0 K\n14  | model.encoder.layer1.1.conv1                | Conv2d           | 36.9 K\n15  | model.encoder.layer1.1.bn1                  | BatchNorm2d      | 128   \n16  | model.encoder.layer1.1.relu                 | ReLU             | 0     \n17  | model.encoder.layer1.1.conv2                | Conv2d           | 36.9 K\n18  | model.encoder.layer1.1.bn2                  | BatchNorm2d      | 128   \n19  | model.encoder.layer1.2                      | BasicBlock       | 74.0 K\n20  | model.encoder.layer1.2.conv1                | Conv2d           | 36.9 K\n21  | model.encoder.layer1.2.bn1                  | BatchNorm2d      | 128   \n22  | model.encoder.layer1.2.relu                 | ReLU             | 0     \n23  | model.encoder.layer1.2.conv2                | Conv2d           | 36.9 K\n24  | model.encoder.layer1.2.bn2                  | BatchNorm2d      | 128   \n25  | model.encoder.layer2                        | Sequential       | 1.1 M \n26  | model.encoder.layer2.0                      | BasicBlock       | 230 K \n27  | model.encoder.layer2.0.conv1                | Conv2d           | 73.7 K\n28  | model.encoder.layer2.0.bn1                  | BatchNorm2d      | 256   \n29  | model.encoder.layer2.0.relu                 | ReLU             | 0     \n30  | model.encoder.layer2.0.conv2                | Conv2d           | 147 K \n31  | model.encoder.layer2.0.bn2                  | BatchNorm2d      | 256   \n32  | model.encoder.layer2.0.downsample           | Sequential       | 8.4 K \n33  | model.encoder.layer2.0.downsample.0         | Conv2d           | 8.2 K \n34  | model.encoder.layer2.0.downsample.1         | BatchNorm2d      | 256   \n35  | model.encoder.layer2.1                      | BasicBlock       | 295 K \n36  | model.encoder.layer2.1.conv1                | Conv2d           | 147 K \n37  | model.encoder.layer2.1.bn1                  | BatchNorm2d      | 256   \n38  | model.encoder.layer2.1.relu                 | ReLU             | 0     \n39  | model.encoder.layer2.1.conv2                | Conv2d           | 147 K \n40  | model.encoder.layer2.1.bn2                  | BatchNorm2d      | 256   \n41  | model.encoder.layer2.2                      | BasicBlock       | 295 K \n42  | model.encoder.layer2.2.conv1                | Conv2d           | 147 K \n43  | model.encoder.layer2.2.bn1                  | BatchNorm2d      | 256   \n44  | model.encoder.layer2.2.relu                 | ReLU             | 0     \n45  | model.encoder.layer2.2.conv2                | Conv2d           | 147 K \n46  | model.encoder.layer2.2.bn2                  | BatchNorm2d      | 256   \n47  | model.encoder.layer2.3                      | BasicBlock       | 295 K \n48  | model.encoder.layer2.3.conv1                | Conv2d           | 147 K \n49  | model.encoder.layer2.3.bn1                  | BatchNorm2d      | 256   \n50  | model.encoder.layer2.3.relu                 | ReLU             | 0     \n51  | model.encoder.layer2.3.conv2                | Conv2d           | 147 K \n52  | model.encoder.layer2.3.bn2                  | BatchNorm2d      | 256   \n53  | model.encoder.layer3                        | Sequential       | 6.8 M \n54  | model.encoder.layer3.0                      | BasicBlock       | 919 K \n55  | model.encoder.layer3.0.conv1                | Conv2d           | 294 K \n56  | model.encoder.layer3.0.bn1                  | BatchNorm2d      | 512   \n57  | model.encoder.layer3.0.relu                 | ReLU             | 0     \n58  | model.encoder.layer3.0.conv2                | Conv2d           | 589 K \n59  | model.encoder.layer3.0.bn2                  | BatchNorm2d      | 512   \n60  | model.encoder.layer3.0.downsample           | Sequential       | 33.3 K\n61  | model.encoder.layer3.0.downsample.0         | Conv2d           | 32.8 K\n62  | model.encoder.layer3.0.downsample.1         | BatchNorm2d      | 512   \n63  | model.encoder.layer3.1                      | BasicBlock       | 1.2 M \n64  | model.encoder.layer3.1.conv1                | Conv2d           | 589 K \n65  | model.encoder.layer3.1.bn1                  | BatchNorm2d      | 512   \n66  | model.encoder.layer3.1.relu                 | ReLU             | 0     \n67  | model.encoder.layer3.1.conv2                | Conv2d           | 589 K \n68  | model.encoder.layer3.1.bn2                  | BatchNorm2d      | 512   \n69  | model.encoder.layer3.2                      | BasicBlock       | 1.2 M \n70  | model.encoder.layer3.2.conv1                | Conv2d           | 589 K \n71  | model.encoder.layer3.2.bn1                  | BatchNorm2d      | 512   \n72  | model.encoder.layer3.2.relu                 | ReLU             | 0     \n73  | model.encoder.layer3.2.conv2                | Conv2d           | 589 K \n74  | model.encoder.layer3.2.bn2                  | BatchNorm2d      | 512   \n75  | model.encoder.layer3.3                      | BasicBlock       | 1.2 M \n76  | model.encoder.layer3.3.conv1                | Conv2d           | 589 K \n77  | model.encoder.layer3.3.bn1                  | BatchNorm2d      | 512   \n78  | model.encoder.layer3.3.relu                 | ReLU             | 0     \n79  | model.encoder.layer3.3.conv2                | Conv2d           | 589 K \n80  | model.encoder.layer3.3.bn2                  | BatchNorm2d      | 512   \n81  | model.encoder.layer3.4                      | BasicBlock       | 1.2 M \n82  | model.encoder.layer3.4.conv1                | Conv2d           | 589 K \n83  | model.encoder.layer3.4.bn1                  | BatchNorm2d      | 512   \n84  | model.encoder.layer3.4.relu                 | ReLU             | 0     \n85  | model.encoder.layer3.4.conv2                | Conv2d           | 589 K \n86  | model.encoder.layer3.4.bn2                  | BatchNorm2d      | 512   \n87  | model.encoder.layer3.5                      | BasicBlock       | 1.2 M \n88  | model.encoder.layer3.5.conv1                | Conv2d           | 589 K \n89  | model.encoder.layer3.5.bn1                  | BatchNorm2d      | 512   \n90  | model.encoder.layer3.5.relu                 | ReLU             | 0     \n91  | model.encoder.layer3.5.conv2                | Conv2d           | 589 K \n92  | model.encoder.layer3.5.bn2                  | BatchNorm2d      | 512   \n93  | model.encoder.layer4                        | Sequential       | 13.1 M\n94  | model.encoder.layer4.0                      | BasicBlock       | 3.7 M \n95  | model.encoder.layer4.0.conv1                | Conv2d           | 1.2 M \n96  | model.encoder.layer4.0.bn1                  | BatchNorm2d      | 1.0 K \n97  | model.encoder.layer4.0.relu                 | ReLU             | 0     \n98  | model.encoder.layer4.0.conv2                | Conv2d           | 2.4 M \n99  | model.encoder.layer4.0.bn2                  | BatchNorm2d      | 1.0 K \n100 | model.encoder.layer4.0.downsample           | Sequential       | 132 K \n101 | model.encoder.layer4.0.downsample.0         | Conv2d           | 131 K \n102 | model.encoder.layer4.0.downsample.1         | BatchNorm2d      | 1.0 K \n103 | model.encoder.layer4.1                      | BasicBlock       | 4.7 M \n104 | model.encoder.layer4.1.conv1                | Conv2d           | 2.4 M \n105 | model.encoder.layer4.1.bn1                  | BatchNorm2d      | 1.0 K \n106 | model.encoder.layer4.1.relu                 | ReLU             | 0     \n107 | model.encoder.layer4.1.conv2                | Conv2d           | 2.4 M \n108 | model.encoder.layer4.1.bn2                  | BatchNorm2d      | 1.0 K \n109 | model.encoder.layer4.2                      | BasicBlock       | 4.7 M \n110 | model.encoder.layer4.2.conv1                | Conv2d           | 2.4 M \n111 | model.encoder.layer4.2.bn1                  | BatchNorm2d      | 1.0 K \n112 | model.encoder.layer4.2.relu                 | ReLU             | 0     \n113 | model.encoder.layer4.2.conv2                | Conv2d           | 2.4 M \n114 | model.encoder.layer4.2.bn2                  | BatchNorm2d      | 1.0 K \n115 | model.decoder                               | UnetDecoder      | 3.2 M \n116 | model.decoder.center                        | Identity         | 0     \n117 | model.decoder.blocks                        | ModuleList       | 3.2 M \n118 | model.decoder.blocks.0                      | DecoderBlock     | 2.4 M \n119 | model.decoder.blocks.0.conv1                | Conv2dReLU       | 1.8 M \n120 | model.decoder.blocks.0.conv1.0              | Conv2d           | 1.8 M \n121 | model.decoder.blocks.0.conv1.1              | BatchNorm2d      | 512   \n122 | model.decoder.blocks.0.conv1.2              | ReLU             | 0     \n123 | model.decoder.blocks.0.attention1           | Attention        | 0     \n124 | model.decoder.blocks.0.attention1.attention | Identity         | 0     \n125 | model.decoder.blocks.0.conv2                | Conv2dReLU       | 590 K \n126 | model.decoder.blocks.0.conv2.0              | Conv2d           | 589 K \n127 | model.decoder.blocks.0.conv2.1              | BatchNorm2d      | 512   \n128 | model.decoder.blocks.0.conv2.2              | ReLU             | 0     \n129 | model.decoder.blocks.0.attention2           | Attention        | 0     \n130 | model.decoder.blocks.0.attention2.attention | Identity         | 0     \n131 | model.decoder.blocks.1                      | DecoderBlock     | 590 K \n132 | model.decoder.blocks.1.conv1                | Conv2dReLU       | 442 K \n133 | model.decoder.blocks.1.conv1.0              | Conv2d           | 442 K \n134 | model.decoder.blocks.1.conv1.1              | BatchNorm2d      | 256   \n135 | model.decoder.blocks.1.conv1.2              | ReLU             | 0     \n136 | model.decoder.blocks.1.attention1           | Attention        | 0     \n137 | model.decoder.blocks.1.attention1.attention | Identity         | 0     \n138 | model.decoder.blocks.1.conv2                | Conv2dReLU       | 147 K \n139 | model.decoder.blocks.1.conv2.0              | Conv2d           | 147 K \n140 | model.decoder.blocks.1.conv2.1              | BatchNorm2d      | 256   \n141 | model.decoder.blocks.1.conv2.2              | ReLU             | 0     \n142 | model.decoder.blocks.1.attention2           | Attention        | 0     \n143 | model.decoder.blocks.1.attention2.attention | Identity         | 0     \n144 | model.decoder.blocks.2                      | DecoderBlock     | 147 K \n145 | model.decoder.blocks.2.conv1                | Conv2dReLU       | 110 K \n146 | model.decoder.blocks.2.conv1.0              | Conv2d           | 110 K \n147 | model.decoder.blocks.2.conv1.1              | BatchNorm2d      | 128   \n148 | model.decoder.blocks.2.conv1.2              | ReLU             | 0     \n149 | model.decoder.blocks.2.attention1           | Attention        | 0     \n150 | model.decoder.blocks.2.attention1.attention | Identity         | 0     \n151 | model.decoder.blocks.2.conv2                | Conv2dReLU       | 37.0 K\n152 | model.decoder.blocks.2.conv2.0              | Conv2d           | 36.9 K\n153 | model.decoder.blocks.2.conv2.1              | BatchNorm2d      | 128   \n154 | model.decoder.blocks.2.conv2.2              | ReLU             | 0     \n155 | model.decoder.blocks.2.attention2           | Attention        | 0     \n156 | model.decoder.blocks.2.attention2.attention | Identity         | 0     \n157 | model.decoder.blocks.3                      | DecoderBlock     | 46.2 K\n158 | model.decoder.blocks.3.conv1                | Conv2dReLU       | 36.9 K\n159 | model.decoder.blocks.3.conv1.0              | Conv2d           | 36.9 K\n160 | model.decoder.blocks.3.conv1.1              | BatchNorm2d      | 64    \n161 | model.decoder.blocks.3.conv1.2              | ReLU             | 0     \n162 | model.decoder.blocks.3.attention1           | Attention        | 0     \n163 | model.decoder.blocks.3.attention1.attention | Identity         | 0     \n164 | model.decoder.blocks.3.conv2                | Conv2dReLU       | 9.3 K \n165 | model.decoder.blocks.3.conv2.0              | Conv2d           | 9.2 K \n166 | model.decoder.blocks.3.conv2.1              | BatchNorm2d      | 64    \n167 | model.decoder.blocks.3.conv2.2              | ReLU             | 0     \n168 | model.decoder.blocks.3.attention2           | Attention        | 0     \n169 | model.decoder.blocks.3.attention2.attention | Identity         | 0     \n170 | model.decoder.blocks.4                      | DecoderBlock     | 7.0 K \n171 | model.decoder.blocks.4.conv1                | Conv2dReLU       | 4.6 K \n172 | model.decoder.blocks.4.conv1.0              | Conv2d           | 4.6 K \n173 | model.decoder.blocks.4.conv1.1              | BatchNorm2d      | 32    \n174 | model.decoder.blocks.4.conv1.2              | ReLU             | 0     \n175 | model.decoder.blocks.4.attention1           | Attention        | 0     \n176 | model.decoder.blocks.4.attention1.attention | Identity         | 0     \n177 | model.decoder.blocks.4.conv2                | Conv2dReLU       | 2.3 K \n178 | model.decoder.blocks.4.conv2.0              | Conv2d           | 2.3 K \n179 | model.decoder.blocks.4.conv2.1              | BatchNorm2d      | 32    \n180 | model.decoder.blocks.4.conv2.2              | ReLU             | 0     \n181 | model.decoder.blocks.4.attention2           | Attention        | 0     \n182 | model.decoder.blocks.4.attention2.attention | Identity         | 0     \n183 | model.segmentation_head                     | SegmentationHead | 145   \n184 | model.segmentation_head.0                   | Conv2d           | 145   \n185 | model.segmentation_head.1                   | Identity         | 0     \n186 | model.segmentation_head.2                   | Activation       | 0     \n187 | model.segmentation_head.2.activation        | Sigmoid          | 0     \n188 | l2_loss                                     | MSELoss          | 0     \n189 | l1_loss                                     | L1Loss           | 0     \n-----------------------------------------------------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.758    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24080f209e954b6ebb9a0f743b6d335e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]}]}