{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7843411,"sourceType":"datasetVersion","datasetId":4598564}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.921475Z","iopub.execute_input":"2024-03-14T19:28:27.922463Z","iopub.status.idle":"2024-03-14T19:28:27.929453Z","shell.execute_reply.started":"2024-03-14T19:28:27.922425Z","shell.execute_reply":"2024-03-14T19:28:27.928354Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction3/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.955831Z","iopub.execute_input":"2024-03-14T19:28:27.956105Z","iopub.status.idle":"2024-03-14T19:28:27.964187Z","shell.execute_reply.started":"2024-03-14T19:28:27.956081Z","shell.execute_reply":"2024-03-14T19:28:27.963283Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/input/helpfunction3/dataset.py\n/kaggle/input/helpfunction3/pre_processing.py\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.997394Z","iopub.execute_input":"2024-03-14T19:28:27.997638Z","iopub.status.idle":"2024-03-14T19:29:21.869794Z","shell.execute_reply.started":"2024-03-14T19:28:27.997618Z","shell.execute_reply":"2024-03-14T19:29:21.868441Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.872442Z","iopub.execute_input":"2024-03-14T19:29:21.873392Z","iopub.status.idle":"2024-03-14T19:29:21.880457Z","shell.execute_reply.started":"2024-03-14T19:29:21.873331Z","shell.execute_reply":"2024-03-14T19:29:21.879397Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=1,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n        self.record_trainloss=[]\n        self.record_valoss=[]\n        self.record_testloss=[]\n        \n        self.validation_step_outputs = []\n        self.training_step_outputs = []\n    \n    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        loss = self.l1_loss(preds, masks)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        self.training_step_outputs.append(loss)\n\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        self.validation_step_outputs.append(val_loss)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        \n        metrics = {\"test_loss\": test_loss}\n        self.log_dict(metrics)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n    def on_train_epoch_end(self):\n        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n        \n        self.record_trainloss.append(avg_train_loss)\n        \n        self.training_step_outputs=[]\n        #self.log('all_train_losses', all_train_loss)\n\n    def on_validation_epoch_end(self):\n        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n        \n        \n        self.record_valoss.append(avg_val_loss)\n        \n        self.validation_step_outputs=[]\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.881919Z","iopub.execute_input":"2024-03-14T19:29:21.882195Z","iopub.status.idle":"2024-03-14T19:29:21.898484Z","shell.execute_reply.started":"2024-03-14T19:29:21.882172Z","shell.execute_reply":"2024-03-14T19:29:21.897629Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        \n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        \n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n             \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=4, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4,persistent_workers=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.900463Z","iopub.execute_input":"2024-03-14T19:29:21.900863Z","iopub.status.idle":"2024-03-14T19:29:21.914209Z","shell.execute_reply.started":"2024-03-14T19:29:21.900838Z","shell.execute_reply":"2024-03-14T19:29:21.913192Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.915554Z","iopub.execute_input":"2024-03-14T19:29:21.915866Z","iopub.status.idle":"2024-03-14T19:29:21.928051Z","shell.execute_reply.started":"2024-03-14T19:29:21.915843Z","shell.execute_reply":"2024-03-14T19:29:21.927320Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=1,  # Save all checkpoints\n    monitor=None,  # Disable monitoring\n    verbose=True\n)\n\ntrainer = L.Trainer(max_epochs=8,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=20),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, \\\n                                              verbose=False)])\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\ntrainer.fit(model, data_module)\n\ntrain_loss=model.record_trainloss\nval_loss=model.record_valoss\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n\n# test the model \ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.929284Z","iopub.execute_input":"2024-03-14T19:29:21.929576Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type    | Params\n------------------------------------\n0 | model   | Unet    | 24.4 M\n1 | l2_loss | MSELoss | 0     \n2 | l1_loss | L1Loss  | 0     \n------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.758    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d27644a7cb441d5afd99b3c84f458d5"}},"metadata":{}}]},{"cell_type":"code","source":"train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\nval_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n\nplt.figure()\nplt.plot(train_losscpu, label=\"training Loss\")\nplt.plot(val_losscpu, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf youâ€™re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\nfor idx_ in range(4):\n    current_timestep = 10*idx_\n    print(f'plotting for time step: {current_timestep}')\n    image, mask = data_module.example_dataset[current_timestep] # get some sample\n    visualize(\n        concentration=image[0,:, :].squeeze(),\n        eps=image[1,:, :].squeeze(),\n        Ux=image[2,:, :].squeeze(),\n        Uy=image[3,:, :].squeeze(),\n        dissolution=mask.squeeze(),\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use original code to do plotting (see test results!)","metadata":{}},{"cell_type":"code","source":"def read_simulation_hdf(file_name):\n    print(f'loading the file: {file_name}')\n    data_dict = {}\n\n    with h5py.File(file_name, \"r\") as file_handle:\n        # List all groups\n        print(f\"Keys: {file_handle.keys()}\")\n        scaling_factor = 1\n        for key_ in file_handle.keys():\n            if 'key_' == 'C':\n                scaling_factor = 1 # 100\n            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n                scaling_factor = 1 # 1000\n            \n            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n\n        print(f'Done with {file_name} == closing file now')\n\n    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n\n\ndef load_datafiles(data_filenames):\n    # snapshot_indices will split the data in time into train and validation\n    data_dict = {\n        'C': [], # list of np arrays\n        'eps': [],\n        'Ux': [],\n        'Uy': [],\n    }\n\n    for filename in data_filenames:\n        C, eps, Ux, Uy = read_simulation_hdf(filename)\n        data_dict['C'].append(C[2:-2, 2:-2, :])\n        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n    return data_dict\n\ndef get_filelist():\n    from itertools import permutations, product\n    # permutations = list(permutations(range(4), 2))\n    permutations = list(product(range(4), repeat=2))\n\n    file_list = []\n    properties_list = []\n    for idx1, idx2 in permutations:\n        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n        file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n        file_list.append(file_name)\n    return file_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, RandomSampler\n\ndef preprocess_data_cube(data_dict, scaling_dict):\n    print(f'preprocess_data_cube')\n\n    masks = []\n    images = []\n    for file_idx in range(len(data_dict['C'])):\n        C = data_dict['C'][file_idx][:, :, :-1]\n        eps = data_dict['eps'][file_idx][:, :, :-1]\n        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n\n        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n        mask = eps_t #- eps\n\n        # these should be moved to preprocessing\n        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n        C = C*scaling_dict['C_scaling'] - 0.5\n        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n\n\n        image = np.stack([C, eps, Ux, Uy], axis=-1)\n        image = np.swapaxes(image, 3, 2)\n\n        masks.append(mask)\n        images.append(image)\n    \n    masks = np.concatenate(masks, axis=-1)\n    images = np.concatenate(images, axis=-1)\n    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n    return images, masks\n\nclass DissolutionDataset(Dataset):\n    \"\"\"Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        data_dir (str): path to data folder\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n  \n    def __init__(\n            self,\n            data_filenames,\n            scaling_dict,\n            augmentation=None, \n            preprocessing=None,\n    ):\n\n        # self.scaling_dict = scaling_dict\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        data_dict = load_datafiles(data_filenames)\n        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n        print(self.image.shape, self.mask.shape)\n        self.data_len = self.image.shape[-1]\n\n    \n    def __getitem__(self, idx):\n        \n        image = self.image[:, :, :, idx]\n        mask = self.mask[:, :, idx]\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # assume one file for now\n        return self.data_len # last element we cann't predict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor_img(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef to_tensor_mask(x, **kwargs):\n    return x.astype('float32')\n\ndef get_preprocessing():\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n    ]\n    return albu.Compose(_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import RandomSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = get_filelist()\nprint(file_list)\n\ndata_list = load_datafiles(file_list)\n# estimate sample mean and std -- this should be done better\n\nn_training_samples = 8\n\n\nC = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\neps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\nUx = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\nUy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n\nUx_mean, Ux_std = Ux.mean(), Ux.std()\nUy_mean, Uy_std = Uy.mean(), Uy.std()\neps_mean, eps_std = eps.mean(), eps.std()\n\n\nprint(Ux_mean, Ux_std)\nprint(Uy_mean, Uy_std)\nprint(eps_mean, eps_std)\n\nC_scaling = 100\ndata_scalingdict = {\n    'C_scaling': C_scaling,\n    'Ux_mean': Ux_mean,\n    'Ux_std': Ux_std,\n    'Uy_mean': Uy_mean,\n    'Uy_std': Uy_std,\n    'eps_mean': eps_mean,\n    'eps_std': eps_std,\n}\n\ndel Ux, Uy, eps, C","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\ndata_filenames = get_filelist()\n\ndataset_train = DissolutionDataset(\n    data_filenames[:8],\n    scaling_dict=data_scalingdict,\n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndataset_valid = DissolutionDataset(\n    data_filenames[12:],\n    scaling_dict=data_scalingdict,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndel data_scalingdict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\ntrain_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\ntrain_loader = DataLoader(dataset_train, batch_size=64, num_workers=4, sampler=train_sampler, drop_last=True)                        \nvalid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=4, shuffle=False, drop_last=True)\n\ndel dataset_train, dataset_valid\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyLightningModel.load_from_checkpoint('/kaggle/working/example.ckpt')\nmodel.eval()\n\ncpu_device = torch.device('cpu')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nl2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\nl1_loss = torch.nn.L1Loss() # solution is sparse\n\nwith torch.no_grad():\n    # loop 1 on training data\n    train_loss = []\n    # train_loss_scaled = []\n    preds_list_train = []\n    masks_list_train = []\n\n    for imgs, masks in tqdm(train_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True) # .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        train_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_train.append(masks.to(cpu_device).numpy())\n        preds_list_train.append(preds.to(cpu_device).numpy())\n\n    train_loss = np.array(train_loss)\n    print(f'train_loss: {train_loss.mean()}')\n    \n    val_loss = []\n    preds_list_val = []\n    masks_list_val = []\n\n    # loop 2 on validation data\n    for imgs, masks in tqdm(valid_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)# .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        val_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_val.append(masks.to(cpu_device).numpy())\n        preds_list_val.append(preds.to(cpu_device).numpy())\n\n    val_loss = np.array(val_loss)\n    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_list_train = np.concatenate(preds_list_train)\nmasks_list_train = np.concatenate(masks_list_train)\n\npreds_list_val = np.concatenate(preds_list_val)\nmasks_list_val = np.concatenate(masks_list_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    \n    v_max = max(truth.max(), pred.max())\n    v_min = max(truth.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[2].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\nfor sample_idx in range(1): #12):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_train[sample_idx*100+time_step, :, :]\n        masks = masks_list_train[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks, \n            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n\n        \nfor sample_idx in range(1): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_val[sample_idx*100+time_step, :, :]\n        masks = masks_list_val[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# for dirname, _, filenames in os.walk('/kaggle/working/tb_logs'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}