{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7847033,"sourceType":"datasetVersion","datasetId":4601199}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Script run on www.Kaggle.com\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:31:30.804075Z","iopub.execute_input":"2024-03-15T19:31:30.804487Z","iopub.status.idle":"2024-03-15T19:31:30.967401Z","shell.execute_reply.started":"2024-03-15T19:31:30.804454Z","shell.execute_reply":"2024-03-15T19:31:30.966558Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#you need to put dataset.py and pre_processing.py files as a \"dataset\" in the folder named helpfunction9\nimport sys\nsys.path.append('/kaggle/input/helpfunction9/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:31:30.969342Z","iopub.execute_input":"2024-03-15T19:31:30.969608Z","iopub.status.idle":"2024-03-15T19:31:31.007496Z","shell.execute_reply.started":"2024-03-15T19:31:30.969585Z","shell.execute_reply":"2024-03-15T19:31:31.006634Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n/kaggle/input/helpfunction9/dataset.py\n/kaggle/input/helpfunction9/pre_processing.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n!pip install torchviz","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:31:31.008692Z","iopub.execute_input":"2024-03-15T19:31:31.008955Z","iopub.status.idle":"2024-03-15T19:32:48.327644Z","shell.execute_reply.started":"2024-03-15T19:31:31.008932Z","shell.execute_reply":"2024-03-15T19:32:48.326314Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\n  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=a1f40605bb4b9c74fe67be0c30eeb41ffbd3f3afeebe96dea87227e6de31a6ba\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=eb3bff21f6d5d7b455015d7a825120dd3dcc7b8b80737ac4420326403cc64d94\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.16\n    Uninstalling timm-0.9.16:\n      Successfully uninstalled timm-0.9.16\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\nLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting lightning\n  Downloading lightning-2.2.1-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.1\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nCollecting torchviz\n  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=6476025ea776207db089a1783613a8832430814c05627b6acdec1749f35939c2\n  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:32:48.332425Z","iopub.execute_input":"2024-03-15T19:32:48.332731Z","iopub.status.idle":"2024-03-15T19:33:00.822784Z","shell.execute_reply.started":"2024-03-15T19:32:48.332705Z","shell.execute_reply":"2024-03-15T19:33:00.821742Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=4,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n        self.record_trainloss=[]\n        self.record_valoss=[]\n        self.record_testloss=[]\n        \n        self.validation_step_outputs = []\n        self.training_step_outputs = []\n    \n    \n#         #record test\n#         self.all_test_input=[]\n#         self.all_test_preds = []\n#         self.all_test_masks = []\n            \n        \n    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        loss = self.l1_loss(preds, masks)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        self.training_step_outputs.append(loss)\n\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        #print(f\"imgs {imgs.shape},masks{masks.shape},preds{preds.shape}\")\n        \n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        self.validation_step_outputs.append(val_loss)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n#         self.all_test_input.append( imgs[:,1,:,:].squeeze() )\n#         self.all_test_masks.append( masks[:,1,:,:].squeeze() )\n#         self.all_test_preds.append( preds[:,1,:,:].squeeze() )\n        \n        \n        \n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        \n#         metrics = {\"test_loss\": test_loss}\n#         self.log_dict(metrics)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n    def on_train_epoch_end(self):\n        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n        \n        self.record_trainloss.append(avg_train_loss)\n        \n        self.training_step_outputs=[]\n        #self.log('all_train_losses', all_train_loss)\n        \n        \n    \n    def on_validation_epoch_end(self):\n        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n        \n        \n        self.record_valoss.append(avg_val_loss)\n        \n        self.validation_step_outputs=[]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:33:00.827591Z","iopub.execute_input":"2024-03-15T19:33:00.828145Z","iopub.status.idle":"2024-03-15T19:33:00.845498Z","shell.execute_reply.started":"2024-03-15T19:33:00.828109Z","shell.execute_reply":"2024-03-15T19:33:00.844518Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        #self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples], None, self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], None, self.preprocessing)\n     \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=2, drop_last=False, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False, drop_last=False, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False, persistent_workers=True)\n    \n    \n    def get_train_data(self):\n        return DataLoader(self.train_dataset, batch_size=1, shuffle=False, num_workers=1)\n    \n    def get_test_data(self):\n        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, num_workers=1)\n    \n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:33:00.846704Z","iopub.execute_input":"2024-03-15T19:33:00.847029Z","iopub.status.idle":"2024-03-15T19:33:00.873576Z","shell.execute_reply.started":"2024-03-15T19:33:00.847003Z","shell.execute_reply":"2024-03-15T19:33:00.872755Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n#         albu.HorizontalFlip(p=0.5),\n#         albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:33:00.874692Z","iopub.execute_input":"2024-03-15T19:33:00.875029Z","iopub.status.idle":"2024-03-15T19:33:00.881141Z","shell.execute_reply.started":"2024-03-15T19:33:00.875004Z","shell.execute_reply":"2024-03-15T19:33:00.880191Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=1,  # Save all checkpoints\n    monitor=None,  # Disable monitoring\n    verbose=True\n)\n\n\ntrainer = L.Trainer(max_epochs=7,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=32),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=3, \\\n                                              verbose=False)])\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\n#stop\ntrainer.fit(model, data_module)\n\ntrain_loss=model.record_trainloss\nval_loss=model.record_valoss\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n\n# test the model  [for data to plot!]\ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:33:00.882559Z","iopub.execute_input":"2024-03-15T19:33:00.883044Z","iopub.status.idle":"2024-03-15T19:44:44.512369Z","shell.execute_reply.started":"2024-03-15T19:33:00.883011Z","shell.execute_reply":"2024-03-15T19:44:44.511272Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n2024-03-15 19:33:04.039055: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-15 19:33:04.039167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-15 19:33:04.172379: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type    | Params\n------------------------------------\n0 | model   | Unet    | 24.4 M\n1 | l2_loss | MSELoss | 0     \n2 | l1_loss | L1Loss  | 0     \n------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.760    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd2b1257b96c42048eaa4707bee7897d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=7` reached.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48819c0400d4da09d0678d5fa5759c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.004098101984709501   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.004098101984709501    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[{'test_loss': 0.004098101984709501}]"},"metadata":{}}]},{"cell_type":"code","source":"# from torchviz import make_dot\n\n# x = torch.zeros(64, 4, 256, 256, dtype=torch.float, requires_grad=False)\n# out = model(x)\n\n# make_dot(out) \n\n# make_dot(out, params=dict(list(model.named_parameters()))).render(\"plotmodel\", format=\"png\")","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.515844Z","iopub.execute_input":"2024-03-15T19:44:44.516165Z","iopub.status.idle":"2024-03-15T19:44:44.520369Z","shell.execute_reply.started":"2024-03-15T19:44:44.516136Z","shell.execute_reply":"2024-03-15T19:44:44.519480Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_loss=model.record_trainloss\nval_loss=model.record_valoss","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.521499Z","iopub.execute_input":"2024-03-15T19:44:44.521777Z","iopub.status.idle":"2024-03-15T19:44:44.537657Z","shell.execute_reply.started":"2024-03-15T19:44:44.521753Z","shell.execute_reply":"2024-03-15T19:44:44.536850Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\nval_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n\nplt.figure()\nplt.plot(train_losscpu, label=\"training Loss\")\nplt.plot(val_losscpu, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.538790Z","iopub.execute_input":"2024-03-15T19:44:44.539068Z","iopub.status.idle":"2024-03-15T19:44:44.860150Z","shell.execute_reply.started":"2024-03-15T19:44:44.539043Z","shell.execute_reply":"2024-03-15T19:44:44.859096Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABISklEQVR4nO3de1hU9b4/8PeagRnuA8jdUEJR8QYKguguLdmhmVvNiswCzcuu0DK2ZxfHUqtdWFmHUrempWZlWP3SPJVXjlkZhunGNInSFCi5SMpVuc2s3x8DIyMXB5iZNZf363nmmTVr1uUz5Dnz3t/1Wd8RRFEUQURERGQjZFIXQERERGRMDDdERERkUxhuiIiIyKYw3BAREZFNYbghIiIim8JwQ0RERDaF4YaIiIhsioPUBZibRqPBhQsX4O7uDkEQpC6HiIiIDCCKIqqrqxEUFASZrPOxGbsLNxcuXEBwcLDUZRAREVE3FBUV4aabbup0G7sLN+7u7gC0fxwPDw+JqyEiIiJDVFVVITg4WPc93hm7Czctl6I8PDwYboiIiKyMIS0lbCgmIiIim8JwQ0RERDaF4YaIiIhsit313BARUc+p1Wo0NjZKXQbZGIVCccPbvA3BcENERAYTRRElJSWoqKiQuhSyQTKZDDfffDMUCkWPjsNwQ0REBmsJNn5+fnBxceFkqGQ0LZPsFhcXo0+fPj36t8VwQ0REBlGr1bpg06tXL6nLIRvk6+uLCxcuoKmpCY6Ojt0+DhuKiYjIIC09Ni4uLhJXQraq5XKUWq3u0XEYboiIqEt4KYpMxVj/thhuiIiIyKYw3BAREZFNYbghIiLqgpCQEGRkZBi8/VdffQVBEHj7vBkx3BhTbTlQlid1FURE1Mr48eOxePFiox3v6NGjWLBggcHbjxkzBsXFxVCpVEaroT0MUdcw3BhL/m7g1X7Ajr9LXQkREXWRKIpoamoyaFtfX98u3TGmUCgQEBDARmwzsohws3btWoSEhMDJyQmxsbHIycnpcNstW7ZAEAS9h5OTkxmr7YBfuPa59DTQVC9tLUREZiCKIq40NEnyEEXRoBpnz56NQ4cO4Y033tB9Z5w/f143yrF7925ERUVBqVTi22+/xdmzZzF16lT4+/vDzc0No0aNwoEDB/SOef1lKUEQ8Pbbb2P69OlwcXFBWFgYdu3apXv/+hGVLVu2wNPTE3v37kV4eDjc3NwwceJEFBcX6/ZpamrC448/Dk9PT/Tq1QtPPfUUkpOTMW3atG7/97p8+TKSkpLg5eUFFxcXTJo0Cb/++qvu/YKCAkyZMgVeXl5wdXXFkCFD8OWXX+r2nTVrFnx9feHs7IywsDBs3ry527WYmuST+G3fvh2pqalYv349YmNjkZGRgYSEBOTn58PPz6/dfTw8PJCfn697bRFp2LMv4OQJ1FUAZaeBoBFSV0REZFJXG9UYvGyvJOc+/XwCXBQ3/gp744038Msvv2Do0KF4/vnnAWhHXs6fPw8AePrpp7Fq1SqEhobCy8sLRUVFuPPOO/Hiiy9CqVRi69atmDJlCvLz89GnT58Oz/Pcc8/hlVdewauvvorVq1dj1qxZKCgogLe3d7vbX7lyBatWrcJ7770HmUyGBx98EEuWLMEHH3wAAHj55ZfxwQcfYPPmzQgPD8cbb7yBnTt34rbbbuviX+qa2bNn49dff8WuXbvg4eGBp556CnfeeSdOnz4NR0dHpKSkoKGhAV9//TVcXV1x+vRpuLm5AQCeffZZnD59Grt374aPjw/OnDmDq1evdrsWU5M83Lz++uuYP38+5syZAwBYv349vvjiC2zatAlPP/10u/sIgoCAgABzlnljggAERQK/fQVcyGW4ISKyACqVCgqFAi4uLu1+bzz//PP461//qnvt7e2NiIgI3esXXngBO3bswK5du7Bw4cIOzzN79mzMnDkTAPDSSy/hzTffRE5ODiZOnNju9o2NjVi/fj369esHAFi4cKEufAHA6tWrkZaWhunTpwMA1qxZoxtF6Y6WUHP48GGMGTMGAPDBBx8gODgYO3fuxL333ovCwkLMmDEDw4YNAwCEhobq9i8sLMSIESMQHR0NQDt6ZckkDTcNDQ04duwY0tLSdOtkMhni4+ORnZ3d4X41NTXo27cvNBoNRo4ciZdeeglDhgxpd9v6+nrU11+7TFRVVWW8D3C9wEhtuCnONd05iIgshLOjHKefT5Ds3MbQ8mXdoqamBitWrMAXX3yB4uJiNDU14erVqygsLOz0OMOHD9ctu7q6wsPDA2VlZR1u7+Liogs2ABAYGKjbvrKyEqWlpYiJidG9L5fLERUVBY1G06XP1yIvLw8ODg6IjY3VrevVqxcGDhyIvDztjTCPP/44Hn30Uezbtw/x8fGYMWOG7nM9+uijmDFjBo4fP4477rgD06ZN04UkSyRpz015eTnUajX8/f311vv7+6OkpKTdfQYOHIhNmzbhs88+w/vvvw+NRoMxY8bg999/b3f79PR0qFQq3SM4ONjon0MnKFL7fCHXdOcgIrIQgiDAReEgycNY7Qiurq56r5csWYIdO3bgpZdewjfffIPc3FwMGzYMDQ0NnR7n+t9BEgSh0yDS3vaG9hGZyrx58/Dbb7/hoYcewsmTJxEdHY3Vq1cDACZNmoSCggI8+eSTuHDhAiZMmIAlS5ZIWm9nLKKhuCvi4uKQlJSEyMhIjBs3Dp9++il8fX3x1ltvtbt9WloaKisrdY+ioiLTFRcYqX0uOw00df5/CEREZB4KhcLg3yo6fPgwZs+ejenTp2PYsGEICAjQ9eeYi0qlgr+/P44ePapbp1arcfz48W4fMzw8HE1NTfj+++916/7880/k5+dj8ODBunXBwcF45JFH8Omnn+If//gHNm7cqHvP19cXycnJeP/995GRkYENGzZ0ux5Tk/SylI+PD+RyOUpLS/XWl5aWGtxT4+joiBEjRuDMmTPtvq9UKqFUKntcq0G8Qq5rKo40z3mJiKhDISEh+P7773H+/Hm4ubl12OQLAGFhYfj0008xZcoUCIKAZ599ttuXgnpi0aJFSE9PR//+/TFo0CCsXr0aly9fNmjE6uTJk3B3d9e9FgQBERERmDp1KubPn4+33noL7u7uePrpp9G7d29MnToVALB48WJMmjQJAwYMwOXLl3Hw4EGEh2vvBF62bBmioqIwZMgQ1NfX4/PPP9e9Z4kkHblRKBSIiopCVlaWbp1Go0FWVhbi4uIMOoZarcbJkycRGBhoqjINJwhAYHMjGvtuiIgswpIlSyCXyzF48GD4+vp22j/z+uuvw8vLC2PGjMGUKVOQkJCAkSNHmrFaraeeegozZ85EUlIS4uLi4ObmhoSEBIOmPrn11lsxYsQI3SMqKgoAsHnzZkRFReGuu+5CXFwcRFHEl19+qbtEplarkZKSgvDwcEycOBEDBgzAv//9bwDa7+u0tDQMHz4ct956K+RyOTIzM033B+ghQZT4It/27duRnJyMt956CzExMcjIyMBHH32En3/+Gf7+/khKSkLv3r2Rnp4OQNvZPnr0aPTv3x8VFRV49dVXsXPnThw7dkxvaK0jVVVVUKlUqKyshIeHh/E/0P5lwOE3gKg5wJQM4x+fiEgidXV1OHfuHG6++WbLmF/Mjmg0GoSHh+O+++7DCy+8IHU5JtPZv7GufH9Lfit4YmIiLl68iGXLlqGkpASRkZHYs2ePrsm4sLAQMtm1AabLly9j/vz5KCkpgZeXF6KiovDdd98ZFGzMoqXvhiM3RETUTQUFBdi3bx/GjRuH+vp6rFmzBufOncMDDzwgdWlWQfKRG3Mz+cjNpd+AN0cAcgWQ9gfgoDD+OYiIJMCRG/MpKirC/fffj1OnTkEURQwdOhQrV67ErbfeKnVpJmUzIzc2x+tmwEkF1FUCF/Ou9eAQEREZKDg4GIcPH5a6DKtldbeCW7zWTcWc74aIiMjsGG5MgX03REREkmG4MQXOVExERCQZhhtTaBm5Kf0JUDdKWgoREZG9YbgxBe9QQKkC1PVAWZ7U1RAREdkVhhtTEAQgsPkXYtl3Q0Rk9caPH4/FixfrXoeEhCAjI6PTfQRBwM6dO3t8bmMdx54w3JgK+26IiCQ3ZcoUTJw4sd33vvnmGwiCgB9//LHLxz169CgWLFjQ0/L0rFixApGRkW3WFxcXY9KkSUY91/W2bNkCT09Pk57DnBhuTIV3TBERSW7u3LnYv38/fv/99zbvbd68GdHR0Rg+fHiXj+vr6wsXFxdjlHhDAQEB5vsBaBvBcGMqQSO0zyWn2FRMRCSRu+66C76+vtiyZYve+pqaGnz88ceYO3cu/vzzT8ycORO9e/eGi4sLhg0bhg8//LDT415/WerXX3/FrbfeCicnJwwePBj79+9vs89TTz2FAQMGwMXFBaGhoXj22WfR2Kj9ftiyZQuee+45nDhxAoIgQBAEXc3XX5Y6efIkbr/9djg7O6NXr15YsGABampqdO/Pnj0b06ZNw6pVqxAYGIhevXohJSVFd67uKCwsxNSpU+Hm5gYPDw/cd999KC0t1b1/4sQJ3HbbbXB3d4eHhweioqLwww8/AND+lMSUKVPg5eUFV1dXDBkyBF9++WW3azEEZyg2Fa+bAaUHUF8FXPwZCBgmdUVERMYlikDjFWnO7eii7W+8AQcHByQlJWHLli1YunQphOZ9Pv74Y6jVasycORM1NTWIiorCU089BQ8PD3zxxRd46KGH0K9fP8TExNzwHBqNBnfffTf8/f3x/fffo7KyUq8/p4W7uzu2bNmCoKAgnDx5EvPnz4e7uzv++c9/IjExEadOncKePXtw4MABAIBKpWpzjNraWiQkJCAuLg5Hjx5FWVkZ5s2bh4ULF+oFuIMHDyIwMBAHDx7EmTNnkJiYiMjISMyfP/+Gn6e9z9cSbA4dOoSmpiakpKQgMTERX331FQBg1qxZGDFiBNatWwe5XI7c3Fzdr42npKSgoaEBX3/9NVxdXXH69Gm4ubl1uY6uYLgxFZlMO1Px+W+0fTcMN0RkaxqvAC8FSXPu/74AKFwN2vThhx/Gq6++ikOHDmH8+PEAtJekZsyYAZVKBZVKhSVLlui2X7RoEfbu3YuPPvrIoHBz4MAB/Pzzz9i7dy+CgrR/j5deeqlNn8wzzzyjWw4JCcGSJUuQmZmJf/7zn3B2doabmxscHBwQEBDQ4bm2bduGuro6bN26Fa6u2s+/Zs0aTJkyBS+//LLuR6e9vLywZs0ayOVyDBo0CJMnT0ZWVla3wk1WVhZOnjyJc+fOITg4GACwdetWDBkyBEePHsWoUaNQWFiI//qv/8KgQYMAAGFhYbr9CwsLMWPGDAwbpv0eDA0N7XINXcXLUqbU8jMM7LshIpLMoEGDMGbMGGzatAkAcObMGXzzzTeYO3cuAECtVuOFF17AsGHD4O3tDTc3N+zduxeFhYUGHT8vLw/BwcG6YAMAcXFxbbbbvn07xo4di4CAALi5ueGZZ54x+BytzxUREaELNgAwduxYaDQa5Ofn69YNGTIEcrlc9zowMBBlZWVdOlfrcwYHB+uCDQAMHjwYnp6eyMvTTneSmpqKefPmIT4+HitXrsTZs2d12z7++OP417/+hbFjx2L58uXdauDuKo7cmFJL3w3vmCIiW+Tooh1BkercXTB37lwsWrQIa9euxebNm9GvXz+MGzcOAPDqq6/ijTfeQEZGBoYNGwZXV1csXrwYDQ0NRis3Ozsbs2bNwnPPPYeEhASoVCpkZmbitddeM9o5Wmu5JNRCEARoNBqTnAvQ3un1wAMP4IsvvsDu3buxfPlyZGZmYvr06Zg3bx4SEhLwxRdfYN++fUhPT8drr72GRYsWmawejtyYkm6m4lOAuknSUoiIjE4QtJeGpHgY0G/T2n333QeZTIZt27Zh69atePjhh3X9N4cPH8bUqVPx4IMPIiIiAqGhofjll18MPnZ4eDiKiopQXFysW3fkyBG9bb777jv07dsXS5cuRXR0NMLCwlBQUKC3jUKhgFqtvuG5Tpw4gdraWt26w4cPQyaTYeDAgQbX3BUtn6+oqEi37vTp06ioqMDgwYN16wYMGIAnn3wS+/btw913343Nmzfr3gsODsYjjzyCTz/9FP/4xz+wceNGk9TaguHGlLxDAYU70FSnbSomIiJJuLm5ITExEWlpaSguLsbs2bN174WFhWH//v347rvvkJeXh7///e96dwLdSHx8PAYMGIDk5GScOHEC33zzDZYuXaq3TVhYGAoLC5GZmYmzZ8/izTffxI4dO/S2CQkJwblz55Cbm4vy8nLU19e3OdesWbPg5OSE5ORknDp1CgcPHsSiRYvw0EMP6fptukutViM3N1fvkZeXh/j4eAwbNgyzZs3C8ePHkZOTg6SkJIwbNw7R0dG4evUqFi5ciK+++goFBQU4fPgwjh49ivDwcADA4sWLsXfvXpw7dw7Hjx/HwYMHde+ZCsONKbU0FQPsuyEiktjcuXNx+fJlJCQk6PXHPPPMMxg5ciQSEhIwfvx4BAQEYNq0aQYfVyaTYceOHbh69SpiYmIwb948vPjii3rb/O1vf8OTTz6JhQsXIjIyEt999x2effZZvW1mzJiBiRMn4rbbboOvr2+7t6O7uLhg7969uHTpEkaNGoV77rkHEyZMwJo1a7r2x2hHTU0NRowYofeYMmUKBEHAZ599Bi8vL9x6662Ij49HaGgotm/fDgCQy+X4888/kZSUhAEDBuC+++7DpEmT8NxzzwHQhqaUlBSEh4dj4sSJGDBgAP7973/3uN7OCKIoiiY9g4WpqqqCSqVCZWUlPDw8TH/CvUuB7DXAqPnA5FWmPx8RkYnU1dXh3LlzuPnmm+Hk5CR1OWSDOvs31pXvb47cmBpnKiYiIjIrhhtTa/mNqRI2FRMREZkDw42pefdrbiq+CpTn33h7IiIi6hGGG1Nr3VTM+W6IiIhMjuHGHFouTbHvhohsgJ3dh0JmZKx/Www35tDSVMyRGyKyYi2z3l65ItGPZZLNa5kVuvVPR3QHf37BHHRNxSe1TcVy/tmJyPrI5XJ4enrqfqPIxcVFN8svUU9pNBpcvHgRLi4ucHDo2fckv2XNoaWpuKEaKP8F8B98432IiCxQyy9Wd/dHGIk6I5PJ0KdPnx6HZoYbc5DJgMDhQMFhbd8Nww0RWSlBEBAYGAg/Pz80NjZKXQ7ZGIVCAZms5x0zDDfmEhipDTcXcoHIB6SuhoioR+RyeY/7IohMhQ3F5sI7poiIiMyC4cZcWu6YKjkJaDr/SXsiIiLqPoYbc+nVH1C4AY1XtE3FREREZBIMN+YikwEBw7XLF/4jbS1EREQ2jOHGnFr6bjiZHxERkckw3JhTS98Nm4qJiIhMhuHGnFrPVMymYiIiIpNguDGnXv0BR1c2FRMREZkQw405yeTamYoB9t0QERGZCMONubHvhoiIyKQYbsyNd0wRERGZFMONuelmKv6RTcVEREQmwHBjbj5hrZqKf5W6GiIiIpvDcGNuMjkQMEy7zL4bIiIio2O4kQL7boiIiEyG4UYKvGOKiIjIZBhupNAyclPMpmIiIiJjY7iRgs8AwNEFaKwF/jwjdTVEREQ2heFGCq2bitl3Q0REZFQMN1Jh3w0REZFJMNxIhXdMERERmQTDjVT0ZirWSFoKERGRLWG4kYrPAMDBGWioYVMxERGRETHcSEXuwJmKiYiITIDhRkrsuyEiIjI6hhsp8Y4pIiIio2O4kZLeTMVsKiYiIjIGhhsp+QxsbiquBi6dlboaIiIim2AR4Wbt2rUICQmBk5MTYmNjkZOTY9B+mZmZEAQB06ZNM22BpiJ3AAKGapfZd0NERGQUkoeb7du3IzU1FcuXL8fx48cRERGBhIQElJWVdbrf+fPnsWTJEtxyyy1mqtRE2HdDRERkVJKHm9dffx3z58/HnDlzMHjwYKxfvx4uLi7YtGlTh/uo1WrMmjULzz33HEJDQ81YrQnwjikiIiKjkjTcNDQ04NixY4iPj9etk8lkiI+PR3Z2dof7Pf/88/Dz88PcuXNveI76+npUVVXpPSyKbuTmBJuKiYiIjEDScFNeXg61Wg1/f3+99f7+/igpKWl3n2+//RbvvPMONm7caNA50tPToVKpdI/g4OAe121UvoMAB6fmpuLfpK6GiIjI6kl+Waorqqur8dBDD2Hjxo3w8fExaJ+0tDRUVlbqHkVFRSausovkDoB/c1Mx+26IiIh6zEHKk/v4+EAul6O0tFRvfWlpKQICAtpsf/bsWZw/fx5TpkzRrdM0X8pxcHBAfn4++vXrp7ePUqmEUqk0QfVGFBQJ/PEDcOE/wLB7pK6GiIjIqkk6cqNQKBAVFYWsrCzdOo1Gg6ysLMTFxbXZftCgQTh58iRyc3N1j7/97W+47bbbkJuba3mXnAzVuu+GiIiIekTSkRsASE1NRXJyMqKjoxETE4OMjAzU1tZizpw5AICkpCT07t0b6enpcHJywtChQ/X29/T0BIA2662Kbqbi5qZimVVdLSQiIrIokoebxMREXLx4EcuWLUNJSQkiIyOxZ88eXZNxYWEhZLb+Ze87CJArgfoq4PI5oFe/G+9DRERE7RJEURSlLsKcqqqqoFKpUFlZCQ8PD6nLuWbj7cAfx4AZ77DvhoiI6Dpd+f628SERK8KZiomIiIyC4cZScKZiIiIio2C4sRS6kZsfAfu6UkhERGRUDDeWwi+8uam4kjMVExER9QDDjaWQOwL+Q7TL7LshIiLqNoYbS8K+GyIioh5juLEkvGOKiIioxxhuLEnrmYrZVExERNQtDDeWxDcckCuAukrtTMVERETUZQw3lsRBca2pmH03RERE3cJwY2nYd0NERNQjDDeWhndMERER9QjDjaXRjdywqZiIiKg7GG4sjd/g5qbiCuDyeamrISIisjoMN5bGQaENOAD7boiIiLqB4cYSse+GiIio2xhuLBHvmCIiIuo2hhtL1Hrkhk3FREREXcJwY4n8BgMyR21TcUWB1NUQERFZFYYbS+SgBPybm4rZd0NERNQlDDeWin03RERE3cJwY6l4xxQREVG3MNxYqtYjN2wqJiIiMhjDjaXyH6JtKr56GagolLoaIiIiq8FwY6kclIBfuHaZfTdEREQGY7ixZOy7ISIi6jKGG0vGO6aIiIi6jOHGknGmYiIioi5juLFkfkMAmQNw9RJQWSR1NURERFaB4caSOTpdaypm3w0REZFBGG4sHftuiIiIuoThxtLxjikiIqIuYbixdIEjtM+cqZiIiMggDDeWzr+5qfjKn0Dl71JXQ0REZPEYbiydoxPgy5mKiYiIDMVwYw2CIrTP7LshIiK6IYYba8A7poiIiAzGcGMNgpqbijlTMRER0Q0x3FgD/yGAIAeulANVf0hdDRERkUVjuLEGjs6cqZiIiMhADDfWgn03REREBmG4sRacqZiIiMggDDfWovXIDZuKiYiIOsRwYy0ChmqbimsvAlUXpK6GiIjIYjHcWIvWTcXsuyEiIuoQw401abk0xb4bIiKiDjHcWJOWpmKO3BAREXWI4caatB65YVMxERFRuxhurImuqbgMqC6WuhoiIiKLxHBjTRydAd9B2mX23RAREbWL4cbasO+GiIioUww31oZ3TBEREXWK4cbacOSGiIioUww31sZ/KCDIgJpSoIpNxURERNdjuLE2CpdrTcUcvSEiImqD4cYase+GiIioQxYRbtauXYuQkBA4OTkhNjYWOTk5HW776aefIjo6Gp6ennB1dUVkZCTee+89M1ZrAVr6bi78R9IyiIiILJHk4Wb79u1ITU3F8uXLcfz4cURERCAhIQFlZWXtbu/t7Y2lS5ciOzsbP/74I+bMmYM5c+Zg7969Zq5cQi0jN7wsRURE1IYgitLO4x8bG4tRo0ZhzZo1AACNRoPg4GAsWrQITz/9tEHHGDlyJCZPnowXXnihzXv19fWor6/Xva6qqkJwcDAqKyvh4eFhnA9hbg1XgPTegKgBUn8GPAKlroiIiMikqqqqoFKpDPr+lnTkpqGhAceOHUN8fLxunUwmQ3x8PLKzs2+4vyiKyMrKQn5+Pm699dZ2t0lPT4dKpdI9goODjVa/ZBQugM9A7TJHb4iIiPRIGm7Ky8uhVqvh7++vt97f3x8lJSUd7ldZWQk3NzcoFApMnjwZq1evxl//+td2t01LS0NlZaXuUVRUZNTPIBld302ulFUQERFZHAepC+gOd3d35ObmoqamBllZWUhNTUVoaCjGjx/fZlulUgmlUmn+Ik0tMBI48SFHboiIiK4jabjx8fGBXC5HaWmp3vrS0lIEBAR0uJ9MJkP//v0BAJGRkcjLy0N6enq74cZmceSGiIioXZJellIoFIiKikJWVpZunUajQVZWFuLi4gw+jkaj0WsatgsBw5pnKi4Bqju+hEdERGRvJL8slZqaiuTkZERHRyMmJgYZGRmora3FnDlzAABJSUno3bs30tPTAWgbhKOjo9GvXz/U19fjyy+/xHvvvYd169ZJ+THMT+EK+AwALv6sHb0ZOFHqioiIiCyC5OEmMTERFy9exLJly1BSUoLIyEjs2bNH12RcWFgImezaAFNtbS0ee+wx/P7773B2dsagQYPw/vvvIzExUaqPIJ3ASG24Kc5luCEiImom+Tw35taV++Qt3pF1wJ6ngQGTgAcypa6GiIjIZKxmnhvqIc5UTERE1AbDjTULGAZAAKqLgerSG25ORERkDxhurJnSTdtUDHD0hoiIqBnDjbXjfDdERER6GG6sHftuiIiI9DDcWDuO3BAREelhuLF2AcOhbSq+ANSUSV0NERGR5BhurJ3SDfAJ0y5z9IaIiIjhxiaw74aIiEiH4cYWsO+GiIhIh+HGFnDkhoiISIfhxhYENjcVV/0B1FyUuhoiIiJJdSvcFBUV4ffff9e9zsnJweLFi7FhwwajFUZdoHQHevXXLnP0hoiI7Fy3ws0DDzyAgwcPAgBKSkrw17/+FTk5OVi6dCmef/55oxZIBmLfDREREYBuhptTp04hJiYGAPDRRx9h6NCh+O677/DBBx9gy5YtxqyPDMW+GyIiIgDdDDeNjY1QKpUAgAMHDuBvf/sbAGDQoEEoLi42XnVkOI7cEBERAehmuBkyZAjWr1+Pb775Bvv378fEiRMBABcuXECvXr2MWiAZKGC49rnqd6C2XNpaiIiIJNStcPPyyy/jrbfewvjx4zFz5kxEREQAAHbt2qW7XEVm5uRxramYozdERGTHHLqz0/jx41FeXo6qqip4eXnp1i9YsAAuLi5GK466KDAS+PMMUPwfICxe6mqIiIgk0a2Rm6tXr6K+vl4XbAoKCpCRkYH8/Hz4+fkZtUDqAvbdEBERdS/cTJ06FVu3bgUAVFRUIDY2Fq+99hqmTZuGdevWGbVA6gLdHVMnJC2DiIhISt0KN8ePH8ctt9wCAPjkk0/g7++PgoICbN26FW+++aZRC6QuCGxuKq4sAmr/lLYWIiIiiXQr3Fy5cgXu7u4AgH379uHuu++GTCbD6NGjUVBQYNQCqQucVIB3P+1y8X+krYWIiEgi3Qo3/fv3x86dO1FUVIS9e/fijjvuAACUlZXBw8PDqAVSF7HvhoiI7Fy3ws2yZcuwZMkShISEICYmBnFxcQC0ozgjRowwaoHURZypmIiI7Fy3bgW/55578Je//AXFxcW6OW4AYMKECZg+fbrRiqNu0I3csKmYiIjsU7fCDQAEBAQgICBA9+vgN910EyfwswSBzWGzshC4cglw8Za2HiIiIjPr1mUpjUaD559/HiqVCn379kXfvn3h6emJF154ARqNxtg1Ulc4qQDvUO3yBTYVExGR/enWyM3SpUvxzjvvYOXKlRg7diwA4Ntvv8WKFStQV1eHF1980ahFUhcFRgKXftP23fSfIHU1REREZtWtcPPuu+/i7bff1v0aOAAMHz4cvXv3xmOPPcZwI7WgSOCnT3nHFBER2aVuXZa6dOkSBg0a1Gb9oEGDcOnSpR4XRT3EO6aIiMiOdSvcREREYM2aNW3Wr1mzBsOHD+9xUdRDLU3FFc1NxURERHakW5elXnnlFUyePBkHDhzQzXGTnZ2NoqIifPnll0YtkLrB2RPwuhm4fE47etPvdqkrIiIiMptujdyMGzcOv/zyC6ZPn46KigpUVFTg7rvvxk8//YT33nvP2DVSd3CmYiIislOCKIqisQ524sQJjBw5Emq12liHNLqqqiqoVCpUVlba9k9FfJsBHFgODJ4K3LdV6mqIiIh6pCvf390auSErwJEbIiKyUww3tkrXVFzApmIiIrIrDDe2ytkL8ArRLhfzd6aIiMh+dOluqbvvvrvT9ysqKnpSCxlbYCRw+XzzHVO3SVwMERGReXQp3KhUqhu+n5SU1KOCyIiCIoHTO9l3Q0REdqVL4Wbz5s2mqoNMgTMVExGRHWLPjS1raSq+fB64elnSUoiIiMyF4caWuXgDnn21y2wqJiIiO8FwY+s43w0REdkZhhtbx74bIiKyMww3to4jN0REZGcYbmxdy8jN5XPA1QopKyEiIjILhhtb5+INePbRLrOpmIiI7ADDjT1g3w0REdkRhht7wL4bIiKyIww39oAjN0REZEcYbuxB0Ajt86XfgLpKaWshIiIyMYYbe+DiDajYVExERPaB4cZeBDX/zhT7boiIyMYx3NgL9t0QEZGdYLixF7xjioiI7IRFhJu1a9ciJCQETk5OiI2NRU5OTofbbty4Ebfccgu8vLzg5eWF+Pj4TrenZoEtTcVn2VRMREQ2TfJws337dqSmpmL58uU4fvw4IiIikJCQgLKysna3/+qrrzBz5kwcPHgQ2dnZCA4Oxh133IE//vjDzJVbGddegCpYu1z8o7S1EBERmZAgiqIoZQGxsbEYNWoU1qxZAwDQaDQIDg7GokWL8PTTT99wf7VaDS8vL6xZswZJSUlt3q+vr0d9fb3udVVVFYKDg1FZWQkPDw/jfRBrkDkL+Plz4I5/AWMWSV0NERGRwaqqqqBSqQz6/pZ05KahoQHHjh1DfHy8bp1MJkN8fDyys7MNOsaVK1fQ2NgIb2/vdt9PT0+HSqXSPYKDg41Su1Vi3w0REdkBScNNeXk51Go1/P399db7+/ujpKTEoGM89dRTCAoK0gtIraWlpaGyslL3KCoq6nHdVqul74Z3TBERkQ1zkLqAnli5ciUyMzPx1VdfwcnJqd1tlEollEqlmSuzUC0jN3+eAeqqACc7uyxHRER2QdKRGx8fH8jlcpSWluqtLy0tRUBAQKf7rlq1CitXrsS+ffswfPhwU5ZpO1x9AI+btMslbComIiLbJGm4USgUiIqKQlZWlm6dRqNBVlYW4uLiOtzvlVdewQsvvIA9e/YgOjraHKXaDvbdEBGRjZP8VvDU1FRs3LgR7777LvLy8vDoo4+itrYWc+bMAQAkJSUhLS1Nt/3LL7+MZ599Fps2bUJISAhKSkpQUlKCmpoaqT6CdWkJN+y7ISIiGyV5z01iYiIuXryIZcuWoaSkBJGRkdizZ4+uybiwsBAy2bUMtm7dOjQ0NOCee+7RO87y5cuxYsUKc5ZunVqaijlyQ0RENkryeW7MrSv3yduk2nLg1X4ABCCtCFC6S10RERHRDVnNPDckAV1TsciZiomIyCYx3Ngj9t0QEZENY7ixR4GR2mf23RARkQ1iuLFHHLkhIiIbxnBjj1pGbsp/BeqrJS2FiIjI2Bhu7JGbL+DRG4AIlJyUuhoiIiKjYrixV+y7ISIiG8VwY6/Yd0NERDaK4cZeceSGiIhsFMONvWoZuSn/Bajn73IREZHtYLixV25+gHsQ2FRMRES2huHGnrHvhoiIbBDDjT1j3w0REdkghht7xpEbIiKyQQw39qxl5OZiPpuKiYjIZjDc2DN3f8A9EGwqJiIiW8JwY+9aRm94aYqIiGwEw429a+m7YVMxERHZCIYbe8eRGyIisjEMN/au9UzFDbWSlkJERGQMDDf2zj0AcAsARA2biomIyCYw3BD7boiIyKYw3BD7boiIyKYw3BBHboiIyKYw3NC1kZvyfDYVExGR1WO4IcAjEHDzb24qPiV1NURERD3CcENa7LshIiIbwXBDWuy7ISIiG8FwQ1ocuSEiIhvBcENaLSM3F38GGq5IWgoREVFPMNyQlnsg4OqnbSouZVMxERFZL4Yb0hIE9t0QEZFNYLiha9h3Q0RENoDhhq7hyA0REdkAhhu6pmXk5uLPQONVSUshIiLqLoYbusYjCHD1BUQ1ZyomIiKrxXBD1wgC+26IiMjqMdyQPvbdEBGRlWO4IX0cuSEiIivHcEP6WkZuyvLYVExERFaJ4Yb0efQGXHy0TcWlP0ldDRERUZcx3JA+vZmK/yNpKURERN3BcENtse+GiIisGMMNtaUbuTkhaRlERETdwXBDbelmKs4DGuskLYWIiKirGG6oLdVNgEsvQNPEpmIiIrI6DDfUlt5MxWwqJiIi68JwQ+3jTMVERGSlGG6ofbxjioiIrBTDDbVPb6ZiNhUTEZH1YLih9qmCAWdvbVNxGZuKiYjIejDcUPv0ZirOlbISIiKiLmG4MaLvzpbjUm2D1GUYD/tuiIjICjHcGMm3v5Zj9uajeGDjEZTX1EtdjnFw5IaIiKwQw42RBKiUUDk74ueSaszccARl1TbQhNsyclOWBzTZSGAjIiKbJ3m4Wbt2LUJCQuDk5ITY2Fjk5OR0uO1PP/2EGTNmICQkBIIgICMjw3yF3kB/P3dsXzAaAR5O+LWsBvdvOILSKisPOJ59AGcvQNPImYqJiMhqSBputm/fjtTUVCxfvhzHjx9HREQEEhISUFZW1u72V65cQWhoKFauXImAgAAzV3tjob5u2P730ejt6YzfLtYi8a1sXKi4KnVZ3ac3U3GulJUQEREZTNJw8/rrr2P+/PmYM2cOBg8ejPXr18PFxQWbNm1qd/tRo0bh1Vdfxf333w+lUmnmag3Tt5crMheMxk1ezjj/5xUkbshG0aUrUpfVfey7ISIiKyNZuGloaMCxY8cQHx9/rRiZDPHx8cjOzjbaeerr61FVVaX3MLVgbxd89Pc49O3lgqJLV3H/hiMo/NNKAw5HboiIyMpIFm7Ky8uhVqvh7++vt97f3x8lJSVGO096ejpUKpXuERwcbLRjdybI0xnbF8Qh1McVf1RcReKGbJwrrzXLuY2qZeSm9DSbiomIyCpI3lBsamlpaaisrNQ9ioqKzHbuAJUTMv8+Gv393FBcWYfEt7JxpqzGbOc3Cs++gJOntqm47LTU1RAREd2QZOHGx8cHcrkcpaWleutLS0uN2iysVCrh4eGh9zAnP3cnZC4YjUEB7iirrsf9G47gl9Jqs9bQI5ypmIiIrIxk4UahUCAqKgpZWVm6dRqNBllZWYiLi5OqLJPwcVNi2/zRGBzogfIabcDJKzZ974/RsO+GiIisiKSXpVJTU7Fx40a8++67yMvLw6OPPora2lrMmTMHAJCUlIS0tDTd9g0NDcjNzUVubi4aGhrwxx9/IDc3F2fOnJHqIxjM21WBbfNjMay3CpdqGzBz4xGc+qNS6rIMw5EbIiKyIpKGm8TERKxatQrLli1DZGQkcnNzsWfPHl2TcWFhIYqLi3XbX7hwASNGjMCIESNQXFyMVatWYcSIEZg3b55UH6FLPF0UeH9eLCKDPVFxpREPbDyCE0UVUpd1Y7qZik8DTTb021lERGSTBFEURamLMKeqqiqoVCpUVlaavf+mRXVdI2ZvPopjBZfhrnTAlodjENXXS5JaDCKKwMshQF0FsODQtZEcIiIiM+nK97fN3y1lidydHLH14RjE3OyN6vomJL3zPY6evyR1WR0TBCAwQrvMvhsiIrJwDDcScVU6YMucURjTrxdqG9RI3pSD7LN/Sl1Wx9h3Q0REVoLhRkIuCgdsmj0Kt4T54EqDGnO25ODbX8ulLqt9vGOKiIisBMONxJwc5diYFI3bBvqirlGDue8exVf57f9wqKR0MxX/xKZiIiKyaAw3FsDJUY71D0UhPtwf9U0aLNh6DFl5pTfe0Zy8bgacVIC6AbiYJ3U1REREHWK4sRBKBzn+PWskJg4JQINag0feP4a9PxnvN7Z6rHVTMftuiIjIgjHcWBCFgwyrHxiBu4YHolEtIuWD4/jyZPGNdzQX9t0QEZEVYLixMI5yGTISIzEtMghNGhGLPvwPPsv9Q+qytHjHFBERWQGGGwvkIJfhtfsicU/UTVBrRDy5PRefHv9d6rKujdyU/gSoGyUthYiIqCMMNxZKLhPwyozhmBkTDI0I/OPjE/joaJG0RXmHAkoVoK4HythUTERElonhxoLJZAJenDYMD43uC1EE/vn/fsQH3xdIV5AgAIHDtcvsuyEiIgvFcGPhZDIBz08dgjljQwAAS3ecwrvfnZeuIPbdEBGRhWO4sQKCIGDZXYOx4NZQAMDyXT/h7W9+k6YY3jFFREQWjuHGSgiCgLRJg5ByWz8AwL++yMP6Q2fNX0jQCO1zySk2FRMRkUViuLEigiBgyR0D8cSEMADAyt0/Y3XWr+YtwutmQOmhbSq++LN5z01ERGQAhhsrIwgCnvzrACy5YwAA4LX9v+B/9v8CURTNU4BMxpmKiYjIojHcWKmFt4fh6UmDAABvZP2KVfvyzRdwWsIN+26IiMgCMdxYsUfG9cMzk8MBAGsPnkX67p/NE3Ba+m44ckNERBaI4cbKzbslFM9PHQIA2PD1b3j+89OmDzi6mYpPAeom056LiIioixhubEBSXAhemj4MALD58Hks++wnaDQmDDjeoYDCHWiqY1MxERFZHIYbG/FAbB+8MmM4BAF470gBlu48abqAI5Ndm8yPfTdERGRhGG5syH2jgvHavRGQCcCHOUX45//7EWpTBRzeMUVERBaK4cbG3D3yJvxPYiTkMgGfHPsd//goF01qjfFP1NJUzJEbIiKyMAw3NmhqZG+8ef8IOMgE7My9gMXbc9Fo7IDT0lRcwqZiIiKyLAw3Nmry8ECsnTUSjnIBn/9YjMc//A8amowYcHRNxVeB8nzjHZeIiKiHGG5sWMKQAKx/MAoKuQy7T5XgsQ+Oo75JbZyDc6ZiIiKyUAw3Nm5CuD82JEVB4SDDgbxSPPLeMdQ1Ging8I4pIiKyQAw3dmD8QD9sSh4FJ0cZDuZfxPytPxgn4LT03XDkhoiILAjDjZ34S5gPNs+OgYtCjm9+LcfDW47iSkMPG4FbRm5KTrKpmIiILAbDjR2J69cL7z4cA1eFHN+d/ROzNx9FTX0PQol3v1ZNxb8Yr1AiIqIeYLixM6NCvPHevFi4Kx2Qc+4SkjfloLqusXsHk8mAwOHaZfbdEBGRhWC4sUMj+3jh/Xmx8HBywLGCy3jwnRxUXu1mwGHfDRERWRiGGzsVEeyJbfNHw9PFESeKKvDg29+j4kpD1w/EO6aIiMjCMNzYsaG9Vfhw/mh4uypw8o9KzNz4PS7VdjHg6GYqPglojHSLORERUQ8w3Ni58EAPZC4YDR83JfKKqzBzwxGU19QbfoBe/QGFG9B4hU3FRERkERhuCAP83ZG5YDT83JXIL63G/RuOoKyqzrCdZTIgoLmpmH03RERkARhuCADQ388N2/8eh0CVE86U1eD+DUdQUmlgwGHfDRERWRCGG9K52ccV2xfEobenM34rr0Xihmz8UXH1xjvyjikiIrIgDDekp08vF2QuGI1gb2cU/HkFiW9lo+jSlc530s1U/CObiomISHIMN9RGsLcLti+IQ0gvF/x++Sru33AEBX/WdrxDr/6Ao2tzU/Gv5iuUiIioHQw31K4gT2dkLohDqK8r/qi4isS3juC3izXtbyyTc6ZiIiKyGAw31KEAlRMyF4xGmJ8bSqrqcP+GIzhTVt3+xrq+m/+YrT4iIqL2MNxQp/zcnfDhgtEYFOCOsup63L/hCPJL2gk4LX03bComIiKJMdzQDfm4KbFt/mgMDvRAeU0DZm48gtMXqvQ30s1UzKZiIiKSliCKoih1EeZUVVUFlUqFyspKeHh4SF2OVam40oCkTTn48fdKeLo44v25sRjaW6V9U6MG0oOBxlqgdzTg6gM4qQx4eAJKD0DuIOlnIyIiy9aV72+GG+qSyquNSN6Ug9yiCng4OWDr3FhEBntq39yWCPyyp3sHVrgZHoauX8dwRERk8xhuOsFw03PVdY2Ys/kofii4DHelA7Y8HIOovl5AwxXg9xzgagVQV2nYo7GTW8y7QuFuYDjq4CGTG6cOIiIyCYabTjDcGEdtfRMe3nIU35+7BFeFHJvnxCDmZu+uH0jdCNRVAXUVhgciSwhHzp76I0cMR0REJsVw0wmGG+O50tCE+Vt/wOEzf8LZUY53ZkdjTD8f8xbR1ADUVzWHnYpuhKMbzL5sKKXHtbDj6gu4BwBu/oB7IODe/Ozmr13v6GyccxIR2RGGm04w3BhXXaMaC947hq9/uQgnRxnWPxiFW8N8IZMJUpdmGCnCkZMKcAvQBp2WR+vXLaFI4WL0j0tEZK0YbjrBcGN8dY1qPPbBcfzfz2UAAEEA3JUO8HB2hIeTIzycHZqf9V+rnFvWNW/bvOymdIAgWGk4unoZqLkIVBcDNaXa5+rSa6+bDPyldUA7GnT9CJBeKGoeDVK6mezjERFZCoabTjDcmEZDkwb//OQEduZe6PGxZALg3hyCVC2BqE1Iah2eWoKS9n0Xhdwyw5EoakNQdQlQU6J9ri5pPwR1ZURI4d5O8GkZDWp1SUzprk2eRERWiOGmEww3plXXqEZ1XROq6hpRdbURVXVNqLraiMqrjc3r2r6nW3+1EQ1qTY9rkMuE68JPByNF7YwqqZwd4eQokzYciaJ2NKi6VD8EXR+Kqku61lDt6Nq2/+f6S2LuAdoRI4YgIrIwDDedYLixXKIoor5Jows8lR0GofZDUuXVRjRpev7P2VEudDBC1CoIXfeeqvk9dydHKB1k5us5qq/WH/GpLrluufnR0MFvgrXHwfm6ENROU7R7gHbOIYYgIjIThptOMNzYLlEUUdeoaQ5GjfqjQu2EpMp2QpLaCOEIABxkAhzkAhzlMijksnaXtY+Oljt+z0EuQNG8zqH5mI4OAhxk2vcV1y07ymVwVNfBpaEMyqsXobhaBserpXC8chHy2hLIa8sg1JRCqC4B6isN/5ByZdsmaHd/beiRyQFBBgjNz7rXsutet7wv62D7G73f0fEMeZ/BjMiadOX7m9O6ks0QBAHOCjmcFXL4ezh1eX9RFHGlQa0XiCqvNHZ8Ga1OPyRV1zWiJRs1aUQ0abRhy/J4ND/C9NbKZQLc5Q0IlFciUFYJf1kFAoTL8EUFfHEZPuJl9BIvwVtzCW5iDaCuByoKtA8rJXYShoSehDNB1hyehOuWhQ7WG2t7GSCgi9s3h7wubX+D2iRnKf+bXdD/e7V5Rsd/W4Of0c39Wj8bUmtH/707OIbCDfDpb8o/bqcsItysXbsWr776KkpKShAREYHVq1cjJiamw+0//vhjPPvsszh//jzCwsLw8ssv48477zRjxWSLBEGAq9IBrkoHBKq6vr9GI6K2oQkNTRo0aUQ0NGnQqG673NikQYNag0a1iCa1/nKjWoOGDpYb1WLzc+fLTc37taxrUovN59Bfvn7MVq0RUaFxREWjD/LQ+XxFSjTAV6iAHyrgL1yGn1ABP+Ey/IUKuOEqZNA0P0TIoYHQ/CyDCJmgfU/3ut3tWu0vXLc/ru3fdn3za8GwLzdBVAOiGtA0dv0/OBF17KYYYN5+yU4vebjZvn07UlNTsX79esTGxiIjIwMJCQnIz8+Hn59fm+2/++47zJw5E+np6bjrrruwbds2TJs2DcePH8fQoUMl+AREWjKZAHcnR6nLMJhaIzYHKG3oaVRrdMHs+uXGJg0am4NZo7rj5ZLmAKfRiFCL4rVlDaARRaib12vXtVoW0c665tfNj2v7X9u2zTFFERoNoFZrIIpqQKN9FkUNRI0aokYDQVRD1KghiCKE1iFLaBWOWgWm9kOUBvJWQe36cCZAhAwiABECABk00P7v2dbrtctCyzaCdpSv9fqWZ7RzDKH5OHrH6Gi90LJv22N0vL71Q//8wvX1C9eOIYOmeQ/jM8VxTTXGM6qvF/p4OWvPIIoGPAMQNQZu296+3dmv9TO6uL2m831dfU30lzWM5D03sbGxGDVqFNasWQMA0Gg0CA4OxqJFi/D000+32T4xMRG1tbX4/PPPdetGjx6NyMhIrF+//obnY88NEQHay5AaEe2HpNZBqdW6ptbbtlrWPl87lihqw5z+MppfX1vWiNo6DNlGo2le1tXeenvo3m+9v4gbb9NyHED7efXOef059D5T+9u09K2J0L7Wfe+1et3ytaNdbtlGbPVa1PvO1tvnumO1/Le8dp5rr3XHaNle1N++5Vj652ne47pj6bYx8Fyv3huBv0UEGeOfKjWzmp6bhoYGHDt2DGlpabp1MpkM8fHxyM7Obnef7OxspKam6q1LSEjAzp07292+vr4e9fX1utdVVVU9L5yIrJ4gCJAL2l4jIrItknZ+lZeXQ61Ww9/fX2+9v78/SkpK2t2npKSkS9unp6dDpVLpHsHBwcYpnoiIiCySJbS1m1RaWhoqKyt1j6KiIqlLIiIiIhOS9LKUj48P5HI5SktL9daXlpYiICCg3X0CAgK6tL1SqYRSqTROwURERGTxJB25USgUiIqKQlZWlm6dRqNBVlYW4uLi2t0nLi5Ob3sA2L9/f4fbExERkX2R/Fbw1NRUJCcnIzo6GjExMcjIyEBtbS3mzJkDAEhKSkLv3r2Rnp4OAHjiiScwbtw4vPbaa5g8eTIyMzPxww8/YMOGDVJ+DCIiIrIQkoebxMREXLx4EcuWLUNJSQkiIyOxZ88eXdNwYWEhZLJrA0xjxozBtm3b8Mwzz+C///u/ERYWhp07d3KOGyIiIgJgAfPcmBvnuSEiIrI+Xfn+tvm7pYiIiMi+MNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKZJP4mduLdP6VFVVSVwJERERGarle9uQ6fnsLtxUV1cDAIKDgyWuhIiIiLqquroaKpWq023sboZijUaDCxcuwN3dHYIgGPXYVVVVCA4ORlFRkV3Ofmzvnx/g34Cf374/P8C/gb1/fsB0fwNRFFFdXY2goCC9n2Vqj92N3MhkMtx0000mPYeHh4fd/qMG+PkB/g34+e378wP8G9j75wdM8ze40YhNCzYUExERkU1huCEiIiKbwnBjREqlEsuXL4dSqZS6FEnY++cH+Dfg57fvzw/wb2Dvnx+wjL+B3TUUExERkW3jyA0RERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcGMnatWsREhICJycnxMbGIicnR+qSzObrr7/GlClTEBQUBEEQsHPnTqlLMqv09HSMGjUK7u7u8PPzw7Rp05Cfny91WWa1bt06DB8+XDdpV1xcHHbv3i11WZJZuXIlBEHA4sWLpS7FbFasWAFBEPQegwYNkross/rjjz/w4IMPolevXnB2dsawYcPwww8/SF2WWYSEhLT57y8IAlJSUiSph+HGCLZv347U1FQsX74cx48fR0REBBISElBWViZ1aWZRW1uLiIgIrF27VupSJHHo0CGkpKTgyJEj2L9/PxobG3HHHXegtrZW6tLM5qabbsLKlStx7Ngx/PDDD7j99tsxdepU/PTTT1KXZnZHjx7FW2+9heHDh0tditkNGTIExcXFuse3334rdUlmc/nyZYwdOxaOjo7YvXs3Tp8+jddeew1eXl5Sl2YWR48e1ftvv3//fgDAvffeK01BIvVYTEyMmJKSonutVqvFoKAgMT09XcKqpAFA3LFjh9RlSKqsrEwEIB46dEjqUiTl5eUlvv3221KXYVbV1dViWFiYuH//fnHcuHHiE088IXVJZrN8+XIxIiJC6jIk89RTT4l/+ctfpC7DYjzxxBNiv379RI1GI8n5OXLTQw0NDTh27Bji4+N162QyGeLj45GdnS1hZSSVyspKAIC3t7fElUhDrVYjMzMTtbW1iIuLk7ocs0pJScHkyZP1/v+BPfn1118RFBSE0NBQzJo1C4WFhVKXZDa7du1CdHQ07r33Xvj5+WHEiBHYuHGj1GVJoqGhAe+//z4efvhho/9AtaEYbnqovLwcarUa/v7+euv9/f1RUlIiUVUkFY1Gg8WLF2Ps2LEYOnSo1OWY1cmTJ+Hm5galUolHHnkEO3bswODBg6Uuy2wyMzNx/PhxpKenS12KJGJjY7Flyxbs2bMH69atw7lz53DLLbegurpa6tLM4rfffsO6desQFhaGvXv34tFHH8Xjjz+Od999V+rSzG7nzp2oqKjA7NmzJavB7n4VnMiUUlJScOrUKbvqNWgxcOBA5ObmorKyEp988gmSk5Nx6NAhuwg4RUVFeOKJJ7B//344OTlJXY4kJk2apFsePnw4YmNj0bdvX3z00UeYO3euhJWZh0ajQXR0NF566SUAwIgRI3Dq1CmsX78eycnJEldnXu+88w4mTZqEoKAgyWrgyE0P+fj4QC6Xo7S0VG99aWkpAgICJKqKpLBw4UJ8/vnnOHjwIG666SapyzE7hUKB/v37IyoqCunp6YiIiMAbb7whdVlmcezYMZSVlWHkyJFwcHCAg4MDDh06hDfffBMODg5Qq9VSl2h2np6eGDBgAM6cOSN1KWYRGBjYJsiHh4fb1aU5ACgoKMCBAwcwb948SetguOkhhUKBqKgoZGVl6dZpNBpkZWXZXb+BvRJFEQsXLsSOHTvwf//3f7j55pulLskiaDQa1NfXS12GWUyYMAEnT55Ebm6u7hEdHY1Zs2YhNzcXcrlc6hLNrqamBmfPnkVgYKDUpZjF2LFj20wB8csvv6Bv374SVSSNzZs3w8/PD5MnT5a0Dl6WMoLU1FQkJycjOjoaMTExyMjIQG1tLebMmSN1aWZRU1Oj97/Ozp07h9zcXHh7e6NPnz4SVmYeKSkp2LZtGz777DO4u7vreq1UKhWcnZ0lrs480tLSMGnSJPTp0wfV1dXYtm0bvvrqK+zdu1fq0szC3d29TY+Vq6srevXqZTe9V0uWLMGUKVPQt29fXLhwAcuXL4dcLsfMmTOlLs0snnzySYwZMwYvvfQS7rvvPuTk5GDDhg3YsGGD1KWZjUajwebNm5GcnAwHB4njhST3aNmg1atXi3369BEVCoUYExMjHjlyROqSzObgwYMigDaP5ORkqUszi/Y+OwBx8+bNUpdmNg8//LDYt29fUaFQiL6+vuKECRPEffv2SV2WpOztVvDExEQxMDBQVCgUYu/evcXExETxzJkzUpdlVv/7v/8rDh06VFQqleKgQYPEDRs2SF2SWe3du1cEIObn50tdiiiIoihKE6uIiIiIjI89N0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENoXhhoiIiGwKww0RERHZFIYbIiIisikMN0RklwRBwM6dO6Uug4hMgOGGiMxu9uzZEAShzWPixIlSl0ZENoA/nElEkpg4cSI2b96st06pVEpUDRHZEo7cEJEklEolAgIC9B5eXl4AtJeM1q1bh0mTJsHZ2RmhoaH45JNP9PY/efIkbr/9djg7O6NXr15YsGABampq9LbZtGkThgwZAqVSicDAQCxcuFDv/fLyckyfPh0uLi4ICwvDrl27dO9dvnwZs2bNgq+vL5ydnREWFtYmjBGRZWK4ISKL9Oyzz2LGjBk4ceIEZs2ahfvvvx95eXkAgNraWiQkJMDLywtHjx7Fxx9/jAMHDuiFl3Xr1iElJQULFizAyZMnsWvXLvTv31/vHM899xzuu+8+/Pjjj7jzzjsxa9YsXLp0SXf+06dPY/fu3cjLy8O6devg4+Njvj8AEXWf1D9LTkT2Jzk5WZTL5aKrq6ve48UXXxRFURQBiI888ojePrGxseKjjz4qiqIobtiwQfTy8hJramp073/xxReiTCYTS0pKRFEUxaCgIHHp0qUd1gBAfOaZZ3Sva2pqRADi7t27RVEUxSlTpohz5swxzgcmIrNizw0RSeK2227DunXr9NZ5e3vrluPi4vTei4uLQ25uLgAgLy8PERERcHV11b0/duxYaDQa5OfnQxAEXLhwARMmTOi0huHDh+uWXV1d4eHhgbKyMgDAo48+ihkzZuD48eO44447MG3aNIwZM6Zbn5WIzIvhhogk4erq2uYykbE4OzsbtJ2jo6Pea0EQoNFoAACTJk1CQUEBvvzyS+zfvx8TJkxASkoKVq1aZfR6ici42HNDRBbpyJEjbV6Hh4cDAMLDw3HixAnU1tbq3j98+DBkMhkGDhwId3d3hISEICsrq0c1+Pr6Ijk5Ge+//z4yMjKwYcOGHh2PiMyDIzdEJIn6+nqUlJTorXNwcNA17X788ceIjo7GX/7yF3zwwQfIycnBO++8AwCYNWsWli9fjuTkZKxYsQIXL17EokWL8NBDD8Hf3x8AsGLFCjzyyCPw8/PDpEmTUF1djcOHD2PRokUG1bds2TJERUVhyJAhqK+vx+eff64LV0Rk2RhuiEgSe/bsQWBgoN66gQMH4ueffwagvZMpMzMTjz32GAIDA/Hhhx9i8ODBAAAXFxfs3bsXTzzxBEaNGgUXFxfMmDEDr7/+uu5YycnJqKurw//8z/9gyZIl8PHxwT333GNwfQqFAmlpaTh//jycnZ1xyy23IDMz0wifnIhMTRBFUZS6CCKi1gRBwI4dOzBt2jSpSyEiK8SeGyIiIrIpDDdERERkU9hzQ0QWh1fLiagnOHJDRERENoXhhoiIiGwKww0RERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKb8v8BaPfzfwQ1LkQAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.861454Z","iopub.execute_input":"2024-03-15T19:44:44.861750Z","iopub.status.idle":"2024-03-15T19:44:44.865723Z","shell.execute_reply.started":"2024-03-15T19:44:44.861725Z","shell.execute_reply":"2024-03-15T19:44:44.864870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"# #the following code is not correct for dataloading, deleted\n# def visualize(**images):\n#     \"\"\"Plot images in one row.\"\"\"\n#     n = len(images)\n#     plt.figure(figsize=(16, 5))\n#     for i, (name, image) in enumerate(images.items()):\n#         plt.subplot(1, n, i + 1)\n#         plt.xticks([])\n#         plt.yticks([])\n#         plt.title(' '.join(name.split('_')).title())\n#         plt.imshow(image)\n#     plt.show()\n\n# for idx_ in range(4):\n#     current_timestep = 20*idx_\n#     print(f'plotting for time step: {current_timestep}')\n#     image, mask = data_module.val_dataset[current_timestep] # get some sample\n#     visualize(\n#         concentration=image[0,:, :].squeeze(),\n#         eps=image[1,:, :].squeeze(),\n#         Ux=image[2,:, :].squeeze(),\n#         Uy=image[3,:, :].squeeze(),\n#         dissolution=mask[1,:, :].squeeze(),\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.867052Z","iopub.execute_input":"2024-03-15T19:44:44.867410Z","iopub.status.idle":"2024-03-15T19:44:44.877681Z","shell.execute_reply.started":"2024-03-15T19:44:44.867383Z","shell.execute_reply":"2024-03-15T19:44:44.876722Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"use class dataloader to plot","metadata":{}},{"cell_type":"code","source":"#concatenate data\n\n# input_list_val = torch.cat(model.all_test_input, dim=0)\n# masks_list_val= torch.cat(model.all_test_masks, dim=0)\n# preds_list_val  = torch.cat(model.all_test_preds, dim=0)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.878964Z","iopub.execute_input":"2024-03-15T19:44:44.879428Z","iopub.status.idle":"2024-03-15T19:44:44.889023Z","shell.execute_reply.started":"2024-03-15T19:44:44.879394Z","shell.execute_reply":"2024-03-15T19:44:44.888080Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def matshow_error(input1, pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 4, figsize=figsize)\n    \n    v_max = max(input1.max(), pred.max())\n    v_min = max(input1.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(input1, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(input1, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    \n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} input')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))   \n    \n    \n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.2)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[2].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[2].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    \n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.3)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[3].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[3].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[3])\n    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.4)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\n    \n    \n    \n# for sample_idx in range(1): #12):\n#     for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n#         preds = preds_list_train[sample_idx*100+time_step, :, :]\n#         masks = masks_list_train[sample_idx*100+time_step, :, :]\n#         # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n#         matshow_error(\n#             preds,\n#             masks, \n#             title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n#             filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n#             figsize=(15, 7))\n        \nfor sample_idx in range(0): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        inputs= input_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        preds = preds_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        masks = masks_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(inputs,\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.890384Z","iopub.execute_input":"2024-03-15T19:44:44.890652Z","iopub.status.idle":"2024-03-15T19:44:44.910136Z","shell.execute_reply.started":"2024-03-15T19:44:44.890628Z","shell.execute_reply":"2024-03-15T19:44:44.909267Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = MyLightningModel.load_from_checkpoint('/kaggle/working/example.ckpt')\nmodel.eval()\n\ntrain_dataloader = data_module.get_train_data()\n\ntrain_inputlist=[]\n# train_outputlist=[]\ntrain_truelist=[]\n\nfor batch in train_dataloader:\n    img,mask=batch\n    #pred=model(img)\n    train_inputlist.append(img)\n    train_truelist.append(mask)\n    \ntrain_inputlists = torch.cat(train_inputlist, dim=0)\ntrain_inputlists = torch.cat(train_truelist, dim=0)\n\ntrain_outputlist=model(train_inputlists).squeeze()\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T19:44:44.911463Z","iopub.execute_input":"2024-03-15T19:44:44.911831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#auto-regression, see here:https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/10-autoregressive-image-modeling.html","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}