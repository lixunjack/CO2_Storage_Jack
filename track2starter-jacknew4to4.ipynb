{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7847033,"sourceType":"datasetVersion","datasetId":4601199}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T11:56:50.897341Z","iopub.execute_input":"2024-03-15T11:56:50.898507Z","iopub.status.idle":"2024-03-15T11:56:50.905693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction9/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T11:56:50.907565Z","iopub.execute_input":"2024-03-15T11:56:50.907871Z","iopub.status.idle":"2024-03-15T11:56:50.931186Z","shell.execute_reply.started":"2024-03-15T11:56:50.907841Z","shell.execute_reply":"2024-03-15T11:56:50.930449Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n/kaggle/input/helpfunction9/dataset.py\n/kaggle/input/helpfunction9/pre_processing.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n!pip install torchviz","metadata":{"execution":{"iopub.status.busy":"2024-03-15T11:56:50.932110Z","iopub.execute_input":"2024-03-15T11:56:50.932350Z","iopub.status.idle":"2024-03-15T11:57:59.108793Z","shell.execute_reply.started":"2024-03-15T11:56:50.932329Z","shell.execute_reply":"2024-03-15T11:57:59.107520Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nRequirement already satisfied: torchviz in /opt/conda/lib/python3.10/site-packages (0.0.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-15T11:57:59.111278Z","iopub.execute_input":"2024-03-15T11:57:59.111594Z","iopub.status.idle":"2024-03-15T11:57:59.119943Z","shell.execute_reply.started":"2024-03-15T11:57:59.111566Z","shell.execute_reply":"2024-03-15T11:57:59.119085Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=4,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n        self.record_trainloss=[]\n        self.record_valoss=[]\n        self.record_testloss=[]\n        \n        self.validation_step_outputs = []\n        self.training_step_outputs = []\n    \n    \n        #record test\n        self.all_test_input=[]\n        self.all_test_preds = []\n        self.all_test_masks = []\n            \n        \n    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        loss = self.l1_loss(preds, masks)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        self.training_step_outputs.append(loss)\n\n\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        #print(f\"imgs {imgs.shape},masks{masks.shape},preds{preds.shape}\")\n        \n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        self.validation_step_outputs.append(val_loss)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        self.all_test_input.append( imgs[:,1,:,:].squeeze() )\n        self.all_test_masks.append( masks[:,1,:,:].squeeze() )\n        self.all_test_preds.append( preds[:,1,:,:].squeeze() )\n        \n        \n        \n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        \n#         metrics = {\"test_loss\": test_loss}\n#         self.log_dict(metrics)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n    def on_train_epoch_end(self):\n        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n        \n        self.record_trainloss.append(avg_train_loss)\n        \n        self.training_step_outputs=[]\n        #self.log('all_train_losses', all_train_loss)\n        \n        \n    \n    def on_validation_epoch_end(self):\n        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n        \n        \n        self.record_valoss.append(avg_val_loss)\n        \n        self.validation_step_outputs=[]\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        #self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n    def train_dataloader(self):\n        #train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=1, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=1, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=1, shuffle=False, persistent_workers=True)\n    \n    \n    \n    def get_train_data(self):\n        return DataLoader(self.train_dataset, batch_size=1, shuffle=False, num_workers=1)\n    \n    def get_test_data(self):\n        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, num_workers=1)\n    \n\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=1,  # Save all checkpoints\n    monitor=None,  # Disable monitoring\n    verbose=True\n)\n\ntrainer = L.Trainer(max_epochs=8,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=32),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=3, \\\n                                              verbose=False)])\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\n\ntrainer.fit(model, data_module)\n\ntrain_loss=model.record_trainloss\nval_loss=model.record_valoss\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n\n# test the model  [for data to plot!]\ntrainer.test(model, data_module) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchviz import make_dot\n\nx = torch.zeros(64, 4, 256, 256, dtype=torch.float, requires_grad=False)\nout = model(x)\n\nmake_dot(out) \n\nmake_dot(out, params=dict(list(model.named_parameters()))).render(\"plotmodel\", format=\"png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss=model.record_trainloss\nval_loss=model.record_valoss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\nval_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n\nplt.figure()\nplt.plot(train_losscpu, label=\"training Loss\")\nplt.plot(val_losscpu, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\nfor idx_ in range(4):\n    current_timestep = 10*idx_\n    print(f'plotting for time step: {current_timestep}')\n    image, mask = data_module.train_dataset[current_timestep] # get some sample\n    visualize(\n        concentration=image[0,:, :].squeeze(),\n        eps=image[1,:, :].squeeze(),\n        Ux=image[2,:, :].squeeze(),\n        Uy=image[3,:, :].squeeze(),\n        dissolution=mask[1,:, :].squeeze(),\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"use class dataloader to plot","metadata":{}},{"cell_type":"code","source":"#concatenate data\n\ninput_list_val = torch.cat(model.all_test_input, dim=0)\nmasks_list_val= torch.cat(model.all_test_masks, dim=0)\npreds_list_val  = torch.cat(model.all_test_preds, dim=0)\n\n\n#training has to be data extra\n#model(preds_list_val)#train_dataset\n\n\ntrain_dataloader = data_module.get_train_data()\ntrain_inputlist=[]\ntrain_outputlist=[]\ntrain_truelist=[]\n\nfor batch in train_dataloader:\n    img,mask=batch\n    pred=model(batch)\n    \n    train_inputlist.append(img)\n    train_outputlist.append(pred)\n    train_truelist.append(mask)\n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matshow_error(input1, pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 4, figsize=figsize)\n    \n    v_max = max(truth.max(), pred.max())\n    v_min = max(truth.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(input1, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(input1, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    \n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} input')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))   \n    \n    \n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[2].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[2].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    \n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[3].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[3].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[3])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\n# for sample_idx in range(1): #12):\n#     for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n#         preds = preds_list_train[sample_idx*100+time_step, :, :]\n#         masks = masks_list_train[sample_idx*100+time_step, :, :]\n#         # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n#         matshow_error(\n#             preds,\n#             masks, \n#             title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n#             filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n#             figsize=(15, 7))\n        \nfor sample_idx in range(1): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        inputs= input_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        preds = preds_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        masks = masks_list_val[sample_idx*100+time_step, :, :].cpu().numpy()\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(inputs,\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.all_test_input[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}