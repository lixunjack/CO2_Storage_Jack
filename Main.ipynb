{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b80d217",
   "metadata": {},
   "source": [
    "#### TorchLightening Script for project 2\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
    "\n",
    "- 13, March, 2024\n",
    "- By Jack Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bdd5d",
   "metadata": {},
   "source": [
    "    IMPORT BASIC PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13bb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "h5py._errors.unsilence_errors()\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "# SUGGESTION: create all folders for storing results\n",
    "if not os.path.exists('./vis'):\n",
    "    os.mkdir('./vis')\n",
    "\n",
    "if not os.path.exists('./vis_results'):\n",
    "    os.mkdir('./vis_results')\n",
    "\n",
    "if not os.path.exists('./model256_weights'):\n",
    "    os.mkdir('./model256_weights')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d608f",
   "metadata": {},
   "source": [
    "    Import Lightning: Import the necessary modules from PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef67ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "import h5py\n",
    "\n",
    "\n",
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d1ac6",
   "metadata": {},
   "source": [
    "    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e861bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLightningModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name='resnet34',\n",
    "            encoder_weights=None,\n",
    "            in_channels=4,\n",
    "            classes=1,\n",
    "            activation='sigmoid'\n",
    "        )\n",
    "        self.l2_loss = torch.nn.MSELoss()\n",
    "        self.l1_loss = torch.nn.L1Loss()\n",
    "        \n",
    "        #save all hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, masks = batch\n",
    "        preds = self.model(imgs).squeeze()\n",
    "        loss = self.l2_loss(preds, masks)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, masks = batch\n",
    "        preds = self.model(imgs).squeeze()\n",
    "        val_loss = self.l2_loss(preds, masks)\n",
    "        \n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        imgs, masks = batch\n",
    "        preds = self.model(imgs).squeeze()\n",
    "        test_loss = self.l2_loss(preds, masks)\n",
    "        \n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b8650",
   "metadata": {},
   "source": [
    "    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f39acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset import MyDataset\n",
    "\n",
    "    \n",
    "class MyDataModule(L.LightningDataModule):\n",
    "    def __init__(self, augmentation=None, preprocessing=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.n_training_samples = 10\n",
    "        self.n_valid_samples = 2\n",
    "        self.n_test_samples = 4\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        #get the file names\n",
    "        permutations = list(product(range(4), repeat=2))\n",
    "        file_list = []\n",
    "        properties_list = []\n",
    "        for idx1, idx2 in permutations:\n",
    "            file_name = f'256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n",
    "            file_list.append(file_name)\n",
    "\n",
    "\n",
    "        # # set breakpoint\n",
    "        # import pdb\n",
    "        #pdb.set_trace()\n",
    "        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n",
    "        \n",
    "        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n",
    "        \n",
    "        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n",
    "        \n",
    "        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n",
    "     \n",
    "    \n",
    "                      \n",
    "    def train_dataloader(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=2, drop_last=True, persistent_workers=True) # \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False, drop_last=True, persistent_workers=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6343461",
   "metadata": {},
   "source": [
    "    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n",
    "\n",
    "* from commandline, type tensorboard --logdir=lightning_logs/\n",
    "\n",
    "If youâ€™re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf5344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.Resize(256, 256),  # not needed\n",
    "        # albu.HorizontalFlip(p=0.5),\n",
    "        # albu.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Resize to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(256, 256),  \n",
    "    ]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a036a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class VisualizationCallback(Callback):\n",
    "    def on_setup(self, trainer, pl_module, datamodule):\n",
    "        for idx_ in range(4):\n",
    "            current_timestep = 10 * idx_\n",
    "            print(f'plotting for time step: {current_timestep}')\n",
    "            image, mask = datamodule.example_dataset[current_timestep]  # get some sample\n",
    "            self.visualize_tensorboard(trainer, pl_module, image, mask, current_timestep)\n",
    "\n",
    "    def visualize_tensorboard(self, trainer, pl_module, image, mask, current_timestep):\n",
    "        # Add image to TensorBoard\n",
    "        concentration = image[0, :, :].squeeze()\n",
    "        eps = image[1, :, :].squeeze()\n",
    "        Ux = image[2, :, :].squeeze()\n",
    "        Uy = image[3, :, :].squeeze()\n",
    "        dissolution = mask.squeeze()\n",
    "\n",
    "        # You need to add code here to convert these tensors to numpy arrays for visualization\n",
    "        # For example, you can use image.cpu().numpy()\n",
    "\n",
    "        # Assuming you have converted tensors to numpy arrays, you can add them to TensorBoard like this:\n",
    "        trainer.logger.experiment.add_images(\n",
    "            f'Visualization/Time_Step_{current_timestep}',\n",
    "            torch.stack([concentration, eps, Ux, Uy, dissolution], dim=0),\n",
    "            global_step=trainer.global_step,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7311b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-03-14 00:43:43.829408: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "    | Name                                        | Type             | Params\n",
      "-----------------------------------------------------------------------------------\n",
      "0   | model                                       | Unet             | 24.4 M\n",
      "1   | model.encoder                               | ResNetEncoder    | 21.3 M\n",
      "2   | model.encoder.conv1                         | Conv2d           | 12.5 K\n",
      "3   | model.encoder.bn1                           | BatchNorm2d      | 128   \n",
      "4   | model.encoder.relu                          | ReLU             | 0     \n",
      "5   | model.encoder.maxpool                       | MaxPool2d        | 0     \n",
      "6   | model.encoder.layer1                        | Sequential       | 221 K \n",
      "7   | model.encoder.layer1.0                      | BasicBlock       | 74.0 K\n",
      "8   | model.encoder.layer1.0.conv1                | Conv2d           | 36.9 K\n",
      "9   | model.encoder.layer1.0.bn1                  | BatchNorm2d      | 128   \n",
      "10  | model.encoder.layer1.0.relu                 | ReLU             | 0     \n",
      "11  | model.encoder.layer1.0.conv2                | Conv2d           | 36.9 K\n",
      "12  | model.encoder.layer1.0.bn2                  | BatchNorm2d      | 128   \n",
      "13  | model.encoder.layer1.1                      | BasicBlock       | 74.0 K\n",
      "14  | model.encoder.layer1.1.conv1                | Conv2d           | 36.9 K\n",
      "15  | model.encoder.layer1.1.bn1                  | BatchNorm2d      | 128   \n",
      "16  | model.encoder.layer1.1.relu                 | ReLU             | 0     \n",
      "17  | model.encoder.layer1.1.conv2                | Conv2d           | 36.9 K\n",
      "18  | model.encoder.layer1.1.bn2                  | BatchNorm2d      | 128   \n",
      "19  | model.encoder.layer1.2                      | BasicBlock       | 74.0 K\n",
      "20  | model.encoder.layer1.2.conv1                | Conv2d           | 36.9 K\n",
      "21  | model.encoder.layer1.2.bn1                  | BatchNorm2d      | 128   \n",
      "22  | model.encoder.layer1.2.relu                 | ReLU             | 0     \n",
      "23  | model.encoder.layer1.2.conv2                | Conv2d           | 36.9 K\n",
      "24  | model.encoder.layer1.2.bn2                  | BatchNorm2d      | 128   \n",
      "25  | model.encoder.layer2                        | Sequential       | 1.1 M \n",
      "26  | model.encoder.layer2.0                      | BasicBlock       | 230 K \n",
      "27  | model.encoder.layer2.0.conv1                | Conv2d           | 73.7 K\n",
      "28  | model.encoder.layer2.0.bn1                  | BatchNorm2d      | 256   \n",
      "29  | model.encoder.layer2.0.relu                 | ReLU             | 0     \n",
      "30  | model.encoder.layer2.0.conv2                | Conv2d           | 147 K \n",
      "31  | model.encoder.layer2.0.bn2                  | BatchNorm2d      | 256   \n",
      "32  | model.encoder.layer2.0.downsample           | Sequential       | 8.4 K \n",
      "33  | model.encoder.layer2.0.downsample.0         | Conv2d           | 8.2 K \n",
      "34  | model.encoder.layer2.0.downsample.1         | BatchNorm2d      | 256   \n",
      "35  | model.encoder.layer2.1                      | BasicBlock       | 295 K \n",
      "36  | model.encoder.layer2.1.conv1                | Conv2d           | 147 K \n",
      "37  | model.encoder.layer2.1.bn1                  | BatchNorm2d      | 256   \n",
      "38  | model.encoder.layer2.1.relu                 | ReLU             | 0     \n",
      "39  | model.encoder.layer2.1.conv2                | Conv2d           | 147 K \n",
      "40  | model.encoder.layer2.1.bn2                  | BatchNorm2d      | 256   \n",
      "41  | model.encoder.layer2.2                      | BasicBlock       | 295 K \n",
      "42  | model.encoder.layer2.2.conv1                | Conv2d           | 147 K \n",
      "43  | model.encoder.layer2.2.bn1                  | BatchNorm2d      | 256   \n",
      "44  | model.encoder.layer2.2.relu                 | ReLU             | 0     \n",
      "45  | model.encoder.layer2.2.conv2                | Conv2d           | 147 K \n",
      "46  | model.encoder.layer2.2.bn2                  | BatchNorm2d      | 256   \n",
      "47  | model.encoder.layer2.3                      | BasicBlock       | 295 K \n",
      "48  | model.encoder.layer2.3.conv1                | Conv2d           | 147 K \n",
      "49  | model.encoder.layer2.3.bn1                  | BatchNorm2d      | 256   \n",
      "50  | model.encoder.layer2.3.relu                 | ReLU             | 0     \n",
      "51  | model.encoder.layer2.3.conv2                | Conv2d           | 147 K \n",
      "52  | model.encoder.layer2.3.bn2                  | BatchNorm2d      | 256   \n",
      "53  | model.encoder.layer3                        | Sequential       | 6.8 M \n",
      "54  | model.encoder.layer3.0                      | BasicBlock       | 919 K \n",
      "55  | model.encoder.layer3.0.conv1                | Conv2d           | 294 K \n",
      "56  | model.encoder.layer3.0.bn1                  | BatchNorm2d      | 512   \n",
      "57  | model.encoder.layer3.0.relu                 | ReLU             | 0     \n",
      "58  | model.encoder.layer3.0.conv2                | Conv2d           | 589 K \n",
      "59  | model.encoder.layer3.0.bn2                  | BatchNorm2d      | 512   \n",
      "60  | model.encoder.layer3.0.downsample           | Sequential       | 33.3 K\n",
      "61  | model.encoder.layer3.0.downsample.0         | Conv2d           | 32.8 K\n",
      "62  | model.encoder.layer3.0.downsample.1         | BatchNorm2d      | 512   \n",
      "63  | model.encoder.layer3.1                      | BasicBlock       | 1.2 M \n",
      "64  | model.encoder.layer3.1.conv1                | Conv2d           | 589 K \n",
      "65  | model.encoder.layer3.1.bn1                  | BatchNorm2d      | 512   \n",
      "66  | model.encoder.layer3.1.relu                 | ReLU             | 0     \n",
      "67  | model.encoder.layer3.1.conv2                | Conv2d           | 589 K \n",
      "68  | model.encoder.layer3.1.bn2                  | BatchNorm2d      | 512   \n",
      "69  | model.encoder.layer3.2                      | BasicBlock       | 1.2 M \n",
      "70  | model.encoder.layer3.2.conv1                | Conv2d           | 589 K \n",
      "71  | model.encoder.layer3.2.bn1                  | BatchNorm2d      | 512   \n",
      "72  | model.encoder.layer3.2.relu                 | ReLU             | 0     \n",
      "73  | model.encoder.layer3.2.conv2                | Conv2d           | 589 K \n",
      "74  | model.encoder.layer3.2.bn2                  | BatchNorm2d      | 512   \n",
      "75  | model.encoder.layer3.3                      | BasicBlock       | 1.2 M \n",
      "76  | model.encoder.layer3.3.conv1                | Conv2d           | 589 K \n",
      "77  | model.encoder.layer3.3.bn1                  | BatchNorm2d      | 512   \n",
      "78  | model.encoder.layer3.3.relu                 | ReLU             | 0     \n",
      "79  | model.encoder.layer3.3.conv2                | Conv2d           | 589 K \n",
      "80  | model.encoder.layer3.3.bn2                  | BatchNorm2d      | 512   \n",
      "81  | model.encoder.layer3.4                      | BasicBlock       | 1.2 M \n",
      "82  | model.encoder.layer3.4.conv1                | Conv2d           | 589 K \n",
      "83  | model.encoder.layer3.4.bn1                  | BatchNorm2d      | 512   \n",
      "84  | model.encoder.layer3.4.relu                 | ReLU             | 0     \n",
      "85  | model.encoder.layer3.4.conv2                | Conv2d           | 589 K \n",
      "86  | model.encoder.layer3.4.bn2                  | BatchNorm2d      | 512   \n",
      "87  | model.encoder.layer3.5                      | BasicBlock       | 1.2 M \n",
      "88  | model.encoder.layer3.5.conv1                | Conv2d           | 589 K \n",
      "89  | model.encoder.layer3.5.bn1                  | BatchNorm2d      | 512   \n",
      "90  | model.encoder.layer3.5.relu                 | ReLU             | 0     \n",
      "91  | model.encoder.layer3.5.conv2                | Conv2d           | 589 K \n",
      "92  | model.encoder.layer3.5.bn2                  | BatchNorm2d      | 512   \n",
      "93  | model.encoder.layer4                        | Sequential       | 13.1 M\n",
      "94  | model.encoder.layer4.0                      | BasicBlock       | 3.7 M \n",
      "95  | model.encoder.layer4.0.conv1                | Conv2d           | 1.2 M \n",
      "96  | model.encoder.layer4.0.bn1                  | BatchNorm2d      | 1.0 K \n",
      "97  | model.encoder.layer4.0.relu                 | ReLU             | 0     \n",
      "98  | model.encoder.layer4.0.conv2                | Conv2d           | 2.4 M \n",
      "99  | model.encoder.layer4.0.bn2                  | BatchNorm2d      | 1.0 K \n",
      "100 | model.encoder.layer4.0.downsample           | Sequential       | 132 K \n",
      "101 | model.encoder.layer4.0.downsample.0         | Conv2d           | 131 K \n",
      "102 | model.encoder.layer4.0.downsample.1         | BatchNorm2d      | 1.0 K \n",
      "103 | model.encoder.layer4.1                      | BasicBlock       | 4.7 M \n",
      "104 | model.encoder.layer4.1.conv1                | Conv2d           | 2.4 M \n",
      "105 | model.encoder.layer4.1.bn1                  | BatchNorm2d      | 1.0 K \n",
      "106 | model.encoder.layer4.1.relu                 | ReLU             | 0     \n",
      "107 | model.encoder.layer4.1.conv2                | Conv2d           | 2.4 M \n",
      "108 | model.encoder.layer4.1.bn2                  | BatchNorm2d      | 1.0 K \n",
      "109 | model.encoder.layer4.2                      | BasicBlock       | 4.7 M \n",
      "110 | model.encoder.layer4.2.conv1                | Conv2d           | 2.4 M \n",
      "111 | model.encoder.layer4.2.bn1                  | BatchNorm2d      | 1.0 K \n",
      "112 | model.encoder.layer4.2.relu                 | ReLU             | 0     \n",
      "113 | model.encoder.layer4.2.conv2                | Conv2d           | 2.4 M \n",
      "114 | model.encoder.layer4.2.bn2                  | BatchNorm2d      | 1.0 K \n",
      "115 | model.decoder                               | UnetDecoder      | 3.2 M \n",
      "116 | model.decoder.center                        | Identity         | 0     \n",
      "117 | model.decoder.blocks                        | ModuleList       | 3.2 M \n",
      "118 | model.decoder.blocks.0                      | DecoderBlock     | 2.4 M \n",
      "119 | model.decoder.blocks.0.conv1                | Conv2dReLU       | 1.8 M \n",
      "120 | model.decoder.blocks.0.conv1.0              | Conv2d           | 1.8 M \n",
      "121 | model.decoder.blocks.0.conv1.1              | BatchNorm2d      | 512   \n",
      "122 | model.decoder.blocks.0.conv1.2              | ReLU             | 0     \n",
      "123 | model.decoder.blocks.0.attention1           | Attention        | 0     \n",
      "124 | model.decoder.blocks.0.attention1.attention | Identity         | 0     \n",
      "125 | model.decoder.blocks.0.conv2                | Conv2dReLU       | 590 K \n",
      "126 | model.decoder.blocks.0.conv2.0              | Conv2d           | 589 K \n",
      "127 | model.decoder.blocks.0.conv2.1              | BatchNorm2d      | 512   \n",
      "128 | model.decoder.blocks.0.conv2.2              | ReLU             | 0     \n",
      "129 | model.decoder.blocks.0.attention2           | Attention        | 0     \n",
      "130 | model.decoder.blocks.0.attention2.attention | Identity         | 0     \n",
      "131 | model.decoder.blocks.1                      | DecoderBlock     | 590 K \n",
      "132 | model.decoder.blocks.1.conv1                | Conv2dReLU       | 442 K \n",
      "133 | model.decoder.blocks.1.conv1.0              | Conv2d           | 442 K \n",
      "134 | model.decoder.blocks.1.conv1.1              | BatchNorm2d      | 256   \n",
      "135 | model.decoder.blocks.1.conv1.2              | ReLU             | 0     \n",
      "136 | model.decoder.blocks.1.attention1           | Attention        | 0     \n",
      "137 | model.decoder.blocks.1.attention1.attention | Identity         | 0     \n",
      "138 | model.decoder.blocks.1.conv2                | Conv2dReLU       | 147 K \n",
      "139 | model.decoder.blocks.1.conv2.0              | Conv2d           | 147 K \n",
      "140 | model.decoder.blocks.1.conv2.1              | BatchNorm2d      | 256   \n",
      "141 | model.decoder.blocks.1.conv2.2              | ReLU             | 0     \n",
      "142 | model.decoder.blocks.1.attention2           | Attention        | 0     \n",
      "143 | model.decoder.blocks.1.attention2.attention | Identity         | 0     \n",
      "144 | model.decoder.blocks.2                      | DecoderBlock     | 147 K \n",
      "145 | model.decoder.blocks.2.conv1                | Conv2dReLU       | 110 K \n",
      "146 | model.decoder.blocks.2.conv1.0              | Conv2d           | 110 K \n",
      "147 | model.decoder.blocks.2.conv1.1              | BatchNorm2d      | 128   \n",
      "148 | model.decoder.blocks.2.conv1.2              | ReLU             | 0     \n",
      "149 | model.decoder.blocks.2.attention1           | Attention        | 0     \n",
      "150 | model.decoder.blocks.2.attention1.attention | Identity         | 0     \n",
      "151 | model.decoder.blocks.2.conv2                | Conv2dReLU       | 37.0 K\n",
      "152 | model.decoder.blocks.2.conv2.0              | Conv2d           | 36.9 K\n",
      "153 | model.decoder.blocks.2.conv2.1              | BatchNorm2d      | 128   \n",
      "154 | model.decoder.blocks.2.conv2.2              | ReLU             | 0     \n",
      "155 | model.decoder.blocks.2.attention2           | Attention        | 0     \n",
      "156 | model.decoder.blocks.2.attention2.attention | Identity         | 0     \n",
      "157 | model.decoder.blocks.3                      | DecoderBlock     | 46.2 K\n",
      "158 | model.decoder.blocks.3.conv1                | Conv2dReLU       | 36.9 K\n",
      "159 | model.decoder.blocks.3.conv1.0              | Conv2d           | 36.9 K\n",
      "160 | model.decoder.blocks.3.conv1.1              | BatchNorm2d      | 64    \n",
      "161 | model.decoder.blocks.3.conv1.2              | ReLU             | 0     \n",
      "162 | model.decoder.blocks.3.attention1           | Attention        | 0     \n",
      "163 | model.decoder.blocks.3.attention1.attention | Identity         | 0     \n",
      "164 | model.decoder.blocks.3.conv2                | Conv2dReLU       | 9.3 K \n",
      "165 | model.decoder.blocks.3.conv2.0              | Conv2d           | 9.2 K \n",
      "166 | model.decoder.blocks.3.conv2.1              | BatchNorm2d      | 64    \n",
      "167 | model.decoder.blocks.3.conv2.2              | ReLU             | 0     \n",
      "168 | model.decoder.blocks.3.attention2           | Attention        | 0     \n",
      "169 | model.decoder.blocks.3.attention2.attention | Identity         | 0     \n",
      "170 | model.decoder.blocks.4                      | DecoderBlock     | 7.0 K \n",
      "171 | model.decoder.blocks.4.conv1                | Conv2dReLU       | 4.6 K \n",
      "172 | model.decoder.blocks.4.conv1.0              | Conv2d           | 4.6 K \n",
      "173 | model.decoder.blocks.4.conv1.1              | BatchNorm2d      | 32    \n",
      "174 | model.decoder.blocks.4.conv1.2              | ReLU             | 0     \n",
      "175 | model.decoder.blocks.4.attention1           | Attention        | 0     \n",
      "176 | model.decoder.blocks.4.attention1.attention | Identity         | 0     \n",
      "177 | model.decoder.blocks.4.conv2                | Conv2dReLU       | 2.3 K \n",
      "178 | model.decoder.blocks.4.conv2.0              | Conv2d           | 2.3 K \n",
      "179 | model.decoder.blocks.4.conv2.1              | BatchNorm2d      | 32    \n",
      "180 | model.decoder.blocks.4.conv2.2              | ReLU             | 0     \n",
      "181 | model.decoder.blocks.4.attention2           | Attention        | 0     \n",
      "182 | model.decoder.blocks.4.attention2.attention | Identity         | 0     \n",
      "183 | model.segmentation_head                     | SegmentationHead | 145   \n",
      "184 | model.segmentation_head.0                   | Conv2d           | 145   \n",
      "185 | model.segmentation_head.1                   | Identity         | 0     \n",
      "186 | model.segmentation_head.2                   | Activation       | 0     \n",
      "187 | model.segmentation_head.2.activation        | Sigmoid          | 0     \n",
      "188 | l2_loss                                     | MSELoss          | 0     \n",
      "189 | l1_loss                                     | L1Loss           | 0     \n",
      "-----------------------------------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.758    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ce667214e04575a7d53936c38e0dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from lightning.pytorch.profilers import AdvancedProfiler\n",
    "\n",
    "from pre_processing import get_preprocessing\n",
    "\n",
    "augumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "model = MyLightningModel()\n",
    "\n",
    "data_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n",
    "\n",
    "\n",
    "\n",
    "# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n",
    "# \n",
    "#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n",
    "\n",
    "#consider trying this https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n",
    "\n",
    "#fast_dev_run=True,\n",
    "\n",
    "trainer = L.Trainer(max_epochs=20, profiler=\"advanced\", \\\n",
    "                     callbacks=[VisualizationCallback(), DeviceStatsMonitor(), ModelSummary(max_depth=-1), TQDMProgressBar(refresh_rate=10), EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")])\n",
    "\n",
    "# check validation before large training step\n",
    "#num_sanity_val_steps=2, \n",
    "\n",
    "\n",
    "# \n",
    "# trainer = Trainer(callbacks=[DeviceStatsMonitor()])\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "\n",
    "# test the model \n",
    "trainer.test(model, data_module) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbb61c",
   "metadata": {},
   "source": [
    "#### data plot\n",
    "\n",
    "* maybe call back function to do the plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2149f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "for idx_ in range(4):\n",
    "    current_timestep = 10*idx_\n",
    "    print(f'plotting for time step: {current_timestep}')\n",
    "    image, mask = data_module.example_dataset[current_timestep] # get some sample\n",
    "    visualize(\n",
    "        concentration=image[0,:, :].squeeze(),\n",
    "        eps=image[1,:, :].squeeze(),\n",
    "        Ux=image[2,:, :].squeeze(),\n",
    "        Uy=image[3,:, :].squeeze(),\n",
    "        dissolution=mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c55ff",
   "metadata": {},
   "source": [
    "# to be plotted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "#     v_max = max(truth.max(), pred.max())\n",
    "#     v_min = max(truth.min(), pred.min())\n",
    "\n",
    "#     if scale:\n",
    "#         im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "#     else:\n",
    "#         im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "#     # im.set_clim(0.0, 0.3)\n",
    "#     ax[0].set_title(f'{title} prediction')\n",
    "#     divider = make_axes_locatable(ax[0])\n",
    "#     cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "#     cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "#     if scale:\n",
    "#         im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "#     else:\n",
    "#         im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n",
    "#     # im.set_clim(0.0, 0.3)\n",
    "#     ax[1].set_title(f'{title} reference')\n",
    "#     divider = make_axes_locatable(ax[1])\n",
    "#     cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "#     plt.colorbar(im, cax=cax)\n",
    "\n",
    "#     # error = np.abs(pred-truth)\n",
    "#     error = pred-truth\n",
    "\n",
    "#     im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n",
    "#     max_abs_error = np.max(np.abs(error))\n",
    "#     # Set the color limits dynamically centered around zero\n",
    "#     clim = (-max_abs_error, max_abs_error)\n",
    "#     im.set_clim(clim)\n",
    "\n",
    "#     ax[2].set_title(f'{title} error')\n",
    "#     divider = make_axes_locatable(ax[2])\n",
    "#     cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "#     plt.colorbar(im, cax=cax)\n",
    "#     plt.tight_layout()\n",
    "#     if filename is not None:\n",
    "#         plt.savefig(filename)\n",
    "#     plt.show()\n",
    "\n",
    "# for sample_idx in range(1): #12):\n",
    "#     for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n",
    "#         preds = preds_list_train[sample_idx*100+time_step, :, :]\n",
    "#         masks = masks_list_train[sample_idx*100+time_step, :, :]\n",
    "#         # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n",
    "#         matshow_error(\n",
    "#             preds,\n",
    "#             masks, \n",
    "#             title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n",
    "#             filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n",
    "#             figsize=(15, 7))\n",
    "        \n",
    "# for sample_idx in range(1): #4):\n",
    "#     for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n",
    "#         preds = preds_list_val[sample_idx*100+time_step, :, :]\n",
    "#         masks = masks_list_val[sample_idx*100+time_step, :, :]\n",
    "#         # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n",
    "#         matshow_error(\n",
    "#             preds,\n",
    "#             masks,\n",
    "#             title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n",
    "#             filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n",
    "#             figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894d5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
