{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7847033,"sourceType":"datasetVersion","datasetId":4601199}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:14:17.100140Z","iopub.execute_input":"2024-03-15T08:14:17.100519Z","iopub.status.idle":"2024-03-15T08:14:17.254910Z","shell.execute_reply.started":"2024-03-15T08:14:17.100489Z","shell.execute_reply":"2024-03-15T08:14:17.254126Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction9/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:14:17.256659Z","iopub.execute_input":"2024-03-15T08:14:17.256929Z","iopub.status.idle":"2024-03-15T08:14:17.270120Z","shell.execute_reply.started":"2024-03-15T08:14:17.256906Z","shell.execute_reply":"2024-03-15T08:14:17.269296Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/helpfunction9/dataset.py\n/kaggle/input/helpfunction9/pre_processing.py\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:14:17.271110Z","iopub.execute_input":"2024-03-15T08:14:17.271371Z","iopub.status.idle":"2024-03-15T08:15:17.479828Z","shell.execute_reply.started":"2024-03-15T08:14:17.271348Z","shell.execute_reply":"2024-03-15T08:15:17.478664Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\n  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=32c21248f79f2047500cca8976bbf1edcef742783c32a1898529b05fbf335367\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=1782ea8f352aea197df96fefcc32d0f25c521b0ba8e310c4a3558e6c0c67e42b\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.16\n    Uninstalling timm-0.9.16:\n      Successfully uninstalled timm-0.9.16\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\nLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting lightning\n  Downloading lightning-2.2.1-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.1\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:17.482358Z","iopub.execute_input":"2024-03-15T08:15:17.482670Z","iopub.status.idle":"2024-03-15T08:15:29.950038Z","shell.execute_reply.started":"2024-03-15T08:15:17.482644Z","shell.execute_reply":"2024-03-15T08:15:29.949276Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=4,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n        self.record_trainloss=[]\n        self.record_valoss=[]\n        self.record_testloss=[]\n        \n        self.validation_step_outputs = []\n        self.training_step_outputs = []\n    \n    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n        loss = self.l1_loss(preds, masks)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        self.training_step_outputs.append(loss)\n\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n\n        \n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        self.validation_step_outputs.append(val_loss)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        \n\n        \n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        \n#         metrics = {\"test_loss\": test_loss}\n#         self.log_dict(metrics)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n    def on_train_epoch_end(self):\n        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n        \n        self.record_trainloss.append(avg_train_loss)\n        \n        self.training_step_outputs=[]\n        #self.log('all_train_losses', all_train_loss)\n\n    def on_validation_epoch_end(self):\n        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n        \n        \n        self.record_valoss.append(avg_val_loss)\n        \n        self.validation_step_outputs=[]\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:29.951138Z","iopub.execute_input":"2024-03-15T08:15:29.951636Z","iopub.status.idle":"2024-03-15T08:15:29.965735Z","shell.execute_reply.started":"2024-03-15T08:15:29.951609Z","shell.execute_reply":"2024-03-15T08:15:29.964840Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n             \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=4, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4,persistent_workers=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:29.966986Z","iopub.execute_input":"2024-03-15T08:15:29.967264Z","iopub.status.idle":"2024-03-15T08:15:29.997742Z","shell.execute_reply.started":"2024-03-15T08:15:29.967225Z","shell.execute_reply":"2024-03-15T08:15:29.996913Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:29.998812Z","iopub.execute_input":"2024-03-15T08:15:29.999054Z","iopub.status.idle":"2024-03-15T08:15:30.005013Z","shell.execute_reply.started":"2024-03-15T08:15:29.999033Z","shell.execute_reply":"2024-03-15T08:15:30.004299Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=1,  # Save all checkpoints\n    monitor=None,  # Disable monitoring\n    verbose=True\n)\n\ntrainer = L.Trainer(max_epochs=20,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=20),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, \\\n                                              verbose=False)])\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \nstop\n\ntrainer.fit(model, data_module)\n\ntrain_loss=model.record_trainloss\nval_loss=model.record_valoss\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n\n# test the model \ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:30.006314Z","iopub.execute_input":"2024-03-15T08:15:30.006579Z","iopub.status.idle":"2024-03-15T08:15:31.821330Z","shell.execute_reply.started":"2024-03-15T08:15:30.006557Z","shell.execute_reply":"2024-03-15T08:15:31.818598Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,default_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/\u001b[39m\u001b[38;5;124m'\u001b[39m,\\\n\u001b[1;32m     41\u001b[0m                      callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback,TQDMProgressBar(refresh_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m),\\\n\u001b[1;32m     42\u001b[0m                                 EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \\\n\u001b[1;32m     43\u001b[0m                                               verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#VisualizationCallback(data_module)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# check validation before large training step\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#num_sanity_val_steps=2, \u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mstop\u001b[49m\n\u001b[1;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n\u001b[1;32m     51\u001b[0m train_loss\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrecord_trainloss\n","\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"],"ename":"NameError","evalue":"name 'stop' is not defined","output_type":"error"}]},{"cell_type":"code","source":"train_loss=model.record_trainloss\nval_loss=model.record_valoss","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:31.822125Z","iopub.status.idle":"2024-03-15T08:15:31.822486Z","shell.execute_reply.started":"2024-03-15T08:15:31.822309Z","shell.execute_reply":"2024-03-15T08:15:31.822325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\nval_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n\nplt.figure()\nplt.plot(train_losscpu, label=\"training Loss\")\nplt.plot(val_losscpu, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:31.823859Z","iopub.status.idle":"2024-03-15T08:15:31.824165Z","shell.execute_reply.started":"2024-03-15T08:15:31.824012Z","shell.execute_reply":"2024-03-15T08:15:31.824025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:31.825263Z","iopub.status.idle":"2024-03-15T08:15:31.825574Z","shell.execute_reply.started":"2024-03-15T08:15:31.825415Z","shell.execute_reply":"2024-03-15T08:15:31.825428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\nfor idx_ in range(4):\n    current_timestep = 10*idx_\n    print(f'plotting for time step: {current_timestep}')\n    image, mask = data_module.example_dataset[current_timestep] # get some sample\n    visualize(\n        concentration=image[0,:, :].squeeze(),\n        eps=image[1,:, :].squeeze(),\n        Ux=image[2,:, :].squeeze(),\n        Uy=image[3,:, :].squeeze(),\n        dissolution=mask[1,:, :].squeeze(),\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:15:31.826994Z","iopub.status.idle":"2024-03-15T08:15:31.827341Z","shell.execute_reply.started":"2024-03-15T08:15:31.827156Z","shell.execute_reply":"2024-03-15T08:15:31.827169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use original code to do plotting (see test results!)","metadata":{}},{"cell_type":"code","source":"def read_simulation_hdf(file_name):\n    print(f'loading the file: {file_name}')\n    data_dict = {}\n\n    with h5py.File(file_name, \"r\") as file_handle:\n        # List all groups\n        print(f\"Keys: {file_handle.keys()}\")\n        scaling_factor = 1\n        for key_ in file_handle.keys():\n            if 'key_' == 'C':\n                scaling_factor = 1 # 100\n            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n                scaling_factor = 1 # 1000\n            \n            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n\n        print(f'Done with {file_name} == closing file now')\n\n    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n\n\ndef load_datafiles(data_filenames):\n    # snapshot_indices will split the data in time into train and validation\n    data_dict = {\n        'C': [], # list of np arrays\n        'eps': [],\n        'Ux': [],\n        'Uy': [],\n    }\n\n    for filename in data_filenames:\n        C, eps, Ux, Uy = read_simulation_hdf(filename)\n        data_dict['C'].append(C[2:-2, 2:-2, :])\n        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n    return data_dict\n\ndef get_filelist():\n    from itertools import permutations, product\n    # permutations = list(permutations(range(4), 2))\n    permutations = list(product(range(4), repeat=2))\n\n    file_list = []\n    properties_list = []\n    for idx1, idx2 in permutations:\n        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n        file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n        file_list.append(file_name)\n    return file_list","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:19:16.995542Z","iopub.execute_input":"2024-03-15T08:19:16.996469Z","iopub.status.idle":"2024-03-15T08:19:17.008395Z","shell.execute_reply.started":"2024-03-15T08:19:16.996437Z","shell.execute_reply":"2024-03-15T08:19:17.007442Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, RandomSampler\n\ndef preprocess_data_cube(data_dict, scaling_dict):\n    print(f'preprocess_data_cube')\n\n    masks = []\n    images = []\n    for file_idx in range(len(data_dict['C'])):\n        C = data_dict['C'][file_idx][:, :, :-1]\n        eps = data_dict['eps'][file_idx][:, :, :-1]\n        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n\n        \n        C_t = data_dict['eps'][file_idx][:, :, 1:]\n        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n        Ux_t = data_dict['Ux'][file_idx][:, :, 1:]\n        Uy_t = data_dict['Uy'][file_idx][:, :, 1:]\n        \n        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n\n        #model baseline\n        #mask = eps_t - eps\n\n        #model II: predict next snapshot directly!\n        #mask = eps_t\n\n        #Model III\n        mask = np.stack([C_t, eps_t, Ux_t, Uy_t], axis=-1)\n        mask = np.swapaxes(mask, 3, 2)\n\n        \n        \n        # these should be moved to preprocessing\n        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n        C = C*scaling_dict['C_scaling'] - 0.5\n        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n\n\n        image = np.stack([C, eps, Ux, Uy], axis=-1)\n        image = np.swapaxes(image, 3, 2)\n\n        masks.append(mask)\n        images.append(image)\n    \n    masks = np.concatenate(masks, axis=-1)\n    images = np.concatenate(images, axis=-1)\n    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n    return images, masks\n\nclass DissolutionDataset(Dataset):\n    \"\"\"Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        data_dir (str): path to data folder\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n  \n    def __init__(\n            self,\n            data_filenames,\n            scaling_dict,\n            augmentation=None, \n            preprocessing=None,\n    ):\n\n        # self.scaling_dict = scaling_dict\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        data_dict = load_datafiles(data_filenames)\n        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n        print(self.image.shape, self.mask.shape)\n        self.data_len = self.image.shape[-1]\n\n    \n    def __getitem__(self, idx):\n        \n        image = self.image[:, :, :, idx]\n        mask = self.mask[:, :, :, idx]\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # assume one file for now\n        return self.data_len # last element we cann't predict\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:19:17.010117Z","iopub.execute_input":"2024-03-15T08:19:17.010436Z","iopub.status.idle":"2024-03-15T08:19:17.030096Z","shell.execute_reply.started":"2024-03-15T08:19:17.010413Z","shell.execute_reply":"2024-03-15T08:19:17.029054Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor_img(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef to_tensor_mask(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing():\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:19:17.031406Z","iopub.execute_input":"2024-03-15T08:19:17.031778Z","iopub.status.idle":"2024-03-15T08:19:17.042297Z","shell.execute_reply.started":"2024-03-15T08:19:17.031748Z","shell.execute_reply":"2024-03-15T08:19:17.041410Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:19:17.044153Z","iopub.execute_input":"2024-03-15T08:19:17.044750Z","iopub.status.idle":"2024-03-15T08:19:17.055825Z","shell.execute_reply.started":"2024-03-15T08:19:17.044719Z","shell.execute_reply":"2024-03-15T08:19:17.054937Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"file_list = get_filelist()\nprint(file_list)\n\ndata_list = load_datafiles(file_list)\n# estimate sample mean and std -- this should be done better\n\nn_training_samples = 8\n\n\nC = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\neps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\nUx = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\nUy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n\nUx_mean, Ux_std = Ux.mean(), Ux.std()\nUy_mean, Uy_std = Uy.mean(), Uy.std()\neps_mean, eps_std = eps.mean(), eps.std()\n\n\nprint(Ux_mean, Ux_std)\nprint(Uy_mean, Uy_std)\nprint(eps_mean, eps_std)\n\nC_scaling = 100\ndata_scalingdict = {\n    'C_scaling': C_scaling,\n    'Ux_mean': Ux_mean,\n    'Ux_std': Ux_std,\n    'Uy_mean': Uy_mean,\n    'Uy_std': Uy_std,\n    'eps_mean': eps_mean,\n    'eps_std': eps_std,\n}\n\ndel Ux, Uy, eps, C","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:19:17.056937Z","iopub.execute_input":"2024-03-15T08:19:17.057220Z","iopub.status.idle":"2024-03-15T08:20:28.501112Z","shell.execute_reply.started":"2024-03-15T08:19:17.057164Z","shell.execute_reply":"2024-03-15T08:20:28.499989Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5', '/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5']\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5 == closing file now\n3.7239328553348663e-06 9.620042645486381e-06\n-2.1806963235718202e-07 6.533084504277473e-06\n0.5015508952231883 0.48431882409167515\n","output_type":"stream"}]},{"cell_type":"code","source":"from datetime import datetime\n\ndata_filenames = get_filelist()\n\ndataset_train = DissolutionDataset(\n    data_filenames[:8],\n    scaling_dict=data_scalingdict,\n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndataset_valid = DissolutionDataset(\n    data_filenames[12:],\n    scaling_dict=data_scalingdict,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndel data_scalingdict","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:20:28.503642Z","iopub.execute_input":"2024-03-15T08:20:28.504088Z","iopub.status.idle":"2024-03-15T08:21:44.627117Z","shell.execute_reply.started":"2024-03-15T08:20:28.504055Z","shell.execute_reply":"2024-03-15T08:21:44.626077Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"loading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5 == closing file now\npreprocess_data_cube\npreprocess_data_cube: (256, 256, 4, 800), (256, 256, 4, 800)\n(256, 256, 4, 800) (256, 256, 4, 800)\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5 == closing file now\nloading the file: /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\nKeys: <KeysViewHDF5 ['C', 'Ux', 'Uy', 'eps', 'time']>\nDone loading the variable C of shape: (260, 260, 101)\nDone loading the variable Ux of shape: (260, 260, 101)\nDone loading the variable Uy of shape: (260, 260, 101)\nDone loading the variable eps of shape: (260, 260, 101)\nDone loading the variable time of shape: (101,)\nDone with /kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5 == closing file now\npreprocess_data_cube\npreprocess_data_cube: (256, 256, 4, 400), (256, 256, 4, 400)\n(256, 256, 4, 400) (256, 256, 4, 400)\n","output_type":"stream"}]},{"cell_type":"code","source":"# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\ntrain_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\ntrain_loader = DataLoader(dataset_train, batch_size=8, num_workers=2, sampler=train_sampler, drop_last=True)                        \nvalid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=2, shuffle=False, drop_last=True)\n\ndel dataset_train, dataset_valid\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:21:44.628134Z","iopub.execute_input":"2024-03-15T08:21:44.628427Z","iopub.status.idle":"2024-03-15T08:21:44.634027Z","shell.execute_reply.started":"2024-03-15T08:21:44.628403Z","shell.execute_reply":"2024-03-15T08:21:44.633151Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = MyLightningModel.load_from_checkpoint('/kaggle/working/model-epoch=0082.ckpt')\nmodel.eval()\n\ncpu_device = torch.device('cpu')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nl2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\nl1_loss = torch.nn.L1Loss() # solution is sparse\n\nwith torch.no_grad():\n    # loop 1 on training data\n    train_loss = []\n    # train_loss_scaled = []\n    preds_list_train = []\n    masks_list_train = []\n\n    for imgs, masks in tqdm(train_loader):\n        \n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True) # .squeeze()\n        preds = model(imgs).squeeze()\n\n        \n        \n        l2_loss_values = l2_loss(masks, preds)\n        train_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_train.append(masks.to(cpu_device).numpy()[:,:,1])\n        preds_list_train.append(preds.to(cpu_device).numpy()[:,:,1])\n\n    train_loss = np.array(train_loss)\n    print(f'train_loss: {train_loss.mean()}')\n    \n    val_loss = []\n    preds_list_val = []\n    masks_list_val = []\n\n    # loop 2 on validation data\n    for imgs, masks in tqdm(valid_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)# .squeeze()\n        preds = model(imgs).squeeze()\n\n    \n        \n        l2_loss_values = l2_loss(masks, preds)\n        val_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_val.append(masks.to(cpu_device).numpy()[:,:,1])\n        preds_list_val.append(preds.to(cpu_device).numpy()[:,:,1])\n\n    val_loss = np.array(val_loss)\n    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:21:44.635330Z","iopub.execute_input":"2024-03-15T08:21:44.635952Z","iopub.status.idle":"2024-03-15T08:21:45.165702Z","shell.execute_reply.started":"2024-03-15T08:21:44.635920Z","shell.execute_reply":"2024-03-15T08:21:45.164297Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyLightningModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/model-epoch=0082.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m cpu_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:56\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     53\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1293\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1293\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:184\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:306\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:311\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    313\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/model-epoch=0082.ckpt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/model-epoch=0082.ckpt'","output_type":"error"}]},{"cell_type":"code","source":"preds_list_train = np.concatenate(preds_list_train)\nmasks_list_train = np.concatenate(masks_list_train)\n\npreds_list_val = np.concatenate(preds_list_val)\nmasks_list_val = np.concatenate(masks_list_val)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:21:45.166999Z","iopub.status.idle":"2024-03-15T08:21:45.167359Z","shell.execute_reply.started":"2024-03-15T08:21:45.167171Z","shell.execute_reply":"2024-03-15T08:21:45.167184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    \n    v_max = max(truth.max(), pred.max())\n    v_min = max(truth.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[2].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\nfor sample_idx in range(1): #12):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_train[sample_idx*100+time_step, :, :].squeeze()\n        masks = masks_list_train[sample_idx*100+time_step, :, :].squeeze()\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks, \n            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n\n        \nfor sample_idx in range(1): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_val[sample_idx*100+time_step, :, :].squeeze()\n        masks = masks_list_val[sample_idx*100+time_step, :, :].squeeze()\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:21:45.168367Z","iopub.status.idle":"2024-03-15T08:21:45.168689Z","shell.execute_reply.started":"2024-03-15T08:21:45.168535Z","shell.execute_reply":"2024-03-15T08:21:45.168548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# for dirname, _, filenames in os.walk('/kaggle/working/tb_logs'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T08:21:45.169957Z","iopub.status.idle":"2024-03-15T08:21:45.170313Z","shell.execute_reply.started":"2024-03-15T08:21:45.170125Z","shell.execute_reply":"2024-03-15T08:21:45.170139Z"},"trusted":true},"execution_count":null,"outputs":[]}]}