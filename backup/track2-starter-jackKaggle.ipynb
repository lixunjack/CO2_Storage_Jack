{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7802501,"sourceType":"datasetVersion","datasetId":4568887},{"sourceId":7843411,"sourceType":"datasetVersion","datasetId":4598564}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nh5py._errors.unsilence_errors()\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# SUGGESTION: create all folders for storing results\nif not os.path.exists('./vis'):\n    os.mkdir('./vis')\n\nif not os.path.exists('./vis_results'):\n    os.mkdir('./vis_results')\n\nif not os.path.exists('./model256_weights'):\n    os.mkdir('./model256_weights')\n\n# if not os.path.exists('./tb_logs'):\n#     os.mkdir('./tb_logs')\n\nif not os.path.exists('./lightning_logs'):\n    os.mkdir('./lightning_logs')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.921475Z","iopub.execute_input":"2024-03-14T19:28:27.922463Z","iopub.status.idle":"2024-03-14T19:28:27.929453Z","shell.execute_reply.started":"2024-03-14T19:28:27.922425Z","shell.execute_reply":"2024-03-14T19:28:27.928354Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#import other module\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nsys.path.append('/kaggle/input/helpfunction3/')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.955831Z","iopub.execute_input":"2024-03-14T19:28:27.956105Z","iopub.status.idle":"2024-03-14T19:28:27.964187Z","shell.execute_reply.started":"2024-03-14T19:28:27.956081Z","shell.execute_reply":"2024-03-14T19:28:27.963283Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/input/helpfunction3/dataset.py\n/kaggle/input/helpfunction3/pre_processing.py\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_3.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_0_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_1.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_1_0.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_3_2.hdf5\n/kaggle/input/track2dataset/256modelruns/Pe1_K1_2_3.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U segmentation_models_pytorch\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n!pip install tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:28:27.997394Z","iopub.execute_input":"2024-03-14T19:28:27.997638Z","iopub.status.idle":"2024-03-14T19:29:21.869794Z","shell.execute_reply.started":"2024-03-14T19:28:27.997618Z","shell.execute_reply":"2024-03-14T19:29:21.868441Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"    Import Lightning: Import the necessary modules from PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\n\n\nimport lightning as L\n\nimport albumentations as albu\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\n\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom itertools import product\nimport h5py\n\n\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.872442Z","iopub.execute_input":"2024-03-14T19:29:21.873392Z","iopub.status.idle":"2024-03-14T19:29:21.880457Z","shell.execute_reply.started":"2024-03-14T19:29:21.873331Z","shell.execute_reply":"2024-03-14T19:29:21.879397Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n\n","metadata":{}},{"cell_type":"code","source":"class MyLightningModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = smp.Unet(\n            encoder_name='resnet34',\n            encoder_weights=None,\n            in_channels=4,\n            classes=1,\n            activation='sigmoid'\n        )\n        self.l2_loss = torch.nn.MSELoss()\n        self.l1_loss = torch.nn.L1Loss()\n        \n        #save all hyperparameters\n        self.save_hyperparameters()\n        \n        self.record_trainloss=[]\n        self.record_valoss=[]\n        self.record_testloss=[]\n        \n        self.validation_step_outputs = []\n        self.training_step_outputs = []\n    \n    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        loss = self.l1_loss(preds, masks)\n        \n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n        \n        self.training_step_outputs.append(loss)\n\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        val_loss = self.l1_loss(preds, masks)\n        \n        self.log('val_loss', val_loss, prog_bar=True)\n        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n        \n        self.validation_step_outputs.append(val_loss)\n        \n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        \n        imgs, masks = batch\n        preds = self.model(imgs).squeeze()\n        test_loss = self.l1_loss(preds, masks)\n        \n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        \n#         metrics = {\"test_loss\": test_loss}\n#         self.log_dict(metrics)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n        \n        return optimizer\n\n    def on_train_epoch_end(self):\n        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n        \n        self.record_trainloss.append(avg_train_loss)\n        \n        self.training_step_outputs=[]\n        #self.log('all_train_losses', all_train_loss)\n\n    def on_validation_epoch_end(self):\n        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n        \n        \n        self.record_valoss.append(avg_val_loss)\n        \n        self.validation_step_outputs=[]\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.881919Z","iopub.execute_input":"2024-03-14T19:29:21.882195Z","iopub.status.idle":"2024-03-14T19:29:21.898484Z","shell.execute_reply.started":"2024-03-14T19:29:21.882172Z","shell.execute_reply":"2024-03-14T19:29:21.897629Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic.","metadata":{}},{"cell_type":"code","source":"\nfrom dataset import MyDataset\n\n    \nclass MyDataModule(L.LightningDataModule):\n    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.n_training_samples = 10\n        self.n_valid_samples = 2\n        self.n_test_samples = 4\n\n        \n    def setup(self, stage=None):\n\n        #get the file names\n        permutations = list(product(range(4), repeat=2))\n        file_list = []\n        properties_list = []\n        for idx1, idx2 in permutations:\n            file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n            file_list.append(file_name)\n\n\n        # # set breakpoint\n        # import pdb\n        #pdb.set_trace()\n        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n        \n        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n        \n        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n        \n        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n     \n             \n    def train_dataloader(self):\n        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=4, drop_last=True, persistent_workers=True) # \n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False, drop_last=True, persistent_workers=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4,persistent_workers=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.900463Z","iopub.execute_input":"2024-03-14T19:29:21.900863Z","iopub.status.idle":"2024-03-14T19:29:21.914209Z","shell.execute_reply.started":"2024-03-14T19:29:21.900838Z","shell.execute_reply":"2024-03-14T19:29:21.913192Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n\n* from commandline, type tensorboard --logdir=lightning_logs/\n\n","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.915554Z","iopub.execute_input":"2024-03-14T19:29:21.915866Z","iopub.status.idle":"2024-03-14T19:29:21.928051Z","shell.execute_reply.started":"2024-03-14T19:29:21.915843Z","shell.execute_reply":"2024-03-14T19:29:21.927320Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelSummary\nfrom lightning.pytorch.callbacks import TQDMProgressBar\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nfrom pre_processing import get_preprocessing\n\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n\naugumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n# if __name__ == \"__main__\":\n\nmodel = MyLightningModel()\n\ndata_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n\n\n# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n#fast_dev_run=True,\n#ModelSummary(max_depth=-1), no need for baseline model\n#profiler=\"simple\"\n\n# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n#save per epoch!\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='model-{epoch:04d}',\n    save_top_k=1,  # Save all checkpoints\n    monitor=None,  # Disable monitoring\n    verbose=True\n)\n\ntrainer = L.Trainer(max_epochs=12,default_root_dir='/kaggle/working/',\\\n                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=20),\\\n                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, \\\n                                              verbose=False)])\n#VisualizationCallback(data_module)\n\n# check validation before large training step\n#num_sanity_val_steps=2, \n\ntrainer.fit(model, data_module)\n\ntrain_loss=model.record_trainloss\nval_loss=model.record_valoss\n\n\ntrainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n\n\n# test the model \ntrainer.test(model, data_module) \n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:29:21.929284Z","iopub.execute_input":"2024-03-14T19:29:21.929576Z","iopub.status.idle":"2024-03-14T19:41:34.509327Z","shell.execute_reply.started":"2024-03-14T19:29:21.929553Z","shell.execute_reply":"2024-03-14T19:41:34.506885Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name    | Type    | Params\n------------------------------------\n0 | model   | Unet    | 24.4 M\n1 | l2_loss | MSELoss | 0     \n2 | l1_loss | L1Loss  | 0     \n------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.758    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d27644a7cb441d5afd99b3c84f458d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=8` reached.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060e8aaa07584401a95288ba56447e8f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_checkpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/example.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# test the model \u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:754\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:794\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    791\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    793\u001b[0m )\n\u001b[0;32m--> 794\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    796\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:425\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[19], line 64\u001b[0m, in \u001b[0;36mMyLightningModel.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_loss}\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/module.py:599\u001b[0m, in \u001b[0;36mLightningModule.log_dict\u001b[0;34m(self, dictionary, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, rank_zero_only)\u001b[0m\n\u001b[1;32m    596\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dictionary\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprog_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_fx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_fx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43msync_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43msync_dist_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_dist_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_dataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_dataloader_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank_zero_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_zero_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/module.py:516\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# we could set false here if there's no configured logger, however, we still need to compute the \"logged\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# metrics anyway because that's what the evaluation loops use as return value\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_fx_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprog_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_fx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_fx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_dataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_dataloader_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_dist\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accelerator_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_distributed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_dist_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_dist_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_dist_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_attribute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_attribute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_zero_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_zero_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_current_fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:410\u001b[0m, in \u001b[0;36m_ResultCollection.log\u001b[0;34m(self, fx, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_fn, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# check the stored metadata and the current one match\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meta \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m[key]\u001b[38;5;241m.\u001b[39mmeta:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou called `self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ...)` twice in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` with different arguments. This is not allowed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_batch_size(\u001b[38;5;28mself\u001b[39m[key], batch_size, meta)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_metrics(key, value, batch_size)\n","\u001b[0;31mMisconfigurationException\u001b[0m: You called `self.log(test_loss, ...)` twice in `test_step` with different arguments. This is not allowed"],"ename":"MisconfigurationException","evalue":"You called `self.log(test_loss, ...)` twice in `test_step` with different arguments. This is not allowed","output_type":"error"}]},{"cell_type":"code","source":"train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\nval_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n\nplt.figure()\nplt.plot(train_losscpu, label=\"training Loss\")\nplt.plot(val_losscpu, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:04:22.397273Z","iopub.execute_input":"2024-03-14T20:04:22.397744Z","iopub.status.idle":"2024-03-14T20:04:22.683926Z","shell.execute_reply.started":"2024-03-14T20:04:22.397712Z","shell.execute_reply":"2024-03-14T20:04:22.682964Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nView logs in tensorboard\n\nIf you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{}},{"cell_type":"code","source":"# !kill 400      \n# %reload_ext tensorboard\n# %tensorboard --logdir lightning_logs/\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.512291Z","iopub.status.idle":"2024-03-14T19:41:34.512777Z","shell.execute_reply.started":"2024-03-14T19:41:34.512545Z","shell.execute_reply":"2024-03-14T19:41:34.512564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Plots","metadata":{}},{"cell_type":"code","source":"\ndef visualize(**images):\n    \"\"\"Plot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\nfor idx_ in range(4):\n    current_timestep = 10*idx_\n    print(f'plotting for time step: {current_timestep}')\n    image, mask = data_module.example_dataset[current_timestep] # get some sample\n    visualize(\n        concentration=image[0,:, :].squeeze(),\n        eps=image[1,:, :].squeeze(),\n        Ux=image[2,:, :].squeeze(),\n        Uy=image[3,:, :].squeeze(),\n        dissolution=mask.squeeze(),\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.514235Z","iopub.status.idle":"2024-03-14T19:41:34.514733Z","shell.execute_reply.started":"2024-03-14T19:41:34.514485Z","shell.execute_reply":"2024-03-14T19:41:34.514505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use original code to do plotting (see test results!)","metadata":{}},{"cell_type":"code","source":"def read_simulation_hdf(file_name):\n    print(f'loading the file: {file_name}')\n    data_dict = {}\n\n    with h5py.File(file_name, \"r\") as file_handle:\n        # List all groups\n        print(f\"Keys: {file_handle.keys()}\")\n        scaling_factor = 1\n        for key_ in file_handle.keys():\n            if 'key_' == 'C':\n                scaling_factor = 1 # 100\n            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n                scaling_factor = 1 # 1000\n            \n            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n\n        print(f'Done with {file_name} == closing file now')\n\n    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n\n\ndef load_datafiles(data_filenames):\n    # snapshot_indices will split the data in time into train and validation\n    data_dict = {\n        'C': [], # list of np arrays\n        'eps': [],\n        'Ux': [],\n        'Uy': [],\n    }\n\n    for filename in data_filenames:\n        C, eps, Ux, Uy = read_simulation_hdf(filename)\n        data_dict['C'].append(C[2:-2, 2:-2, :])\n        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n    return data_dict\n\ndef get_filelist():\n    from itertools import permutations, product\n    # permutations = list(permutations(range(4), 2))\n    permutations = list(product(range(4), repeat=2))\n\n    file_list = []\n    properties_list = []\n    for idx1, idx2 in permutations:\n        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n        file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n        file_list.append(file_name)\n    return file_list","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.516141Z","iopub.status.idle":"2024-03-14T19:41:34.516538Z","shell.execute_reply.started":"2024-03-14T19:41:34.516330Z","shell.execute_reply":"2024-03-14T19:41:34.516369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, RandomSampler\n\ndef preprocess_data_cube(data_dict, scaling_dict):\n    print(f'preprocess_data_cube')\n\n    masks = []\n    images = []\n    for file_idx in range(len(data_dict['C'])):\n        C = data_dict['C'][file_idx][:, :, :-1]\n        eps = data_dict['eps'][file_idx][:, :, :-1]\n        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n\n        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n\n        #model baseline\n        #mask = eps_t - eps\n\n        #model II: predict next snapshot directly!\n        mask = eps_t\n\n        #Model III\n#         mask = np.stack([C_t, eps_t, Ux_t, Uy_t], axis=-1)\n#         mask = np.swapaxes(mask, 3, 2)\n\n        \n        \n        # these should be moved to preprocessing\n        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n        C = C*scaling_dict['C_scaling'] - 0.5\n        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n\n\n        image = np.stack([C, eps, Ux, Uy], axis=-1)\n        image = np.swapaxes(image, 3, 2)\n\n        masks.append(mask)\n        images.append(image)\n    \n    masks = np.concatenate(masks, axis=-1)\n    images = np.concatenate(images, axis=-1)\n    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n    return images, masks\n\nclass DissolutionDataset(Dataset):\n    \"\"\"Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        data_dir (str): path to data folder\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n  \n    def __init__(\n            self,\n            data_filenames,\n            scaling_dict,\n            augmentation=None, \n            preprocessing=None,\n    ):\n\n        # self.scaling_dict = scaling_dict\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        data_dict = load_datafiles(data_filenames)\n        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n        print(self.image.shape, self.mask.shape)\n        self.data_len = self.image.shape[-1]\n\n    \n    def __getitem__(self, idx):\n        \n        image = self.image[:, :, :, idx]\n        mask = self.mask[:, :, idx]\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # assume one file for now\n        return self.data_len # last element we cann't predict\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.518156Z","iopub.status.idle":"2024-03-14T19:41:34.518525Z","shell.execute_reply.started":"2024-03-14T19:41:34.518326Z","shell.execute_reply":"2024-03-14T19:41:34.518354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.Resize(256, 256),  # not needed\n        # albu.HorizontalFlip(p=0.5),\n        # albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Resize to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.Resize(256, 256),  \n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor_img(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef to_tensor_mask(x, **kwargs):\n    return x.astype('float32')\n\ndef get_preprocessing():\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.519398Z","iopub.status.idle":"2024-03-14T19:41:34.519705Z","shell.execute_reply.started":"2024-03-14T19:41:34.519553Z","shell.execute_reply":"2024-03-14T19:41:34.519566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import RandomSampler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.520666Z","iopub.status.idle":"2024-03-14T19:41:34.521005Z","shell.execute_reply.started":"2024-03-14T19:41:34.520841Z","shell.execute_reply":"2024-03-14T19:41:34.520855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = get_filelist()\nprint(file_list)\n\ndata_list = load_datafiles(file_list)\n# estimate sample mean and std -- this should be done better\n\nn_training_samples = 8\n\n\nC = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\neps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\nUx = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\nUy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n\nUx_mean, Ux_std = Ux.mean(), Ux.std()\nUy_mean, Uy_std = Uy.mean(), Uy.std()\neps_mean, eps_std = eps.mean(), eps.std()\n\n\nprint(Ux_mean, Ux_std)\nprint(Uy_mean, Uy_std)\nprint(eps_mean, eps_std)\n\nC_scaling = 100\ndata_scalingdict = {\n    'C_scaling': C_scaling,\n    'Ux_mean': Ux_mean,\n    'Ux_std': Ux_std,\n    'Uy_mean': Uy_mean,\n    'Uy_std': Uy_std,\n    'eps_mean': eps_mean,\n    'eps_std': eps_std,\n}\n\ndel Ux, Uy, eps, C","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.523014Z","iopub.status.idle":"2024-03-14T19:41:34.523507Z","shell.execute_reply.started":"2024-03-14T19:41:34.523233Z","shell.execute_reply":"2024-03-14T19:41:34.523252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\ndata_filenames = get_filelist()\n\ndataset_train = DissolutionDataset(\n    data_filenames[:8],\n    scaling_dict=data_scalingdict,\n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndataset_valid = DissolutionDataset(\n    data_filenames[12:],\n    scaling_dict=data_scalingdict,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(),\n)\n\ndel data_scalingdict","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.524913Z","iopub.status.idle":"2024-03-14T19:41:34.525260Z","shell.execute_reply.started":"2024-03-14T19:41:34.525099Z","shell.execute_reply":"2024-03-14T19:41:34.525113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\ntrain_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\ntrain_loader = DataLoader(dataset_train, batch_size=64, num_workers=4, sampler=train_sampler, drop_last=True)                        \nvalid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=4, shuffle=False, drop_last=True)\n\ndel dataset_train, dataset_valid\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.526590Z","iopub.status.idle":"2024-03-14T19:41:34.526999Z","shell.execute_reply.started":"2024-03-14T19:41:34.526830Z","shell.execute_reply":"2024-03-14T19:41:34.526845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyLightningModel.load_from_checkpoint('/kaggle/working/example.ckpt')\nmodel.eval()\n\ncpu_device = torch.device('cpu')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nl2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\nl1_loss = torch.nn.L1Loss() # solution is sparse\n\nwith torch.no_grad():\n    # loop 1 on training data\n    train_loss = []\n    # train_loss_scaled = []\n    preds_list_train = []\n    masks_list_train = []\n\n    for imgs, masks in tqdm(train_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True) # .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        train_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_train.append(masks.to(cpu_device).numpy())\n        preds_list_train.append(preds.to(cpu_device).numpy())\n\n    train_loss = np.array(train_loss)\n    print(f'train_loss: {train_loss.mean()}')\n    \n    val_loss = []\n    preds_list_val = []\n    masks_list_val = []\n\n    # loop 2 on validation data\n    for imgs, masks in tqdm(valid_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)# .squeeze()\n        preds = model(imgs).squeeze()\n\n        l2_loss_values = l2_loss(masks, preds)\n        val_loss.append(l2_loss_values.item())\n\n        # store the true values\n        # Changed to store in CPU\n        masks_list_val.append(masks.to(cpu_device).numpy())\n        preds_list_val.append(preds.to(cpu_device).numpy())\n\n    val_loss = np.array(val_loss)\n    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.528427Z","iopub.status.idle":"2024-03-14T19:41:34.528781Z","shell.execute_reply.started":"2024-03-14T19:41:34.528623Z","shell.execute_reply":"2024-03-14T19:41:34.528637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_list_train = np.concatenate(preds_list_train)\nmasks_list_train = np.concatenate(masks_list_train)\n\npreds_list_val = np.concatenate(preds_list_val)\nmasks_list_val = np.concatenate(masks_list_val)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.529862Z","iopub.status.idle":"2024-03-14T19:41:34.530191Z","shell.execute_reply.started":"2024-03-14T19:41:34.530027Z","shell.execute_reply":"2024-03-14T19:41:34.530042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    \n    v_max = max(truth.max(), pred.max())\n    v_min = max(truth.min(), pred.min())\n\n    if scale:\n        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[0].set_title(f'{title} prediction')\n    divider = make_axes_locatable(ax[0])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(im, cax=cax)\n\n    if scale:\n        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    else:\n        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n    # im.set_clim(0.0, 0.3)\n    ax[1].set_title(f'{title} reference')\n    divider = make_axes_locatable(ax[1])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n\n    # error = np.abs(pred-truth)\n    error = pred-truth\n\n    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n    max_abs_error = np.max(np.abs(error))\n    # Set the color limits dynamically centered around zero\n    clim = (-max_abs_error, max_abs_error)\n    im.set_clim(clim)\n\n    ax[2].set_title(f'{title} error')\n    divider = make_axes_locatable(ax[2])\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    plt.colorbar(im, cax=cax)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    plt.show()\n\nfor sample_idx in range(1): #12):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_train[sample_idx*100+time_step, :, :]\n        masks = masks_list_train[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks, \n            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n\n        \nfor sample_idx in range(1): #4):\n    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n        preds = preds_list_val[sample_idx*100+time_step, :, :]\n        masks = masks_list_val[sample_idx*100+time_step, :, :]\n        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n        matshow_error(\n            preds,\n            masks,\n            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n            figsize=(15, 7))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.531698Z","iopub.status.idle":"2024-03-14T19:41:34.532154Z","shell.execute_reply.started":"2024-03-14T19:41:34.531918Z","shell.execute_reply":"2024-03-14T19:41:34.531936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# for dirname, _, filenames in os.walk('/kaggle/working/tb_logs'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:41:34.533899Z","iopub.status.idle":"2024-03-14T19:41:34.534264Z","shell.execute_reply.started":"2024-03-14T19:41:34.534086Z","shell.execute_reply":"2024-03-14T19:41:34.534101Z"},"trusted":true},"execution_count":null,"outputs":[]}]}