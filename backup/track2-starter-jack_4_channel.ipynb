{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:03.944892Z","iopub.status.busy":"2024-03-14T21:47:03.944525Z","iopub.status.idle":"2024-03-14T21:47:03.952703Z","shell.execute_reply":"2024-03-14T21:47:03.951634Z","shell.execute_reply.started":"2024-03-14T21:47:03.944863Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","h5py._errors.unsilence_errors()\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","\n","# SUGGESTION: create all folders for storing results\n","if not os.path.exists('./vis'):\n","    os.mkdir('./vis')\n","\n","if not os.path.exists('./vis_results'):\n","    os.mkdir('./vis_results')\n","\n","if not os.path.exists('./model256_weights'):\n","    os.mkdir('./model256_weights')\n","\n","# if not os.path.exists('./tb_logs'):\n","#     os.mkdir('./tb_logs')\n","\n","if not os.path.exists('./lightning_logs'):\n","    os.mkdir('./lightning_logs')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:03.955031Z","iopub.status.busy":"2024-03-14T21:47:03.954681Z","iopub.status.idle":"2024-03-14T21:47:03.970852Z","shell.execute_reply":"2024-03-14T21:47:03.969922Z","shell.execute_reply.started":"2024-03-14T21:47:03.955006Z"},"trusted":true},"outputs":[],"source":["#import other module\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        \n","import sys\n","sys.path.append('/kaggle/input/helpfunction5/')\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:03.972335Z","iopub.status.busy":"2024-03-14T21:47:03.972036Z","iopub.status.idle":"2024-03-14T21:47:59.239215Z","shell.execute_reply":"2024-03-14T21:47:59.238120Z","shell.execute_reply.started":"2024-03-14T21:47:03.972300Z"},"trusted":true},"outputs":[],"source":["# !pip install -U segmentation_models_pytorch\n","# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","# !pip install lightning\n","# !pip install tensorboard\n"]},{"cell_type":"markdown","metadata":{},"source":["    Import Lightning: Import the necessary modules from PyTorch Lightning"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:59.242942Z","iopub.status.busy":"2024-03-14T21:47:59.242511Z","iopub.status.idle":"2024-03-14T21:47:59.249466Z","shell.execute_reply":"2024-03-14T21:47:59.248575Z","shell.execute_reply.started":"2024-03-14T21:47:59.242904Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, random_split\n","\n","\n","import lightning as L\n","\n","import albumentations as albu\n","\n","import segmentation_models_pytorch as smp\n","import numpy as np\n","from datetime import datetime\n","from tqdm.notebook import tqdm\n","\n","import torch.nn.functional as F\n","import pandas as pd\n","\n","from itertools import product\n","import h5py\n","\n","\n","from torch.utils.data import RandomSampler"]},{"cell_type":"markdown","metadata":{},"source":["    Define LightningModule: Create a LightningModule class that inherits from pl.LightningModule. This class will contain your model architecture and training logic.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:59.251683Z","iopub.status.busy":"2024-03-14T21:47:59.251037Z","iopub.status.idle":"2024-03-14T21:47:59.268708Z","shell.execute_reply":"2024-03-14T21:47:59.267842Z","shell.execute_reply.started":"2024-03-14T21:47:59.251651Z"},"trusted":true},"outputs":[],"source":["class MyLightningModel(L.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.model = smp.Unet(\n","            encoder_name='resnet34',\n","            encoder_weights=None,\n","            in_channels=4,\n","            classes=4,\n","            activation='sigmoid'\n","        )\n","        self.l2_loss = torch.nn.MSELoss()\n","        self.l1_loss = torch.nn.L1Loss()\n","        \n","        #save all hyperparameters\n","        self.save_hyperparameters()\n","        \n","        self.record_trainloss=[]\n","        self.record_valoss=[]\n","        self.record_testloss=[]\n","        \n","        self.validation_step_outputs = []\n","        self.training_step_outputs = []\n","    \n","    #When using forward, you are responsible to call eval() and use the no_grad() context manager.\n","    def forward(self, x):\n","        \n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        imgs, masks = batch\n","        preds = self.model(imgs).squeeze()\n","\n","        print(f'training_step imgs preds masks: {imgs.shape}, {preds.shape}, {masks.shape}')\n","        \n","        loss = self.l1_loss(preds, masks)\n","        \n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        #self.logger.experiment.add_scalar('train_loss',loss, self.current_epoch)\n","        \n","        self.training_step_outputs.append(loss)\n","\n","        \n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        \n","        imgs, masks = batch\n","        \n","        \n","        preds = self.model(imgs).squeeze()\n","        print(f'validation_step imgs preds masks: {imgs.shape}, {preds.shape}, {masks.shape}')\n","        \n","        \n","\n","\n","        \n","        val_loss = self.l1_loss(preds, masks)\n","        \n","        self.log('val_loss', val_loss, prog_bar=True)\n","        #self.logger.experiment.add_scalar('val_loss',val_loss, self.current_epoch)\n","        \n","        self.validation_step_outputs.append(val_loss)\n","        \n","        return val_loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        \n","        imgs, masks = batch\n","        preds = self.model(imgs).squeeze()\n","        \n","        print(f'test_step imgs preds masks: {imgs.shape}, {preds.shape}, {masks.shape}')\n","        test_loss = self.l1_loss(preds, masks)\n","        \n","        \n","        \n","        self.log(\"test_loss\", test_loss, prog_bar=True)\n","        \n","        \n","        \n","#         metrics = {\"test_loss\": test_loss}\n","#         self.log_dict(metrics)\n","    \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam([ dict(params=self.model.parameters(), lr=5e-4),])\n","        \n","        return optimizer\n","\n","    def on_train_epoch_end(self):\n","        avg_train_loss = torch.stack(self.training_step_outputs).mean()\n","        \n","        self.record_trainloss.append(avg_train_loss)\n","        \n","        self.training_step_outputs=[]\n","\n","\n","    def on_validation_epoch_end(self):\n","        avg_val_loss = torch.stack(self.validation_step_outputs).mean()\n","        \n","        \n","        self.record_valoss.append(avg_val_loss)\n","        \n","        self.validation_step_outputs=[]\n","        \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["    Define LightningDataModule: If you're using custom data loaders, create a LightningDataModule class that inherits from pl.LightningDataModule. This class will contain your data loading logic."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:59.270484Z","iopub.status.busy":"2024-03-14T21:47:59.270123Z","iopub.status.idle":"2024-03-14T21:47:59.284958Z","shell.execute_reply":"2024-03-14T21:47:59.284130Z","shell.execute_reply.started":"2024-03-14T21:47:59.270454Z"},"trusted":true},"outputs":[],"source":["\n","import importlib\n","import dataset\n","importlib.reload(dataset)\n","\n","from dataset import MyDataset\n","\n","    \n","class MyDataModule(L.LightningDataModule):\n","    def __init__(self, augmentation=None, preprocessing=None, batch_size=64):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","        self.n_training_samples = 10\n","        self.n_valid_samples = 2\n","        self.n_test_samples = 4\n","\n","        \n","    def setup(self, stage=None):\n","\n","        #get the file names\n","        permutations = list(product(range(4), repeat=2))\n","        file_list = []\n","        properties_list = []\n","        for idx1, idx2 in permutations:\n","            file_name = f'256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n","            file_list.append(file_name)\n","\n","\n","        # # set breakpoint\n","        # import pdb\n","        #pdb.set_trace()\n","        self.example_dataset = MyDataset(file_list[:3],self.augmentation[0], self.preprocessing)\n","        self.train_dataset = MyDataset(file_list[:self.n_training_samples],self.augmentation[0], self.preprocessing)\n","        self.val_dataset = MyDataset(file_list[self.n_training_samples : self.n_training_samples+self.n_valid_samples],self.augmentation[1], self.preprocessing)\n","        \n","        self.test_dataset = MyDataset(file_list[-self.n_test_samples :], self.augmentation[2], self.preprocessing)\n","     \n","             \n","    def train_dataloader(self):\n","        train_sampler = RandomSampler(self.train_dataset, replacement=True, num_samples=10000) \n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=train_sampler, num_workers=1, drop_last=True) # \n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=1, shuffle=False, drop_last=True)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["    Training Loop with Trainer: Create a pl.Trainer object and use it to train your LightningModule.\n","\n","* from commandline, type tensorboard --logdir=lightning_logs/\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:59.286474Z","iopub.status.busy":"2024-03-14T21:47:59.286124Z","iopub.status.idle":"2024-03-14T21:47:59.299374Z","shell.execute_reply":"2024-03-14T21:47:59.298550Z","shell.execute_reply.started":"2024-03-14T21:47:59.286443Z"},"trusted":true},"outputs":[],"source":["def get_training_augmentation():\n","    train_transform = [\n","        albu.Resize(256, 256),  # not needed\n","        # albu.HorizontalFlip(p=0.5),\n","        # albu.VerticalFlip(p=0.5),\n","    ]\n","    return albu.Compose(train_transform)\n","\n","def get_validation_augmentation():\n","    \"\"\"Resize to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        albu.Resize(256, 256),  \n","    ]\n","    return albu.Compose(test_transform)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T21:47:59.301145Z","iopub.status.busy":"2024-03-14T21:47:59.300731Z","iopub.status.idle":"2024-03-14T21:49:43.082236Z","shell.execute_reply":"2024-03-14T21:49:43.079780Z","shell.execute_reply.started":"2024-03-14T21:47:59.301122Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","2024-03-15 00:12:44.676027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/Users/captainjack/anaconda3/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /Users/captainjack/Desktop/miscellaous/CO2_Storage_Jack exists and is not empty.\n","\n","  | Name    | Type    | Params\n","------------------------------------\n","0 | model   | Unet    | 24.4 M\n","1 | l2_loss | MSELoss | 0     \n","2 | l1_loss | L1Loss  | 0     \n","------------------------------------\n","24.4 M    Trainable params\n","0         Non-trainable params\n","24.4 M    Total params\n","97.760    Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"632dfb21257b4f9c85874e855a5463e3","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/captainjack/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"]},{"name":"stdout","output_type":"stream","text":["validation_step imgs preds masks: torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256])\n","validation_step imgs preds masks: torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256])\n"]},{"name":"stderr","output_type":"stream","text":["/Users/captainjack/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45c5c645a60f468293c0066cc13b0a4e","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["training_step imgs preds masks: torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256])\n","training_step imgs preds masks: torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256]), torch.Size([64, 4, 256, 256])\n"]},{"name":"stderr","output_type":"stream","text":["/Users/captainjack/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"]}],"source":["from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n","from lightning.pytorch.callbacks import ModelSummary\n","from lightning.pytorch.callbacks import TQDMProgressBar\n","from lightning.pytorch.callbacks import DeviceStatsMonitor\n","from lightning.pytorch.profilers import AdvancedProfiler\n","\n","\n","\n","import importlib\n","import pre_processing\n","importlib.reload(pre_processing)\n","from pre_processing import get_preprocessing\n","\n","\n","from dataset import MyDataset\n","\n","from lightning.pytorch.loggers import TensorBoardLogger\n","\n","\n","augumentations=[get_training_augmentation(), get_validation_augmentation(), get_validation_augmentation()]\n","# if __name__ == \"__main__\":\n","\n","model = MyLightningModel()\n","\n","data_module = MyDataModule(augmentation=augumentations, preprocessing=get_preprocessing())\n","\n","\n","# profiler = AdvancedProfiler(dirpath=\".\", filename=\"lightning_logs/perf_logs\")\n","#profiler=profiler, default_root_dir='/Users/captainjack/Desktop/CO2_Storage_Jack/'\n","#consider trying mix precision https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html\n","#fast_dev_run=True,\n","#ModelSummary(max_depth=-1), no need for baseline model\n","#profiler=\"simple\"\n","\n","# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n","\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","\n","#save per epoch!\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath='.',\n","    filename='model-{epoch:04d}',\n","    save_top_k=1,  # Save all checkpoints\n","    monitor=None,  # Disable monitoring\n","    verbose=True\n",")\n","\n","trainer = L.Trainer(max_epochs=12,\\\n","                     callbacks=[checkpoint_callback,TQDMProgressBar(refresh_rate=20),\\\n","                                EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, \\\n","                                              verbose=False)])\n","#VisualizationCallback(data_module)\n","\n","# check validation before large training step\n","#num_sanity_val_steps=2, \n","\n","trainer.fit(model, data_module)\n","\n","train_loss=model.record_trainloss\n","val_loss=model.record_valoss\n","\n","\n","#trainer.save_checkpoint(\"/kaggle/working/example.ckpt\")\n","\n","\n","# test the model \n","trainer.test(model, data_module) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.083119Z","iopub.status.idle":"2024-03-14T21:49:43.083436Z","shell.execute_reply":"2024-03-14T21:49:43.083296Z","shell.execute_reply.started":"2024-03-14T21:49:43.083282Z"},"trusted":true},"outputs":[],"source":["train_losscpu=[t.cpu().detach().numpy() for t in train_loss]\n","val_losscpu=[t.cpu().detach().numpy() for t in val_loss]\n","\n","plt.figure()\n","plt.plot(train_losscpu, label=\"training Loss\")\n","plt.plot(val_losscpu, label=\"Validation Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","View logs in tensorboard\n","\n","If youâ€™re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n","\n","%reload_ext tensorboard\n","%tensorboard --logdir=lightning_logs/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.084858Z","iopub.status.idle":"2024-03-14T21:49:43.085172Z","shell.execute_reply":"2024-03-14T21:49:43.085033Z","shell.execute_reply.started":"2024-03-14T21:49:43.085020Z"},"trusted":true},"outputs":[],"source":["# !kill 400      \n","# %reload_ext tensorboard\n","# %tensorboard --logdir lightning_logs/\n"]},{"cell_type":"markdown","metadata":{},"source":["Some Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.086504Z","iopub.status.idle":"2024-03-14T21:49:43.086814Z","shell.execute_reply":"2024-03-14T21:49:43.086676Z","shell.execute_reply.started":"2024-03-14T21:49:43.086662Z"},"trusted":true},"outputs":[],"source":["\n","def visualize(**images):\n","    \"\"\"Plot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","\n","for idx_ in range(4):\n","    current_timestep = 10*idx_\n","    print(f'plotting for time step: {current_timestep}')\n","    image, mask = data_module.example_dataset[current_timestep] # get some sample\n","    visualize(\n","        concentration=image[0,:, :].squeeze(),\n","        eps=image[1,:, :].squeeze(),\n","        Ux=image[2,:, :].squeeze(),\n","        Uy=image[3,:, :].squeeze(),\n","        dissolution=mask.squeeze(),\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["Use original code to do plotting (see test results!)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.089205Z","iopub.status.idle":"2024-03-14T21:49:43.089700Z","shell.execute_reply":"2024-03-14T21:49:43.089479Z","shell.execute_reply.started":"2024-03-14T21:49:43.089460Z"},"trusted":true},"outputs":[],"source":["def read_simulation_hdf(file_name):\n","    print(f'loading the file: {file_name}')\n","    data_dict = {}\n","\n","    with h5py.File(file_name, \"r\") as file_handle:\n","        # List all groups\n","        print(f\"Keys: {file_handle.keys()}\")\n","        scaling_factor = 1\n","        for key_ in file_handle.keys():\n","            if 'key_' == 'C':\n","                scaling_factor = 1 # 100\n","            elif 'key_' == 'Ux' or 'key_' == 'Uy':\n","                scaling_factor = 1 # 1000\n","            \n","            data_dict[key_] = scaling_factor * np.array(file_handle[key_])\n","            print(f'Done loading the variable {key_} of shape: {data_dict[key_].shape}')\n","\n","        print(f'Done with {file_name} == closing file now')\n","\n","    return data_dict['C'], data_dict['eps'], data_dict['Ux'], data_dict['Uy'],\n","\n","\n","def load_datafiles(data_filenames):\n","    # snapshot_indices will split the data in time into train and validation\n","    data_dict = {\n","        'C': [], # list of np arrays\n","        'eps': [],\n","        'Ux': [],\n","        'Uy': [],\n","    }\n","\n","    for filename in data_filenames:\n","        C, eps, Ux, Uy = read_simulation_hdf(filename)\n","        data_dict['C'].append(C[2:-2, 2:-2, :])\n","        data_dict['eps'].append(eps[2:-2, 2:-2, :])\n","        data_dict['Ux'].append(Ux[2:-2, 2:-2, :])\n","        data_dict['Uy'].append(Uy[2:-2, 2:-2, :])\n","    return data_dict\n","\n","def get_filelist():\n","    from itertools import permutations, product\n","    # permutations = list(permutations(range(4), 2))\n","    permutations = list(product(range(4), repeat=2))\n","\n","    file_list = []\n","    properties_list = []\n","    for idx1, idx2 in permutations:\n","        # filename_hdf = f'Pe{peclet_value}_K{k_value}_101steps.hdf5'\n","        # filename_hdf = f'data_new/Pe{peclet_value[data_idx]}_K{k_value[data_idx]}.hdf5'\n","        file_name = f'/kaggle/input/track2dataset/256modelruns/Pe1_K1_{idx1}_{idx2}.hdf5'\n","        file_list.append(file_name)\n","    return file_list"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.091269Z","iopub.status.idle":"2024-03-14T21:49:43.091855Z","shell.execute_reply":"2024-03-14T21:49:43.091617Z","shell.execute_reply.started":"2024-03-14T21:49:43.091597Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset, RandomSampler\n","\n","def preprocess_data_cube(data_dict, scaling_dict):\n","    print(f'preprocess_data_cube')\n","\n","    masks = []\n","    images = []\n","    for file_idx in range(len(data_dict['C'])):\n","        C = data_dict['C'][file_idx][:, :, :-1]\n","        eps = data_dict['eps'][file_idx][:, :, :-1]\n","        Ux = data_dict['Ux'][file_idx][:, :, :-1]\n","        Uy = data_dict['Uy'][file_idx][:, :, :-1]\n","        eps_t = data_dict['eps'][file_idx][:, :, 1:]\n","\n","        # mask = log_transform(eps_t - eps[:, :, :-1]) # this scaled from 0 to 1\n","\n","        #model baseline\n","        #mask = eps_t - eps\n","\n","        #model II: predict next snapshot directly!\n","        #mask = eps_t\n","\n","        #Model III\n","        mask = np.stack([C_t, eps_t, Ux_t, Uy_t], axis=-1)\n","        mask = np.swapaxes(mask, 3, 2)\n","\n","        \n","        \n","        # these should be moved to preprocessing\n","        # C_scaled = log_transform(C*scaling_dict['C_scaling']) - 0.5 # scale to be from 0 to 1\n","        C = C*scaling_dict['C_scaling'] - 0.5\n","        Ux = (Ux - scaling_dict['Ux_mean']) / scaling_dict['Ux_std']\n","        Uy = (Uy - scaling_dict['Uy_mean']) / scaling_dict['Uy_std']\n","        eps = (eps - scaling_dict['eps_mean']) / scaling_dict['eps_std']\n","\n","\n","        image = np.stack([C, eps, Ux, Uy], axis=-1)\n","        image = np.swapaxes(image, 3, 2)\n","\n","        masks.append(mask)\n","        images.append(image)\n","    \n","    masks = np.concatenate(masks, axis=-1)\n","    images = np.concatenate(images, axis=-1)\n","    print(f'preprocess_data_cube: {masks.shape}, {images.shape}')\n","    return images, masks\n","\n","class DissolutionDataset(Dataset):\n","    \"\"\"Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        data_dir (str): path to data folder\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","  \n","    def __init__(\n","            self,\n","            data_filenames,\n","            scaling_dict,\n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","\n","        # self.scaling_dict = scaling_dict\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","        data_dict = load_datafiles(data_filenames)\n","        self.image, self.mask = preprocess_data_cube(data_dict, scaling_dict)\n","        print(self.image.shape, self.mask.shape)\n","        self.data_len = self.image.shape[-1]\n","\n","    \n","    def __getitem__(self, idx):\n","        \n","        image = self.image[:, :, :, idx]\n","        mask = self.mask[:, :, :, idx]\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","            \n","        return image, mask\n","        \n","    def __len__(self):\n","        # assume one file for now\n","        return self.data_len # last element we cann't predict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.092947Z","iopub.status.idle":"2024-03-14T21:49:43.093376Z","shell.execute_reply":"2024-03-14T21:49:43.093172Z","shell.execute_reply.started":"2024-03-14T21:49:43.093154Z"},"trusted":true},"outputs":[],"source":["import albumentations as albu\n","\n","def get_training_augmentation():\n","    train_transform = [\n","        albu.Resize(256, 256),  # not needed\n","        # albu.HorizontalFlip(p=0.5),\n","        # albu.VerticalFlip(p=0.5),\n","    ]\n","    return albu.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Resize to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        albu.Resize(256, 256),  \n","    ]\n","    return albu.Compose(test_transform)\n","\n","\n","def to_tensor_img(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","def to_tensor_mask(x, **kwargs):\n","    return x.astype('float32')\n","\n","def get_preprocessing():\n","    \"\"\"Construct preprocessing transform\n","    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \n","    \"\"\"\n","    \n","    _transform = [\n","        albu.Lambda(image=to_tensor_img, mask=to_tensor_mask),\n","    ]\n","    return albu.Compose(_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.095057Z","iopub.status.idle":"2024-03-14T21:49:43.095510Z","shell.execute_reply":"2024-03-14T21:49:43.095304Z","shell.execute_reply.started":"2024-03-14T21:49:43.095285Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import segmentation_models_pytorch as smp\n","from tqdm.notebook import tqdm\n","from torch.utils.data import RandomSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.096714Z","iopub.status.idle":"2024-03-14T21:49:43.097160Z","shell.execute_reply":"2024-03-14T21:49:43.096957Z","shell.execute_reply.started":"2024-03-14T21:49:43.096938Z"},"trusted":true},"outputs":[],"source":["file_list = get_filelist()\n","print(file_list)\n","\n","data_list = load_datafiles(file_list)\n","# estimate sample mean and std -- this should be done better\n","\n","n_training_samples = 8\n","\n","\n","C = np.stack([data_list['C'][idx] for idx in range(n_training_samples)])\n","eps = np.stack([data_list['eps'][idx] for idx in range(n_training_samples)])\n","Ux = np.stack([data_list['Ux'][idx] for idx in range(n_training_samples)])\n","Uy = np.stack([data_list['Uy'][idx] for idx in range(n_training_samples)])\n","\n","Ux_mean, Ux_std = Ux.mean(), Ux.std()\n","Uy_mean, Uy_std = Uy.mean(), Uy.std()\n","eps_mean, eps_std = eps.mean(), eps.std()\n","\n","\n","print(Ux_mean, Ux_std)\n","print(Uy_mean, Uy_std)\n","print(eps_mean, eps_std)\n","\n","C_scaling = 100\n","data_scalingdict = {\n","    'C_scaling': C_scaling,\n","    'Ux_mean': Ux_mean,\n","    'Ux_std': Ux_std,\n","    'Uy_mean': Uy_mean,\n","    'Uy_std': Uy_std,\n","    'eps_mean': eps_mean,\n","    'eps_std': eps_std,\n","}\n","\n","del Ux, Uy, eps, C"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.098808Z","iopub.status.idle":"2024-03-14T21:49:43.099156Z","shell.execute_reply":"2024-03-14T21:49:43.099007Z","shell.execute_reply.started":"2024-03-14T21:49:43.098993Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","\n","data_filenames = get_filelist()\n","\n","dataset_train = DissolutionDataset(\n","    data_filenames[:8],\n","    scaling_dict=data_scalingdict,\n","    augmentation=get_training_augmentation(), \n","    preprocessing=get_preprocessing(),\n",")\n","\n","dataset_valid = DissolutionDataset(\n","    data_filenames[12:],\n","    scaling_dict=data_scalingdict,\n","    augmentation=get_validation_augmentation(), \n","    preprocessing=get_preprocessing(),\n",")\n","\n","del data_scalingdict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.100498Z","iopub.status.idle":"2024-03-14T21:49:43.100970Z","shell.execute_reply":"2024-03-14T21:49:43.100749Z","shell.execute_reply.started":"2024-03-14T21:49:43.100729Z"},"trusted":true},"outputs":[],"source":["# train_sampler = RandomSampler(dataset_train, replacement=False, num_samples=None)\n","train_sampler = RandomSampler(dataset_train, replacement=True, num_samples=10000)\n","train_loader = DataLoader(dataset_train, batch_size=64, num_workers=4, sampler=train_sampler, drop_last=True)                        \n","valid_loader = DataLoader(dataset_valid, batch_size=4, num_workers=4, shuffle=False, drop_last=True)\n","\n","del dataset_train, dataset_valid\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.102369Z","iopub.status.idle":"2024-03-14T21:49:43.102808Z","shell.execute_reply":"2024-03-14T21:49:43.102604Z","shell.execute_reply.started":"2024-03-14T21:49:43.102577Z"},"trusted":true},"outputs":[],"source":["model = MyLightningModel.load_from_checkpoint('/kaggle/working/example.ckpt')\n","model.eval()\n","\n","cpu_device = torch.device('cpu')\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","l2_loss = torch.nn.MSELoss() # smp.losses.DiceLoss()\n","l1_loss = torch.nn.L1Loss() # solution is sparse\n","\n","with torch.no_grad():\n","    # loop 1 on training data\n","    train_loss = []\n","    # train_loss_scaled = []\n","    preds_list_train = []\n","    masks_list_train = []\n","\n","    for imgs, masks in tqdm(train_loader):\n","        imgs = imgs.to(device, non_blocking=True)\n","        masks = masks.to(device, non_blocking=True) # .squeeze()\n","        preds = model(imgs).squeeze()\n","\n","        l2_loss_values = l2_loss(masks, preds)\n","        train_loss.append(l2_loss_values.item())\n","\n","        # store the true values\n","        # Changed to store in CPU\n","        masks_list_train.append(masks.to(cpu_device).numpy())\n","        preds_list_train.append(preds.to(cpu_device).numpy())\n","\n","    train_loss = np.array(train_loss)\n","    print(f'train_loss: {train_loss.mean()}')\n","    \n","    val_loss = []\n","    preds_list_val = []\n","    masks_list_val = []\n","\n","    # loop 2 on validation data\n","    for imgs, masks in tqdm(valid_loader):\n","        imgs = imgs.to(device, non_blocking=True)\n","        masks = masks.to(device, non_blocking=True)# .squeeze()\n","        preds = model(imgs).squeeze()\n","\n","        l2_loss_values = l2_loss(masks, preds)\n","        val_loss.append(l2_loss_values.item())\n","\n","        # store the true values\n","        # Changed to store in CPU\n","        masks_list_val.append(masks.to(cpu_device).numpy())\n","        preds_list_val.append(preds.to(cpu_device).numpy())\n","\n","    val_loss = np.array(val_loss)\n","    print(f'validation_loss: {val_loss.mean()}') #, val_loss_scaled: {val_loss_scaled.mean()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.104271Z","iopub.status.idle":"2024-03-14T21:49:43.104765Z","shell.execute_reply":"2024-03-14T21:49:43.104542Z","shell.execute_reply.started":"2024-03-14T21:49:43.104522Z"},"trusted":true},"outputs":[],"source":["preds_list_train = np.concatenate(preds_list_train)\n","masks_list_train = np.concatenate(masks_list_train)\n","\n","preds_list_val = np.concatenate(preds_list_val)\n","masks_list_val = np.concatenate(masks_list_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.106409Z","iopub.status.idle":"2024-03-14T21:49:43.106750Z","shell.execute_reply":"2024-03-14T21:49:43.106601Z","shell.execute_reply.started":"2024-03-14T21:49:43.106587Z"},"trusted":true},"outputs":[],"source":["def matshow_error(pred, truth, figsize=(40, 18), scale=False, title=None, filename=None):\n","    fig, ax = plt.subplots(1, 3, figsize=figsize)\n","    \n","    v_max = max(truth.max(), pred.max())\n","    v_min = max(truth.min(), pred.min())\n","\n","    if scale:\n","        im = ax[0].matshow(pred, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n","    else:\n","        im = ax[0].matshow(pred, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n","    # im.set_clim(0.0, 0.3)\n","    ax[0].set_title(f'{title} prediction')\n","    divider = make_axes_locatable(ax[0])\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","    cbar = plt.colorbar(im, cax=cax)\n","\n","    if scale:\n","        im = ax[1].matshow(truth, vmin=0, vmax=1, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n","    else:\n","        im = ax[1].matshow(truth, vmin=v_min, vmax=v_max, cmap=plt.get_cmap('Reds'))# 'inferno_r'))\n","    # im.set_clim(0.0, 0.3)\n","    ax[1].set_title(f'{title} reference')\n","    divider = make_axes_locatable(ax[1])\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","    plt.colorbar(im, cax=cax)\n","\n","    # error = np.abs(pred-truth)\n","    error = pred-truth\n","\n","    im = ax[2].matshow(error, cmap=plt.get_cmap('seismic')) #.get_cmap('RdGy'))\n","    max_abs_error = np.max(np.abs(error))\n","    # Set the color limits dynamically centered around zero\n","    clim = (-max_abs_error, max_abs_error)\n","    im.set_clim(clim)\n","\n","    ax[2].set_title(f'{title} error')\n","    divider = make_axes_locatable(ax[2])\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","    plt.colorbar(im, cax=cax)\n","    plt.tight_layout()\n","    if filename is not None:\n","        plt.savefig(filename)\n","    plt.show()\n","\n","for sample_idx in range(1): #12):\n","    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n","        preds = preds_list_train[sample_idx*100+time_step, :, :]\n","        masks = masks_list_train[sample_idx*100+time_step, :, :]\n","        # matshow2(scaling_func(preds), scaling_func(masks), title=f'train sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n","        matshow_error(\n","            preds,\n","            masks, \n","            title=f'train sample: {sample_idx}, timestep: {time_step}, eps: ', \n","            filename=f'vis_results/training_eps_{sample_idx}_{time_step}.pdf',\n","            figsize=(15, 7))\n","\n","        \n","for sample_idx in range(1): #4):\n","    for time_step in [0, 1, 3, 7, 10, 20, 40, 60, 90, 99]:\n","        preds = preds_list_val[sample_idx*100+time_step, :, :]\n","        masks = masks_list_val[sample_idx*100+time_step, :, :]\n","        # matshow2(scaling_func(preds), scaling_func(masks), title=f'validation sample: {sample_idx}, scaled prediction eps', filename='original_eps.pdf')\n","        matshow_error(\n","            preds,\n","            masks,\n","            title=f'validation sample: {sample_idx}, timestep: {time_step}, eps: ', \n","            filename=f'vis_results/validation_eps_{sample_idx}_{time_step}.pdf',\n","            figsize=(15, 7))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-14T21:49:43.108256Z","iopub.status.idle":"2024-03-14T21:49:43.108705Z","shell.execute_reply":"2024-03-14T21:49:43.108481Z","shell.execute_reply.started":"2024-03-14T21:49:43.108462Z"},"trusted":true},"outputs":[],"source":["# import os\n","# for dirname, _, filenames in os.walk('/kaggle/working/tb_logs'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4568887,"sourceId":7802501,"sourceType":"datasetVersion"},{"datasetId":4600908,"sourceId":7846617,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
